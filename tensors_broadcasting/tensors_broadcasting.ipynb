{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì° Broadcasting\n",
    "\n",
    "Broadcasting √© um mecanismo que permite realizar opera√ß√µes aritm√©ticas entre tensores de diferentes formas (ou tamanhos):\n",
    "\n",
    "1. **Como foi poss√≠vel observar at√© agora, cada tensor possui pelo menos uma dimens√£o.**\n",
    "- Isso significa que os tensores envolvidos na opera√ß√£o devem ter pelo menos uma dimens√£o. Um tensor n√£o pode ser um escalar (um √∫nico n√∫mero sem dimens√µes) se quisermos aplicar as regras de broadcasting.\n",
    "\n",
    "2. **O Broadcasting** √© algo que acontece tanto no **Pytorch** quanto no **Numpy** por√©m √© algo ocorrendo debaixo dos panos nao √© nenhum m√©todo que ser√° aplicado ao tensor por exemplo. Entretanto, o broadcasting possui alguns pr√≠ncipios de funcionamento, e compreender isto, aux√≠lia ao desenvolvedor ter um maior dom√≠nio do que est√° acontecendo com seus tensores.\n",
    "\n",
    " Para que o broadcasting entao aconteca esta operacao ir√° sempre comecar verificando se a dimens√£o final possui os tamanhos das compativ√©is que permitem o broadcasting sendo rem resumo:\n",
    " \n",
    "   - As dimensoes mais a direita devem ser iguais;\n",
    "   - Algum dos valores ou dimensao √© 1, ou um deles n√£o existe;\n",
    "  \n",
    "  Como mencionado, o brodcasting inicia:\n",
    "   - Comparando as dimens√µes dos dois tensores, come√ßando pela √∫ltima dimens√£o (a mais √† direita) e movendo-se para a primeira dimens√£o (a mais √† esquerda).\n",
    "   \n",
    "  Agora de maneira detalhada como j√° foi citado anteriormente para cada par de dimens√µes comparadas, tr√™s cen√°rios permitem o broadcasting:\n",
    "     1. **Os tamanhos das dimens√µes s√£o iguais:** Se as dimens√µes correspondentes nos dois tensores t√™m o mesmo tamanho, ent√£o elas s√£o compat√≠veis. (ocorre broadcasting)\n",
    "     2. **Uma das dimens√µes √© 1:** Se um tensor tem uma dimens√£o de tamanho 1 na posi√ß√£o que est√° sendo comparada, ele pode ser \"esticado\" ou \"expandido\" para corresponder ao tamanho da outra dimens√£o. Isso √© feito sem realmente copiar dados em mem√≥ria, mas sim apenas adaptando a opera√ß√£o aritm√©tica para se comportar como se o tensor menor tivesse sido expandido.\n",
    "     3. **Uma das dimens√µes n√£o existe:** Se um dos tensores tem menos dimens√µes que o outro, podemos considerar que ele tem dimens√µes extras de tamanho 1 na frente (mais √† esquerda). Essas dimens√µes \"faltantes\" s√£o implicitamente consideradas como 1, permitindo o broadcasting.\n",
    "\n",
    "![](https://deeplearninguniversity.com/wp-content/uploads/2020/11/Screenshot-2020-11-20-at-1.02.50-PM.png)\n",
    "\n",
    "(Fonte: https://deeplearninguniversity.com/pytorch/pytorch-broadcasting/)\n",
    "\n",
    "O broadcasting permite que voc√™ realize opera√ß√µes aritm√©ticas entre tensores de formas diferentes de uma maneira eficiente e intuitiva. Por exemplo, voc√™ pode adicionar um vetor a cada linha de uma matriz ou multiplicar uma matriz 3D por um vetor, aplicando a opera√ß√£o em cada \"fatia\" da matriz 3D, sem a necessidade de loops expl√≠citos ou duplica√ß√£o de dados.\n",
    "\n",
    "Vamos, observar alguns exemplos para que fique ainda mais claro, e tamb√©m para podermos entender como o broadcasting ocorre na pr√°tica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8484, 0.3655],\n",
       "        [0.6376, 0.4929]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "n = torch.rand(2,2)\n",
    "\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6048, 0.5671])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j = torch.rand(2)\n",
    "\n",
    "j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceba que aqui as regras, para a existencia do **Broadcasting** estao sendo satisfeitas, pois, temos que a dimensao mais a direita √© `1` ou uma dimensao maior, e neste caso se percebermos ambos os tensores possuem o mesmo n√∫mero de colunas (dimensao mais a direita) ambos possuem 2 colunas (dimensoes iguais), portanto o **Tensor j** sofrer√° **Broadcasting** e ter√° seus valores duplicados de tal forma que tenhamos uma matriz (2 x 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4532, 0.9326],\n",
       "        [1.2424, 1.0600]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broad = n + j\n",
    "\n",
    "broad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um outro comentario interessante a se fazer em relacao ao Broadcasting √© que ele √© aplicado a todas as operacoes aritim√©ticas que vimos at√© aqui, incluindo por exemplo produtos escalares entre Tensores, sem nos esquecermos das operacoes arit√©mticas entre **Tensores** fundamentais como por exemplo (+, - , * , /) e etc.\n",
    "\n",
    "O que devemos salientar √© para os produtos escalares que no final das contas, a grosso modo sao modelos simplificados de um neuronio artificial e portanto, ao lidar com Redes MLP, ou Redes Convolucionais, estamos a todo momento trabalhando com o Broadcasting. Vamos ver um exemplo para consildar este conhecimento tao importante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2,2,3)\n",
    "w = torch.rand(3,2)\n",
    "b = torch.rand(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.6143, 1.1010],\n",
       "         [1.2022, 1.0477]],\n",
       "\n",
       "        [[1.0583, 1.0030],\n",
       "         [2.0519, 1.3876]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron = torch.matmul(x, w) + b\n",
    "\n",
    "neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unicamp-ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
