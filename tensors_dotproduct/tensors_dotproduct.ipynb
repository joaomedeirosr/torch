{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📐✖️ Produto Escalar\n",
    "\n",
    "**O produto escalar**, também conhecido como produto interno, é uma operação matemática que permite combinar dois vetores para resultar em um único número escalar. 🎯\n",
    "\n",
    "Por que o produto escalar é tão utilizado no mundo da AI? 🤔💡\n",
    "\n",
    "1. **Combinação Linear de Entradas:** 🧠 Em uma rede neural, cada neurônio calcula uma combinação linear de seus inputs, que são os valores recebidos de outros neurônios ou da entrada inicial. O produto escalar faz justamente isso, multiplicando inputs por pesos e somando-os para criar a saída do neurônio.\n",
    "\n",
    "2. **Produto Escalar é Diferenciável:** 🔄 A diferenciabilidade do produto escalar é essencial para aplicar algoritmos de otimização baseados em gradiente, como o Gradiente Descendente, cruciais para o treinamento de redes neurais.\n",
    "\n",
    "\n",
    "A equação do produto escalar entre dois vetores $ \\mathbf{a} $ e $ \\mathbf{b} $ pode ser escrita como:\n",
    "\n",
    "$$ \\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^{n} a_i b_i $$\n",
    "\n",
    "\n",
    "E como é o produto escalar em matriz?\n",
    "\n",
    "https://www.mathsisfun.com/algebra/matrix-multiplying.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(5)\n",
    "b = torch.rand(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5952, 0.3507, 0.0260, 0.1331, 0.2415, 0.4984])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9816, 0.7538, 0.5317, 0.3351, 0.1633])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2109, 0.5796, 0.4587, 0.3141, 0.0383])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a * b\n",
    "\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceba, que se observarmos matematicamente a equacao matematica do produto escalar vemos que ele realiza a multiplicacao (produto) elemento por elemento realizando um somatório. Portanto, se tentarmos transferir esta ideia, para o `PyTorch` ou até mesmo implementar sem utilizar o Pytorch, temos que após realizar o produto, devemos realizar uma soma. Ou seja, estaremos utilizando um método de reducao, que irá transformar esses produtos elemento a elemento do vetor em um único escalar. Sendo assim, vamos tentar reproduzir esta ideia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2669, 0.9414, 0.8007, 0.3315, 0.8320, 0.5161, 0.5016, 0.4714, 0.3182,\n",
      "        0.8395])\n",
      "tensor([0.3715, 0.6113, 0.8205, 0.5979, 0.8898, 0.4659, 0.5087, 0.2948, 0.3538,\n",
      "        0.2411])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.2197)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dot_product(a,b):\n",
    "    return torch.sum(a * b)\n",
    "\n",
    "vetor1 = torch.rand(10)\n",
    "vetor2 = torch.rand(10)\n",
    "\n",
    "print(vetor1)\n",
    "print(vetor2)\n",
    "dot_product(vetor1, vetor2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas, veja isso gera um certo trabalho adicional, pois é necessário criar uma funcao, e utilizar um método de reducao do Pytorch para só assim entao termos o resultado.\n",
    "\n",
    "Porém seria bem mas conveniente se o Pytorch possuir alguns métodos que já facilitem essa etapa de calcular o produto escalar e é justamente isso que iremos explorar a seguir:\n",
    "\n",
    "O primeiro método, e pode ser considerado o mais comum é o `torch.dot()` o que ele faz ? \n",
    "R: Basicamente o `torch.dot()` reproduz o que implementamos anteriormente, ou seja recebe 2 tensores de ordem 1 e retorna um escalar, porém\n",
    "o torch.dot, nos fornece isso bastando chamá-lo.\n",
    "\n",
    "Como podemos usar entao o `torch.dot()` e seus detalhes:\n",
    "\n",
    "`torch.dot()`:\n",
    "   - **Descrição**: Calcula o produto escalar (🎯) de dois tensores 1-D (vetores).\n",
    "   - **Entrada**: Dois vetores (tensores 1-D) 📏📏.\n",
    "   - **Saída**: Um único número escalar (🔢), que é o produto escalar dos dois vetores.\n",
    "   - **Uso**: Apropriado quando você quer a interação direta ponto a ponto de dois vetores. Por exemplo: `torch.dot(tensor1, tensor2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6679)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(6)\n",
    "b = torch.rand(6)\n",
    "\n",
    "escalar = torch.dot(a,b)\n",
    "\n",
    "escalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos, agora ver uma alternativa existente ao `torch.dot()`. Basicamente o `torch.mm()` ele é muito utilizado, quando temos tensores de ordem 2.\n",
    "\n",
    " `torch.mm()`:\n",
    "   - **Descrição**: Executa a multiplicação de matrizes (✖️) entre dois tensores 2-D.\n",
    "   - **Entrada**: Duas matrizes (tensores 2-D) 📄📄.\n",
    "   - **Saída**: Um tensor 2-D que é o produto das duas matrizes (📘).\n",
    "   - **Uso**: Perfeito para quando você precisa multiplicar duas matrizes como em álgebra linear clássica. Por exemplo: `torch.mm(matrix1, matrix2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2435, 0.7150, 0.6578],\n",
       "        [0.4218, 0.4848, 0.6333]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_m = torch.rand(2,3)\n",
    "b_m = torch.rand(2,3)\n",
    "\n",
    "a_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2435, 0.7150, 0.6578],\n",
       "        [0.4218, 0.4848, 0.6333]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que se executarmos a linha abaixo teremos um erro. Este erro ocorre porque devemos lembrar que na matematica, ou melhor dizendo, matematicamente existem algumas condicoes para que exista o produto escalar entre dois vetores sendo $m = p$ ou seja. Se tiver um tensor `a_m` que é `2 x 3` do tipo `(n x m)` e um outro tensor `b_m` que é `2 x 3` `(p x q)` Veja que para que existe o produto escalar devemos garantir que $m = p$. **Em outras palavras para que possamos, entao realizar o produto escalar, devemos ter que o número de colunas do primeiro tensor deve ser igual ao número de linhas do outro** E podemos, resolver isso sem modificar a estrutura interna do tensor realizando a operacao de transposicao. Caso as condicoes sejam satisfeitas o produto escalar irá existir e sua **saída será sempre no formato: `n x q`** onde podemos fazer um paralelo com redes neurais.\n",
    "\n",
    "\n",
    "Alguns pontos importantes a salientar sao que tudo isso existe porque o produto escalar é um caso especial de multiplicacao de matrizes e expandindo ainda mais devemos lembrar que um produto escalar ou também conhecido como produto interno só pode ser definido se ambos possuem o mesmo número de componentes.\n",
    "\n",
    "Portanto, para que a operacao do produto escalar seja feita precisamos alterar ou melhor dizendo realizar a transposta do segundo tensor transformando-o em uma matriz coluna, isso irá satisfazer a condicao da multiplicacao de matrizes onde $m = p$, pois se m for diferente de p, entao nao teremos o produto escalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x3 and 2x3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma_m\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x3 and 2x3)"
     ]
    }
   ],
   "source": [
    "result = torch.mm(a_m, b_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2983, 1.2372],\n",
       "        [0.9794, 0.9677]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = torch.mm(a_m, b_m.T)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, vamos entender um outro método existente do Pytorch que é o `torch.matmul()` que é extremamente utilizado quando queremos calcular o produto escalar, pois ela se trata de uma ferramenta um pouco mais geral, para calcular produto escalar nao ficando restrita a ordem dos tensores de entrada.\n",
    "\n",
    "`torch.matmul()`:\n",
    "   - **Descrição**: Uma ferramenta de multiplicação de matrizes mais geral (🔗) que lida com matrizes de alta dimensão e broadcasting.\n",
    "   - **Entrada**: Pode ser tensores de qualquer dimensão (⚛️).\n",
    "   - **Saída**: O tensor resultante pode variar em dimensão, de acordo com as regras de broadcasting (📈).\n",
    "   - **Uso**: Versátil e potente, essa função é adequada para uma gama mais ampla de operações de álgebra tensorial. Por exemplo: `torch.matmul(tensor1, tensor2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h0/jqw3wsl561j81vjn6nzz1j740000gn/T/ipykernel_18135/3582778196.py:4: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3641.)\n",
      "  t_result = torch.matmul(a_t,b_t.T)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m a_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      2\u001b[0m b_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m t_result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "a_t = torch.rand(2,2,3)\n",
    "b_t = torch.rand(2,2,3)\n",
    "\n",
    "t_result = torch.matmul(a_t,b_t.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5704, 0.8776],\n",
       "         [1.1949, 1.5624]],\n",
       "\n",
       "        [[0.9798, 0.3756],\n",
       "         [0.1970, 0.0779]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_result = torch.matmul(a_t,b_t.transpose(1,2))\n",
    "\n",
    "t_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unicamp-ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
