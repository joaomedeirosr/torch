{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“âœ–ï¸ Produto Escalar\n",
    "\n",
    "**O produto escalar**, tambÃ©m conhecido como produto interno, Ã© uma operaÃ§Ã£o matemÃ¡tica que permite combinar dois vetores para resultar em um Ãºnico nÃºmero escalar. ðŸŽ¯\n",
    "\n",
    "Por que o produto escalar Ã© tÃ£o utilizado no mundo da AI? ðŸ¤”ðŸ’¡\n",
    "\n",
    "1. **CombinaÃ§Ã£o Linear de Entradas:** ðŸ§  Em uma rede neural, cada neurÃ´nio calcula uma combinaÃ§Ã£o linear de seus inputs, que sÃ£o os valores recebidos de outros neurÃ´nios ou da entrada inicial. O produto escalar faz justamente isso, multiplicando inputs por pesos e somando-os para criar a saÃ­da do neurÃ´nio.\n",
    "\n",
    "2. **Produto Escalar Ã© DiferenciÃ¡vel:** ðŸ”„ A diferenciabilidade do produto escalar Ã© essencial para aplicar algoritmos de otimizaÃ§Ã£o baseados em gradiente, como o Gradiente Descendente, cruciais para o treinamento de redes neurais.\n",
    "\n",
    "\n",
    "A equaÃ§Ã£o do produto escalar entre dois vetores $ \\mathbf{a} $ e $ \\mathbf{b} $ pode ser escrita como:\n",
    "\n",
    "$$ \\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^{n} a_i b_i $$\n",
    "\n",
    "\n",
    "E como Ã© o produto escalar em matriz?\n",
    "\n",
    "https://www.mathsisfun.com/algebra/matrix-multiplying.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(5)\n",
    "b = torch.rand(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5952, 0.3507, 0.0260, 0.1331, 0.2415, 0.4984])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9816, 0.7538, 0.5317, 0.3351, 0.1633])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2109, 0.5796, 0.4587, 0.3141, 0.0383])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a * b\n",
    "\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceba, que se observarmos matematicamente a equacao matematica do produto escalar vemos que ele realiza a multiplicacao (produto) elemento por elemento realizando um somatÃ³rio. Portanto, se tentarmos transferir esta ideia, para o `PyTorch` ou atÃ© mesmo implementar sem utilizar o Pytorch, temos que apÃ³s realizar o produto, devemos realizar uma soma. Ou seja, estaremos utilizando um mÃ©todo de reducao, que irÃ¡ transformar esses produtos elemento a elemento do vetor em um Ãºnico escalar. Sendo assim, vamos tentar reproduzir esta ideia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2669, 0.9414, 0.8007, 0.3315, 0.8320, 0.5161, 0.5016, 0.4714, 0.3182,\n",
      "        0.8395])\n",
      "tensor([0.3715, 0.6113, 0.8205, 0.5979, 0.8898, 0.4659, 0.5087, 0.2948, 0.3538,\n",
      "        0.2411])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.2197)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dot_product(a,b):\n",
    "    return torch.sum(a * b)\n",
    "\n",
    "vetor1 = torch.rand(10)\n",
    "vetor2 = torch.rand(10)\n",
    "\n",
    "print(vetor1)\n",
    "print(vetor2)\n",
    "dot_product(vetor1, vetor2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas, veja isso gera um certo trabalho adicional, pois Ã© necessÃ¡rio criar uma funcao, e utilizar um mÃ©todo de reducao do Pytorch para sÃ³ assim entao termos o resultado.\n",
    "\n",
    "PorÃ©m seria bem mas conveniente se o Pytorch possuir alguns mÃ©todos que jÃ¡ facilitem essa etapa de calcular o produto escalar e Ã© justamente isso que iremos explorar a seguir:\n",
    "\n",
    "O primeiro mÃ©todo, e pode ser considerado o mais comum Ã© o `torch.dot()` o que ele faz ? \n",
    "R: Basicamente o `torch.dot()` reproduz o que implementamos anteriormente, ou seja recebe 2 tensores de ordem 1 e retorna um escalar, porÃ©m\n",
    "o torch.dot, nos fornece isso bastando chamÃ¡-lo.\n",
    "\n",
    "Como podemos usar entao o `torch.dot()` e seus detalhes:\n",
    "\n",
    "`torch.dot()`:\n",
    "   - **DescriÃ§Ã£o**: Calcula o produto escalar (ðŸŽ¯) de dois tensores 1-D (vetores).\n",
    "   - **Entrada**: Dois vetores (tensores 1-D) ðŸ“ðŸ“.\n",
    "   - **SaÃ­da**: Um Ãºnico nÃºmero escalar (ðŸ”¢), que Ã© o produto escalar dos dois vetores.\n",
    "   - **Uso**: Apropriado quando vocÃª quer a interaÃ§Ã£o direta ponto a ponto de dois vetores. Por exemplo: `torch.dot(tensor1, tensor2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6679)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(6)\n",
    "b = torch.rand(6)\n",
    "\n",
    "escalar = torch.dot(a,b)\n",
    "\n",
    "escalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos, agora ver uma alternativa existente ao `torch.dot()`. Basicamente o `torch.mm()` ele Ã© muito utilizado, quando temos tensores de ordem 2.\n",
    "\n",
    " `torch.mm()`:\n",
    "   - **DescriÃ§Ã£o**: Executa a multiplicaÃ§Ã£o de matrizes (âœ–ï¸) entre dois tensores 2-D.\n",
    "   - **Entrada**: Duas matrizes (tensores 2-D) ðŸ“„ðŸ“„.\n",
    "   - **SaÃ­da**: Um tensor 2-D que Ã© o produto das duas matrizes (ðŸ“˜).\n",
    "   - **Uso**: Perfeito para quando vocÃª precisa multiplicar duas matrizes como em Ã¡lgebra linear clÃ¡ssica. Por exemplo: `torch.mm(matrix1, matrix2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2435, 0.7150, 0.6578],\n",
       "        [0.4218, 0.4848, 0.6333]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_m = torch.rand(2,3)\n",
    "b_m = torch.rand(2,3)\n",
    "\n",
    "a_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2435, 0.7150, 0.6578],\n",
       "        [0.4218, 0.4848, 0.6333]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que se executarmos a linha abaixo teremos um erro. Este erro ocorre porque devemos lembrar que na matematica, ou melhor dizendo, matematicamente existem algumas condicoes para que exista o produto escalar entre dois vetores sendo $m = p$ ou seja. Se tiver um tensor `a_m` que Ã© `2 x 3` do tipo `(n x m)` e um outro tensor `b_m` que Ã© `2 x 3` `(p x q)` Veja que para que existe o produto escalar devemos garantir que $m = p$. **Em outras palavras para que possamos, entao realizar o produto escalar, devemos ter que o nÃºmero de colunas do primeiro tensor deve ser igual ao nÃºmero de linhas do outro** E podemos, resolver isso sem modificar a estrutura interna do tensor realizando a operacao de transposicao. Caso as condicoes sejam satisfeitas o produto escalar irÃ¡ existir e sua **saÃ­da serÃ¡ sempre no formato: `n x q`** onde podemos fazer um paralelo com redes neurais.\n",
    "\n",
    "\n",
    "Alguns pontos importantes a salientar sao que tudo isso existe porque o produto escalar Ã© um caso especial de multiplicacao de matrizes e expandindo ainda mais devemos lembrar que um produto escalar ou tambÃ©m conhecido como produto interno sÃ³ pode ser definido se ambos possuem o mesmo nÃºmero de componentes.\n",
    "\n",
    "Portanto, para que a operacao do produto escalar seja feita precisamos alterar ou melhor dizendo realizar a transposta do segundo tensor transformando-o em uma matriz coluna, isso irÃ¡ satisfazer a condicao da multiplicacao de matrizes onde $m = p$, pois se m for diferente de p, entao nao teremos o produto escalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x3 and 2x3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma_m\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x3 and 2x3)"
     ]
    }
   ],
   "source": [
    "result = torch.mm(a_m, b_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2983, 1.2372],\n",
       "        [0.9794, 0.9677]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = torch.mm(a_m, b_m.T)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, vamos entender um outro mÃ©todo existente do Pytorch que Ã© o `torch.matmul()` que Ã© extremamente utilizado quando queremos calcular o produto escalar, pois ela se trata de uma ferramenta um pouco mais geral, para calcular produto escalar nao ficando restrita a ordem dos tensores de entrada.\n",
    "\n",
    "`torch.matmul()`:\n",
    "   - **DescriÃ§Ã£o**: Uma ferramenta de multiplicaÃ§Ã£o de matrizes mais geral (ðŸ”—) que lida com matrizes de alta dimensÃ£o e broadcasting.\n",
    "   - **Entrada**: Pode ser tensores de qualquer dimensÃ£o (âš›ï¸).\n",
    "   - **SaÃ­da**: O tensor resultante pode variar em dimensÃ£o, de acordo com as regras de broadcasting (ðŸ“ˆ).\n",
    "   - **Uso**: VersÃ¡til e potente, essa funÃ§Ã£o Ã© adequada para uma gama mais ampla de operaÃ§Ãµes de Ã¡lgebra tensorial. Por exemplo: `torch.matmul(tensor1, tensor2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h0/jqw3wsl561j81vjn6nzz1j740000gn/T/ipykernel_18135/3582778196.py:4: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3641.)\n",
      "  t_result = torch.matmul(a_t,b_t.T)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m a_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      2\u001b[0m b_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m t_result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "a_t = torch.rand(2,2,3)\n",
    "b_t = torch.rand(2,2,3)\n",
    "\n",
    "t_result = torch.matmul(a_t,b_t.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5704, 0.8776],\n",
       "         [1.1949, 1.5624]],\n",
       "\n",
       "        [[0.9798, 0.3756],\n",
       "         [0.1970, 0.0779]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_result = torch.matmul(a_t,b_t.transpose(1,2))\n",
    "\n",
    "t_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unicamp-ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
