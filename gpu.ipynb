{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando a GPU\n",
    "\n",
    "Até o dado momento estávamos trabalhando com a criacao dos tensores utilizando a CPU (default). Agora iremos ver como é possível  trabalhar com tensores e suas operacoes utilizando a GPU.\n",
    "\n",
    "Uma observacao a se fazer logo no ínicio é que, só iremos abordar as ténicas utilizadas com as GPUs da Nvidia, isso se deve ao fato da compatilidade com o Pytorch, entretanto é bem provável que existam estudos sobre APIs do Pytorch para GPUs AMD, por exemplo.\n",
    "\n",
    "Vamos criar um vetor para entender como o Pytorch funciona nas GPUs e além disso iremos realizar algumas manipulacoes com estes tensores.\n",
    "\n",
    "Um ponto interessante a se observar é que ao criarmos os tensores (ordem 1 até ordem n), todos eles sao armazenados na memória RAM, e ao realizarmos operacoes matemáticas entre eles estamos utilizando a CPU para fazer isso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 16,  3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector1 = torch.tensor([1,2,3])\n",
    "\n",
    "vector2 = torch.tensor([5,8,1])\n",
    "\n",
    "\n",
    "result = vector1 * vector2 \n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizando o CUDA\n",
    "\n",
    "O cuda nos permite fazer uma série de coisas, quando o assunto é GPU. Vamos exibir alguns exemplos abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1_gpu = vector1.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um dos primeiros conceitos e talvez um dos mais importantes que iremos tentar reproduzir aqui é que nao é possível multiplicar, somar, subtrair ou realizar operacoes matemáticas entre Tensores ao qual estes tensores estao em regioes de memória diferentes ou seja:\n",
    "\n",
    "Tensor 1 está alocado na RAM do computador \n",
    "Tensor 2 está alocado na VRAM da GPU\n",
    "\n",
    "Logo, se tentarmos realizar algum tipo de operacao matemática ela irá retornar um erro veja:\n",
    "\n",
    "`Expected all tensor to be on the same device, but found at least two devices cuda:0 and cpu`\n",
    "\n",
    "Ou seja ela fala o seguinte, você está tentando realizar uma operacao matemática com elementos(tensores) que estao em regioes de memória diferente e estao utilizando disposivos diferentes. Pois como abordado anteriormente, para realizarmos operacoes matemáticas por padrao utilizamos a CPU para fazer isso, porém na célula de código acima utilizamos o método `.cuda()` para mover o tensor `vector1` para a memória da GPU e portanto para executar a operacao matemática `vector1 * vector2` é preciso garantir que ambos os tensores sejam executados pelo mesmo dispositivo, ambos na CPU ou GPU e portanto eles precisam estar no mesmo \"contexto\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtensor1_gpu\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvector2\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "tensor1_gpu * vector2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor2_gpu = vector2.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que quando executamos entao agora as operacoes em tensores que estao no mesmo contexto ou melhor estao no mesmo dispositivo, nao recebemos nenhuma mensagem de erro tudo corre bem e ele retorna uma informacao: `device = 'cuda:0'` , isso nos indica que o tensor está guardado dentro da GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 16,  3], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1_gpu * tensor2_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando tensores direto na GPU\n",
    "\n",
    "Até agora, o que mostramos é que criamos vetores normalmente utilizando o módulo `tensor` do `Pytorch` e posteriormente nos apenas movemos eles de uma regiao de memória para outra regiao de memória ou melhor, movemos os tensores para regiao de memória de outro dispositivo sendo este a GPU, através do uso do CUDA\n",
    "\n",
    "Agora, vamos mostrar como é possível criar os tensores diretamente na regiao de memória da GPU, ou seja, nao sendo necessário mais mover utilizando o CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor3_gpu = torch.tensor([1,2,3] , device = 'cuda')\n",
    "\n",
    "tensor3_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logo, veja que agora, nao foi necessário mover nenhum tensor para a GPU, como era feito anteriormente pelo método `.cuda()` agora ao criar ele ja foi informado como argumento adicional dentro do método tensor:  \n",
    "\n",
    "`device='cuda:0'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificando a disponiabilidade do CUDA, no ambiente e boas maneiras ao se trabalhar com o torch CUDA\n",
    "\n",
    "É muito importante, que todas essas etapas, atendam a diferentes ambientes principalmente quando estamos propondo alguma arquitetura de rede neural ou algo do tipo, logo o código deve atender as particularidades do ambiente do usuário e para isso podemos checar se há GPU e torch CUDA disponível no ambiente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 5], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "\n",
    "new_gpu_tensor = torch.tensor([1,2,3,5], device = device)\n",
    "\n",
    "\n",
    "new_gpu_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
