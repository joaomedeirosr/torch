{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Otimizadores no Pytorch\n",
    "\n",
    "Os otimizadores no Pytorch são responsáveis por atualizar os parâmetros do modelo durante o treinamento ou seja iterativamente. Normalmente, eles são utilizados juntos com as loss functions e tem o objetivo de ajustar os pesos e vieses do modelo para minimizar essas funcoes de perda.\n",
    "\n",
    "#### Importando otimizadores\n",
    "\n",
    "Para utilizarmos um otimizador no Pytorch, precisamos importa-lo. E o módulo que contém as implementcoes dos otimizadores é o módulo `torch.optim` e é justamente este módulo que vamos utilizar para importar os otimizadores. Portanto, para importar os otimizadores devemos fazer o seguinte:\n",
    "\n",
    "```python\n",
    "    import torch.optim as optim\n",
    "```\n",
    "Ou podemos importar otimizadores de forma direta:\n",
    "\n",
    "```python\n",
    "    from torch import optim\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bom, entao para utilizarmos um otimizador no Pytorch, após termos importado o módulo, devemos criar uma instancia do otimizador. Vamos ver um exemplo na prática. \n",
    "\n",
    "Para este exemplo, vamos utilizar uma instância da classe `SGD` que é uma implementacao do algoritmo de otimizacao Stochastic Gradient Descent.\n",
    "\n",
    "Logo, ao criar uma instancia do otimizador, devemos fornecer os parâmetros do modelo e também a learning rate como argumento. Além disso, é importante fornecer outros hiperparametros como argumento para o otimizador, pois dependendo do otimizador, eles podem ser necessários. Para o caso do SGD, devemos apenas fornecer os parâmetros do modelo e a learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "model = torch.nn.Linear(10, 1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além do SGD, no Pytorch temos uma variedade de otimizadores disponíveis. Vamos ver alguns exemplos:\n",
    "\n",
    "| Classe Otimizadora | Nome do Otimizador |\n",
    "|--------------------|--------------------|\n",
    "| SGD                | Descida de Gradiente Estocástico. O Otimizador de Momentum pode ser usado passando o argumento `momentum` para SGD. O Otimizador de Gradiente Acelerado de Nesterov (NAG) pode ser usado passando o argumento `nesterov=True` para SGD. |\n",
    "| Adagrad            | Otimizador de Gradiente Adaptável. |\n",
    "| RMSprop            | Otimizador de propagação da raiz quadrada média, mas é comumente conhecido como RMSProp. |\n",
    "| Adam               | Otimizador de Estimativa de Momento Adaptativo, mas é comumente conhecido como Adam. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero Grad\n",
    "\n",
    "No Pytorch, por padrao os gradientes calculados durante o backpropagation no loop de treino continuam sendo acumulados. Como isso é algo indesejavél, é necessário zerar os gradientes após cada loop de treino. E para este contexto o método `zero_grad()` é muito útil. Pois, ele define todos os gradientes como 0 e este método é implementado por todos os otimizadores disponíveis no Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ajuste dos pesos\n",
    "\n",
    "Após os gradientes serem calculados, devemos ajustar os pesos do modelo e isso pode ser feito com o método `step()`. Este método é implementado por todos os otimizadores disponíveis no Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unicamp-ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
