{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As funcoes de ativacao no Pytorch\n",
    "\n",
    "As funcoes de ativacao sao muito utilizadas no contexto de Deep Learning, pois elas permitem que a rede neural aprenda padroes mais complexos e etc.\n",
    "\n",
    "Normalmente, as funcoes de ativacao sao aplicadas a saída da soma ponderada das entradas. De maneira resumida o papel de uma funcao de ativacao é introduzir uma nao linearidade no limiar de decisao da rede neural.\n",
    "\n",
    "As funcoes de ativacao no Pytorch podem ser aplicadas a uma camada da rede neural da seguinte maneira:\n",
    "\n",
    "- Usando a camada de ativacao: Que sao basicamente classes que podem ser utilizadas como funcoes de ativacao.\n",
    "- Usando a definicao de funcao de ativacao: Que sao basicamente métodos ou funcoes que podem ser utilizadas como funcao de ativacao.\n",
    "\n",
    "#### Classe funcao de ativacao\n",
    "\n",
    "Na maior parte das vezes, ao trabalhar com funcoes de ativacao utilizamos a classe da camada de ativacao e á adicionamos a camada da arquitetura do nosso modelo, como se tivessemos adicionando normalmente uma camada de uma rede neural. Portanto, a camada de funcao de ativacao é adicionada dentro do construtor da classe do modelo, através da API \"torch.nn.SequentialAPI.\"\n",
    "\n",
    "Como ela é uma classe devemos importar a camada da funcao de ativacao da seguinte maneira:\n",
    "\n",
    "```python\n",
    "from torch import nn\n",
    "```\n",
    "\n",
    "#### Usando funcao de ativacao com o uso da classe camada de funcao de ativacao.\n",
    "\n",
    "Para utilizar a classe de funcao de ativacao, devemos inicialmente criar um objeto ou uma instancia da classe camada de funcao de ativacao. Em seguida, precisamos fornecer uma entrada para a camada, sendo assim o que irá ocorre é que a camada irá aplicar a funcao na entrada informada e retornará a saída. Vamos entao ver um exemplo na prática onde iremos criar uma camada utilizando um objeto da classe ReLUClass, forneceremos como entrada um tensor qualquer para ver o resultado da saída."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "input = torch.tensor([-2, -1 , 0 , 1 , 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando o objeto da funcao de ativacao ReLU\n",
    "relu_layer1 = nn.ReLU()\n",
    "\n",
    "# Aplicando a funcao de ativacao na entrada, para armazenar o resultado da saída deste processo.\n",
    "output = relu_layer1(input)\n",
    "\n",
    "output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definicoes de funcao de ativacao\n",
    "\n",
    "O Pytorch implementou muitas das funcoes de ativacao conhecidas na bibliografia, funcoes essas que podem ser aplicadas e utilizadas diretamente no nosso modelo apenas chamando estas funcoes ou métodos. Essas funcoes, sao utilizadas dentro do `forward()` o qual é um método que serve ou define o fluxo de dados ao longo do modelo. Ou define, se a rede neural é feedforward ou se possui backpropagation.\n",
    "\n",
    "Como vimos anteriormente, podemos utilizar as funcoes de ativacao de duas maneiras no Pytorch utilizando a classe, ou utilizando a definicao de funcao de ativacao, como já praticamos a maneira de utilizar funcoes de ativacao através das classe, vamos agora praticar utilizando a definicao. E para isso vamos ver como pode importar a definicao de funcao de ativacao.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importando a definicao de funcao de ativacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ser possível utilizar a funcao de ativacao através da definicao, devemos fazer algo de maneira muito parecida com o que tinhamos feito com a classe ou seja, criar um objeto e passar para ele uma entrada. Vamos ver na prática como realizar isso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando um tensor de entrada\n",
    "input2 = torch.tensor([-2, -1 , 0 , 1 , 2])\n",
    "\n",
    "# Aplicando a funcao de ativacao ReLU ao tensor de entrada\n",
    "output2 = F.relu(input2)\n",
    "\n",
    "output2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funcoes de ativacao disponiveis no Pytorch\n",
    "\n",
    "O Pytorch possui uma variedade de funcoes de ativacao, que podem ser aplicadas a uma camada da rede neural. Vamos ver agora algumas das funcoes de ativacao disponiveis no Pytorch. Essas funcoes de ativacao estao disponiveis como Layers(classes) ou as \"definitions\" (métodos) que foi como vimos durante o texto, Veja a tabela abaixo que contém algumas das funcoes de ativacao mais comumente usadas:\n",
    "\n",
    "\n",
    "| Classe de camada de função de ativação (em `torch.nn`) | Definição de função de ativação (em `torch.nn.functional`) | Breve Descrição |\n",
    "|--------------------------------------------------------|-------------------------------------------------------------|------------------|\n",
    "| `Sigmoid`                                              | `sigmoid()`                                                 | Calcula o sigmoide da entrada. A função sigmoide é definida como:<br>σ(x) = 1 / (1 + exp(-x)) |\n",
    "| `Tanh`                                                 | `tanh()`                                                    | Calcula a tangente hiperbólica da entrada. A função é definida como:<br>tanh(x) = 2σ(2x) - 1 |\n",
    "| `ReLU`                                                 | `relu()`                                                    | Unidade Linear Retificada. Definida como:<br>relu(x) = max(0, x) |\n",
    "| `LeakyReLU`                                            | `leaky_relu()`                                              | Unidade Linear Retificada Vazada. Definida como:<br>leakyrelu(x) = max(αx, x)<br>α é um hiperparâmetro (geralmente 0.01). |\n",
    "| `ELU`                                                  | `elu()`                                                     | Unidade Linear Exponencial. Definida como:<br>elu(x) = α(exp(x) – 1) se x < 0<br>elu(x) = x se x ≥ 0 |\n",
    "| `Softmax`                                              | `softmax()`                                                 | Usada como camada final para problemas de classificação multiclasse. Retorna a probabilidade de cada classe. |\n",
    "| `Softplus`                                             | `softplus()`                                                | Aproximação suave da função ReLU. |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unicamp-ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
