{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Fundamentos do Pytorch","text":"<p>Esse material ainda est\u00e1 em desenvolvimento e passa por atualiza\u00e7\u00f5es peri\u00f3dicas. Voc\u00ea pode acompanhar as notas de altera\u00e7\u00f5es</p> <p>Ol\u00e1, boas-vindas ao curso Fundamentos do Pytorch! fico feliz que tenha chegado at\u00e9 aqui. Isso, mostra que voc\u00ea est\u00e1 iniciando sua aventura no mundo do Pytorch.</p>"},{"location":"#quais-sao-os-objetivos-deste-material","title":"Quais sao os objetivos deste material?","text":""},{"location":"#o-que-e-o-pytorch","title":"O que \u00e9 o Pytorch?","text":""},{"location":"#sobre-o-curso","title":"Sobre o curso","text":""},{"location":"#o-que-voce-vai-aprender","title":"O que voce vai aprender?","text":""},{"location":"#este-curso-e-gratuito","title":"\ud83d\udcb0 Este curso \u00e9 gratuito?","text":""},{"location":"#onde-o-curso-sera-disponibilizado","title":"Onde o curso ser\u00e1 disponibilizado?","text":""},{"location":"#pre-requisitos","title":"Pr\u00e9-requisitos","text":""},{"location":"#aulas","title":"Aulas","text":""},{"location":"#quem-vai-ministrar-essas-aulas","title":"\ud83e\udd78 Quem vai ministrar essas aulas?","text":"<p>Oi prazer, meu nome \u00e9 Joao Victor.</p>  ![Uma fotografia minha](assets/readme/){ align=left width=\"300\" .shadow}   Sou Mestre e pesquisador em Rob\u00f3tica human\u00f3ide e Aprendizado por Refor\u00e7o extremamente apaixonado pelo que faco. Venho, tentando Tocar este projeto pessoal e compartilhar um pouco do meu conhecimento com a comunidade.  Adoro escrever, compartilhar conhecimento, cultura DIY, solda, eletr\u00f4nica e quase tudo que envolve esse mundo maluco da rob\u00f3tica."},{"location":"#licenca","title":"\ud83d\udcd6 Licen\u00e7a","text":"<p>Todo esse curso foi escrito e produzido por Joao Victor Rocha (@jvmedeirosr{:target=\"_blank\"}).</p>"},{"location":"dataloader_class/","title":"00. Getting Started with PyTorch","text":"In\u00a0[3]: Copied! <pre>import torch\nfrom torch.utils.data import DataLoader\nsample_dataloader = DataLoader(dataset=\"sample_dataset\", batch_size=32, shuffle=False,num_workers=4)\n</pre> import torch from torch.utils.data import DataLoader sample_dataloader = DataLoader(dataset=\"sample_dataset\", batch_size=32, shuffle=False,num_workers=4)  In\u00a0[4]: Copied! <pre>train_dataloader = DataLoader(\"train_dataset\", batch_size = 32, shuffle = True)\nval_dataloader = DataLoader(\"val_dataset\", batch_size = 32, shuffle = False)\ntest_dataloader = DataLoader(\"test_dataset\", batch_size = 32, shuffle = False)\n</pre> train_dataloader = DataLoader(\"train_dataset\", batch_size = 32, shuffle = True) val_dataloader = DataLoader(\"val_dataset\", batch_size = 32, shuffle = False) test_dataloader = DataLoader(\"test_dataset\", batch_size = 32, shuffle = False)"},{"location":"dataloader_class/#a-classe-dataloader-do-pytorch","title":"A classe Dataloader do PyTorch\u00b6","text":"<p>A classe Dataloader do PyTorch \u00e9 uma classe muito importante que permite carregar dados em lotes ou seja ela \u00e9 usada para alimentar os dados que foi representado no<code>Dataclass</code> e carerga estes dados no modelo da Rede Neural, isso a torna muito \u00fatil para o contexto de Deep Learning. Vamos explorar as m\u00ednucias para entender como ela funciona e como podemos utiliza-la para otimizar o processo de treinamento de nossos modelos.</p>"},{"location":"dataloader_class/#importando-a-classe-dataloader","title":"Importando a classe Dataloader\u00b6","text":"<p>Para importar a classe Dataloader, podemos usar o seguinte comando:</p> <pre>from torch.utils.data import DataLoader\n</pre>"},{"location":"dataloader_class/#usando-a-classe-dataloader","title":"Usando a classe Dataloader\u00b6","text":"<p>Para criar um carregador de dados(DataLoader), a primeira coisa que devemos fazer \u00e9 criar um objeto ou uma inst\u00e2ncia da classe <code>DataLoader</code> e passar alguns argumentos existentes no construtor da classe. Sendo eles:</p> <ul> <li><code>dataset</code>: O conjunto de dados para o qual voc\u00ea deseja criar o DataLoader.</li> <li><code>batch_size</code>: O tamanho de cada lote de dados que ir\u00e1 alimentar o modelo.</li> <li><code>shuffle</code>: Se queremos embaralhar os dados enquanto alimentamos o modelo Se nao for especificado, o padr\u00e3o \u00e9 <code>False</code> Geralmente usamos <code>True</code> quando estamos treinando o modelo, mas False durante a valida\u00e7\u00e3o e teste.</li> <li><code>num_workers</code>: O n\u00famero de threads(processos) que queremos usar para carregar os dados em paralelo.</li> </ul>"},{"location":"dataloader_class/#exemplo","title":"Exemplo:\u00b6","text":"<p>Neste exemplo, vamos criar primeiramente um objeto <code>DataLoader</code> chamado <code>sample_dataloader</code> e vamos passar como argumento o conjunto de dados que queremos carregar que chamamos de <code>sample_dataset</code> e vamos especificar um batch_size de 32 itens por \"batelada\" e sem embaralh\u00e1-los.</p> <p>Obs: N\u00f3s tamb\u00e9m podemos passar outros argumentos para o DataLoader, como por exemplo <code>sampler</code> que \u00e9 um objeto da classe <code>Sampler</code> que \u00e9 respons\u00e1vel por amostrar os dados de uma forma espec\u00edficada, al\u00e9m deste argumento podemos passar outros ainda mais complexos que sao espec\u00edficos para cada casa de uso.</p>"},{"location":"dataloader_class/#entendendo-como-o-dataloader-funciona","title":"Entendendo como o DataLoader funciona\u00b6","text":"<p>Instanciar ou criar um objeto DataLoader, nos retorna um objeto que \u00e9 um iterador sobre o conjunto de dados ou seja um iterav\u00e9l. Este iterav\u00e9l \u00e9 executado no conjunto de dados e retorna um lote de dados a cada itera\u00e7\u00e3o. Portanto, em cada iteracao, um lote itens(dados) ser\u00e1 retornado, e estes serao os dados que alimentarao a rede neural.</p> <p>Mas como o DataLoader faz isso?</p> <p>Bom, para fazer isso o <code>DataLoader</code> utiliza alguns m\u00e9todos built-in que s\u00e3o os m\u00e9todos <code>__getitem__()</code> ou <code>__iter__()</code> dependendo de como criamos ou melhor definimos l\u00e1 na classe <code>DataClass</code>. Portanto, \u00e9 necess\u00e1rio que cada <code>DataClass</code> seja ela implementada ou as fornecidas pelo core do Pytorch, \u00e9 de extrema import\u00e2ncia cada uma das <code>DataClass</code> tenha algum destes 2 m\u00e9todos implementados.</p> <p>No exemplo que vimos anteriormente podemos perceber algumas coisas que sao importantes de serem observadas sao:</p> <ol> <li>N\u00f3s criamos o objeto <code>sample_dataloader</code> para alimentar o modelo <code>sample_dataset</code>.</li> <li><code>sample_dataloader</code> \u00e9 um objeto iterador que ser\u00e1 executado em <code>sample_dataset</code>.</li> <li>Em cada iteracao de <code>sample_dataloader</code> um lote de 32 itens \u00e9 retornado. E entao este lote ir\u00e1 alimentar o modelo da rede neural.</li> <li><code>sample_dataloader</code> internamente faz uso dos m\u00e9todos <code>__getitem__()</code> ou <code>__iter__()</code> dependendo de como criamos ou melhor definimos l\u00e1 na classe <code>DataClass</code>.</li> </ol>"},{"location":"dataloader_class/#criando-um-carregador-de-dados-dataloader","title":"Criando um carregador de dados (DataLoader)\u00b6","text":"<p>Normalmente criamos os carregadores de dados(DataLoader) de maneira separadas para alimentar o modelo, mas como assim separados? Bom normalmente quando trabalhamos com Deep Learning, Machine Learning e etc, n\u00f3s particionamos ou quebramos o dataset em 3 partes que chamamos de <code>train</code>, <code>test</code> e <code>val</code> (treino, teste e validacao), isso serve justamente para ver o quao bom este modelo que acabou de ser treinado \u00e9, pois iremos ver a capacidade de generaliza\u00e7\u00e3o do modelo com dados que ele nunca viu, isso \u00e9 muito importante para otimizar o processo de treinamento e evitar o overfitting.</p> <p>Logo, ao separarmos o dataset em 3 partes, normalmente eles estarao em 3 diretorios diferentes, e sendo assim, precisamos criar um carregador para cada um destas partes do dataset e portanto, o uso de diferentes carregadores de dados neste caso \u00e9 necess\u00e1rio. Da mesma forma, embaralhar os dados pode ser \u00fatil ao treinar o modelo, mas pode n\u00e3o ser necess\u00e1rio durante os processos de valida\u00e7\u00e3o e teste, vamos ver como poderiamos fazer isso:</p>"},{"location":"dataset_class/","title":"Dataset class","text":"In\u00a0[1]: Copied! <pre>from torch.utils.data import Dataset\n</pre> from torch.utils.data import Dataset In\u00a0[5]: Copied! <pre>import os\nimport numpy as np\n\nclass myDataset(Dataset):\n    \"\"\"\n        Args:\n            root_dir (string): Diret\u00f3rio com todas as imagens\n            transform (callable, optional): Transforma\u00e7\u00f5es a serem aplicadas nas imagens\n    \"\"\"\n    \n    def __init__(self, root_dir, transform=None) -&gt; None:\n        self.root_dir = root_dir\n        self.transform = transform\n\n        # Lista todos os arquivos de imagem no diret\u00f3rio (Se sao imagens .png, .jpg, .jpeg)\n        self.image_files = [f for f in os.listdir(root_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n       \n        # Para este exemplo, vamos criar r\u00f3tulos aleat\u00f3rios\n        # Em um caso real, voc\u00ea carregaria os r\u00f3tulos de um arquivo (COCO, json, csv, etc)\n        self.labels = np.random.randint(0, 10, size=len(self.image_files))\n\n    def __getitem__(self, index):\n        # Implementar aqui a l\u00f3gica para pegar cada do item do dataset no indice index.\n        \n        # Obt\u00e9m o r\u00f3tulo correspondente\n        label = self.labels[index]\n        \n        # Aplica transforma\u00e7\u00f5es se houver\n        if self.transform:\n            image = self.transform(image)\n            \n        return image, label\n    \n    def __len__(self):\n        # Implementar aqui a l\u00f3gica para retornar o tamanho do dataset.\n         return len(self.image_files)\n    \nif __name__ == '__main__':\n    dataset = myDataset(root_dir='///', transform = None)\n</pre> import os import numpy as np  class myDataset(Dataset):     \"\"\"         Args:             root_dir (string): Diret\u00f3rio com todas as imagens             transform (callable, optional): Transforma\u00e7\u00f5es a serem aplicadas nas imagens     \"\"\"          def __init__(self, root_dir, transform=None) -&gt; None:         self.root_dir = root_dir         self.transform = transform          # Lista todos os arquivos de imagem no diret\u00f3rio (Se sao imagens .png, .jpg, .jpeg)         self.image_files = [f for f in os.listdir(root_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]                 # Para este exemplo, vamos criar r\u00f3tulos aleat\u00f3rios         # Em um caso real, voc\u00ea carregaria os r\u00f3tulos de um arquivo (COCO, json, csv, etc)         self.labels = np.random.randint(0, 10, size=len(self.image_files))      def __getitem__(self, index):         # Implementar aqui a l\u00f3gica para pegar cada do item do dataset no indice index.                  # Obt\u00e9m o r\u00f3tulo correspondente         label = self.labels[index]                  # Aplica transforma\u00e7\u00f5es se houver         if self.transform:             image = self.transform(image)                      return image, label          def __len__(self):         # Implementar aqui a l\u00f3gica para retornar o tamanho do dataset.          return len(self.image_files)      if __name__ == '__main__':     dataset = myDataset(root_dir='///', transform = None)"},{"location":"dataset_class/#a-classe-dataset-do-pytorch","title":"A classe Dataset do Pytorch\u00b6","text":"<p>A classe Dataset do Pytorch, \u00e9 a classe mais b\u00e1sica para representar e manipular um dado dataset ou conjunto de dados. Todas as outras classes de datasets, que existe no core do Pytorch, herdam da Datasetclass. Enquanto o Pytorch tamb\u00e9m fornece muitas outras classes de conjunto de dados para manipular v\u00e1rios tipos de conjuntos de dados, n\u00f3s podemos criar a nossa pr\u00f3pria classe para representar e ou manipular um dado conjunto de dados espec\u00edfico, e para isso seja poss\u00edvel basta que herdamos nesta nossa classe personalizada a Datasetclass que \u00e9 a classe priordial de datasets do Pytorch.</p> <p>Quando \u00e9 interessante criar nossa pr\u00f3pria classe de dataset ?</p> <p>\u00c9 interessante, criar sua pr\u00f3pria classe de dataset, quando os seus dados rotul\u00e1dos por exemplo, tem um implementacao espec\u00edfica ou, possui um formato que as classes do Pytorch nao dao suporte para aquele tipo espec\u00edfico de dados. Ou seja, resumindo: \u00e9 interessante criar uma classe dataset personalizada quando estamos trabalhando com um modelo o um contexto de dados muito espec\u00edfico.</p> <p>Bom, e como podemos utilizar a classe Dataset, j\u00e1 implementada no Pytorch?</p> <p>Para isso veja abaixo como pode realizar esta operacao:</p>"},{"location":"dataset_class/#criando-sua-propria-classe-dataset","title":"Criando sua pr\u00f3pria classe Dataset\u00b6","text":"<p>Anteriormente comentamos que \u00e9 poss\u00edvel criar uma classe Dataset, para lidar com um contexto espec\u00edfico dos dados do nosso modelo. Mas como fazer isso?</p> <p>Bom, para ser poss\u00edvel criar nossa pr\u00f3pria classe Dataset na pr\u00e1tica, a principal etapa \u00e9 herdar a classe primordial <code>Dataset</code> do Pytorch. Feito isso, \u00e9 muito importante utilizarmos ou melhor implementarmos, 2 dunder methods do Python sendo eles o <code>__getitem__()</code> e o <code>__len__()</code>.</p> <p>Caso, o <code>__getitem__</code> nao for implementado uma Exception, ser\u00e1 levantada e teremos um erro. o que torna ele um dunder method obrigat\u00f3rio para a implementacao da classe Dataset.</p> <p>Adicionalmente, ao <code>__getitem__</code> \u00e9 interessante utilizar o <code>__len__</code> isso se deve ao fato de que a maioria dos samplers ou os loaders de dados, utilizam deste dunder method para carregar o conjunto de dados para alimentar as arquiteturas de Rede neural o que torna ele um m\u00e9todo fundamental e muito \u00fatil por\u00e9m nao obrigat\u00f3rio.</p> <p>Perfeito, por\u00e9m o que estes \"m\u00e9todos m\u00e1gicos\" retornam ou fazem?</p> <p>O <code>__getitem__</code> deve retornar um item do seu conjunto de dados, em um indice ou posicao. Ou seja, os dados retornados devem ser uma tupla contendo (input,label). Ou seja ele cont\u00e9m a matriz ou tensor de entrada, juntamente com seu respectivo r\u00f3tulo, como se estivesse mapeando x em y. J\u00e1 o m\u00e9todo <code>__len__</code> retorna o n\u00famero de elementos ou tamanho do dataset.</p> <p>Obs: S\u00f3 devemos herdar a classe core Dataset do Pytorch caso o meu dataset seja baseado em mapa ou seja onde eu tenho uma entrada (x) e um r\u00f3tulo (y). Ou seja, se minha classe mapeia um determinado indice(r\u00f3tulo) para um tensor de entrada(item)</p>"},{"location":"dataset_class/#outras-classes-do-nucleo-do-pytorch","title":"Outras classes do n\u00facleo do Pytorch\u00b6","text":"<p>Pytorch oferece in\u00fameras outras classes para lidar com conjuntos de dados de v\u00e1rios tipos, veja</p> Classes de Dataset Particularidades <code>IterableDataset</code> <code>IterableDataset</code> \u00e9 uma subclasse de <code>Dataset</code> usada para manipular dados vindos de um fluxo. Ao contr\u00e1rio de <code>Dataset</code>, n\u00e3o \u00e9 um conjunto de dados baseado em mapa. Portanto, em vez de implementar o m\u00e9todo <code>__getitem__()</code>, voc\u00ea ter\u00e1 que implementar o m\u00e9todo <code>__iter__()</code>, que deve retornar um iterador que iterar\u00e1 sobre o conjunto de dados. Voc\u00ea pode herdar a classe <code>IterableDataset</code> para criar sua pr\u00f3pria classe que itera sobre os dados. <code>ChainDataset</code> <code>ChainDataset</code> \u00e9 uma subclasse de <code>IterableDataset</code> usada para encadear eficientemente m\u00faltiplos conjuntos de dados do tipo <code>IterableDataset</code>. Esta classe pode ser \u00fatil ao combinar/encadear <code>IterableDataset</code>s existentes de diferentes fluxos de dados. <code>BufferedShuffleDataset</code> <code>BufferedShuffleDataset</code> \u00e9 uma subclasse de <code>IterableDataset</code> usada para embaralhar itens de um <code>IterableDataset</code>. Esta classe pode ser \u00fatil quando itens de um <code>IterableDataset</code> existente precisam ser embaralhados. <code>TensorDataset</code> <code>TensorDataset</code> \u00e9 uma subclasse de <code>Dataset</code> usada para manipular conjuntos de dados na forma de um tensor. <code>ConcatDataset</code> <code>ConcatDataset</code> \u00e9 uma subclasse de <code>Dataset</code> usada para concatenar v\u00e1rios conjuntos de dados existentes. <code>Subset</code> <code>Subset</code> \u00e9 uma subclasse de <code>Dataset</code> usada para criar um conjunto de dados que \u00e9 um subconjunto do conjunto de dados original. Este subconjunto \u00e9 criado a partir de itens nos \u00edndices fornecidos no conjunto de dados original."},{"location":"gpu/","title":"Gpu","text":"In\u00a0[5]: Copied! <pre>import torch\n</pre> import torch In\u00a0[4]: Copied! <pre>vector1 = torch.tensor([1,2,3])\n\nvector2 = torch.tensor([5,8,1])\n\n\nresult = vector1 * vector2 \n\nresult\n</pre> vector1 = torch.tensor([1,2,3])  vector2 = torch.tensor([5,8,1])   result = vector1 * vector2   result Out[4]: <pre>tensor([ 5, 16,  3])</pre> In\u00a0[15]: Copied! <pre>tensor1_gpu = vector1.cuda()\n</pre> tensor1_gpu = vector1.cuda() <p>Um dos primeiros conceitos e talvez um dos mais importantes que iremos tentar reproduzir aqui \u00e9 que nao \u00e9 poss\u00edvel multiplicar, somar, subtrair ou realizar operacoes matem\u00e1ticas entre Tensores ao qual estes tensores estao em regioes de mem\u00f3ria diferentes ou seja:</p> <p>Tensor 1 est\u00e1 alocado na RAM do computador Tensor 2 est\u00e1 alocado na VRAM da GPU</p> <p>Logo, se tentarmos realizar algum tipo de operacao matem\u00e1tica ela ir\u00e1 retornar um erro veja:</p> <p><code>Expected all tensor to be on the same device, but found at least two devices cuda:0 and cpu</code></p> <p>Ou seja ela fala o seguinte, voc\u00ea est\u00e1 tentando realizar uma operacao matem\u00e1tica com elementos(tensores) que estao em regioes de mem\u00f3ria diferente e estao utilizando disposivos diferentes. Pois como abordado anteriormente, para realizarmos operacoes matem\u00e1ticas por padrao utilizamos a CPU para fazer isso, por\u00e9m na c\u00e9lula de c\u00f3digo acima utilizamos o m\u00e9todo <code>.cuda()</code> para mover o tensor <code>vector1</code> para a mem\u00f3ria da GPU e portanto para executar a operacao matem\u00e1tica <code>vector1 * vector2</code> \u00e9 preciso garantir que ambos os tensores sejam executados pelo mesmo dispositivo, ambos na CPU ou GPU e portanto eles precisam estar no mesmo \"contexto\".</p> In\u00a0[16]: Copied! <pre>tensor1_gpu * vector2\n</pre> tensor1_gpu * vector2 <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[16], line 1\n----&gt; 1 tensor1_gpu * vector2\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!</pre> In\u00a0[17]: Copied! <pre>tensor2_gpu = vector2.cuda()\n</pre> tensor2_gpu = vector2.cuda() <p>Veja que quando executamos entao agora as operacoes em tensores que estao no mesmo contexto ou melhor estao no mesmo dispositivo, nao recebemos nenhuma mensagem de erro tudo corre bem e ele retorna uma informacao: <code>device = 'cuda:0'</code> , isso nos indica que o tensor est\u00e1 guardado dentro da GPU</p> In\u00a0[18]: Copied! <pre>tensor1_gpu * tensor2_gpu\n</pre> tensor1_gpu * tensor2_gpu Out[18]: <pre>tensor([ 5, 16,  3], device='cuda:0')</pre> In\u00a0[21]: Copied! <pre>tensor3_gpu = torch.tensor([1,2,3] , device = 'cuda')\n\ntensor3_gpu\n</pre> tensor3_gpu = torch.tensor([1,2,3] , device = 'cuda')  tensor3_gpu Out[21]: <pre>tensor([1, 2, 3], device='cuda:0')</pre> <p>Logo, veja que agora, nao foi necess\u00e1rio mover nenhum tensor para a GPU, como era feito anteriormente pelo m\u00e9todo <code>.cuda()</code> agora ao criar ele ja foi informado como argumento adicional dentro do m\u00e9todo tensor:</p> <p><code>device='cuda:0'</code></p> In\u00a0[24]: Copied! <pre>if torch.cuda.is_available():\n    device = 'cuda'\nelse:\n    device = 'cpu'\n\n\nnew_gpu_tensor = torch.tensor([1,2,3,5], device = device)\n\n\nnew_gpu_tensor\n</pre> if torch.cuda.is_available():     device = 'cuda' else:     device = 'cpu'   new_gpu_tensor = torch.tensor([1,2,3,5], device = device)   new_gpu_tensor Out[24]: <pre>tensor([1, 2, 3, 5], device='cuda:0')</pre>"},{"location":"gpu/#usando-a-gpu","title":"Usando a GPU\u00b6","text":"<p>At\u00e9 o dado momento est\u00e1vamos trabalhando com a criacao dos tensores utilizando a CPU (default). Agora iremos ver como \u00e9 poss\u00edvel  trabalhar com tensores e suas operacoes utilizando a GPU.</p> <p>Uma observacao a se fazer logo no \u00ednicio \u00e9 que, s\u00f3 iremos abordar as t\u00e9nicas utilizadas com as GPUs da Nvidia, isso se deve ao fato da compatilidade com o Pytorch, entretanto \u00e9 bem prov\u00e1vel que existam estudos sobre APIs do Pytorch para GPUs AMD, por exemplo.</p> <p>Vamos criar um vetor para entender como o Pytorch funciona nas GPUs e al\u00e9m disso iremos realizar algumas manipulacoes com estes tensores.</p> <p>Um ponto interessante a se observar \u00e9 que ao criarmos os tensores (ordem 1 at\u00e9 ordem n), todos eles sao armazenados na mem\u00f3ria RAM, e ao realizarmos operacoes matem\u00e1ticas entre eles estamos utilizando a CPU para fazer isso.</p>"},{"location":"gpu/#utilizando-o-cuda","title":"Utilizando o CUDA\u00b6","text":"<p>O cuda nos permite fazer uma s\u00e9rie de coisas, quando o assunto \u00e9 GPU. Vamos exibir alguns exemplos abaixo:</p>"},{"location":"gpu/#criando-tensores-direto-na-gpu","title":"Criando tensores direto na GPU\u00b6","text":"<p>At\u00e9 agora, o que mostramos \u00e9 que criamos vetores normalmente utilizando o m\u00f3dulo <code>tensor</code> do <code>Pytorch</code> e posteriormente nos apenas movemos eles de uma regiao de mem\u00f3ria para outra regiao de mem\u00f3ria ou melhor, movemos os tensores para regiao de mem\u00f3ria de outro dispositivo sendo este a GPU, atrav\u00e9s do uso do CUDA</p> <p>Agora, vamos mostrar como \u00e9 poss\u00edvel criar os tensores diretamente na regiao de mem\u00f3ria da GPU, ou seja, nao sendo necess\u00e1rio mais mover utilizando o CUDA</p>"},{"location":"gpu/#verificando-a-disponiabilidade-do-cuda-no-ambiente-e-boas-maneiras-ao-se-trabalhar-com-o-torch-cuda","title":"Verificando a disponiabilidade do CUDA, no ambiente e boas maneiras ao se trabalhar com o torch CUDA\u00b6","text":"<p>\u00c9 muito importante, que todas essas etapas, atendam a diferentes ambientes principalmente quando estamos propondo alguma arquitetura de rede neural ou algo do tipo, logo o c\u00f3digo deve atender as particularidades do ambiente do usu\u00e1rio e para isso podemos checar se h\u00e1 GPU e torch CUDA dispon\u00edvel no ambiente:</p>"},{"location":"math_methods/","title":"Math methods","text":"<p>O Pytorch, possui um conjunto muito grande de funcoes, matem\u00e1ticas que sao extremamente \u00fateis, ao lidar ou trabalhar com tensores. Vamos ver algumas delas neste treinamento e ao final poderemos consultar o link da documentacao oficial que mostra praticamente todos os m\u00e9todos m\u00e1tematicos \u00fateis ao lidar com Tensores.</p> <p>Vamos, comecar falando sobre o m\u00e9todo <code>torch.abs()</code>, este m\u00e9todo tem o papel ou melhor dizendo retorna um tensor cujos elementos sao os valores absolutos dos elementos do tensor de entrada. Ou seja, o <code>torch.abs()</code>, nao altera o tensor original, mas sim tem o papel de criar um novo tensor para armazenar este tensor com valores absolutos.</p> <p>Uma coisa que pode nao ter ficado claro , logo de \u00ednicio absoluto, como assim absoluto?</p> <p>Bom esta d\u00favida \u00e9 bem natural e isto se deve ao fato de o nome do m\u00e9todo nao ser tao intuitivo. Sendo assim, o <code>torch.abs()</code>, basicamente nos retorna o m\u00f3dulo dos valores internos do tensor, vamos ver isso na pr\u00e1tica.</p> <p>Veja que vou criar um Tensor(2,2) ou uma matriz quadrada contendo valores negativos, justamente para comprovarmos e vermos na pr\u00e1tica o que este m\u00e9todo esta fazendo por debaixo dos panos:</p> In\u00a0[4]: Copied! <pre>import torch\n\nsquare_matrice = [[-1 , 4],[-2 , 1]]\nx = torch.tensor(square_matrice)\n\nx.shape\n</pre> import torch  square_matrice = [[-1 , 4],[-2 , 1]] x = torch.tensor(square_matrice)  x.shape Out[4]: <pre>torch.Size([2, 2])</pre> <p>Veja entao de fato que o m\u00e9todo basicamente aplicou o m\u00f3dulo e tornou os valores que eram negativos em positivos.</p> In\u00a0[6]: Copied! <pre>y = torch.abs(x)\n\ny\n</pre> y = torch.abs(x)  y Out[6]: <pre>tensor([[1, 4],\n        [2, 1]])</pre> <p>Vamos agora, abordar o m\u00e9todo <code>torch.round()</code> este m\u00e9todo ja possui um nome mais intuitivo o que facilita, o entendimento do que ele faz. Logo, o papel deste m\u00e9todo \u00e9 realizar o arredondamento dos valores internos do tensor para o inteiro mais pr\u00f3ximo, assim como o <code>torch.abs()</code> o <code>torch.round()</code>, nao modifica o tensor original, mas sim cria um outro tensor para armazenar os valores arredondados dado um tensor de entrada.</p> <p>Vamos ver na pr\u00e1tica como isso funciona, aqui vamos criar um tensor do tipo <code>torch.float32</code> para de fato vermos este comportamento acontecer:</p> In\u00a0[8]: Copied! <pre>x1 = torch.tensor([[-1.2, 4.7],[-2.9, 1.24]], dtype=torch.float32)\n\nx1\n</pre> x1 = torch.tensor([[-1.2, 4.7],[-2.9, 1.24]], dtype=torch.float32)  x1 Out[8]: <pre>tensor([[-1.2000,  4.7000],\n        [-2.9000,  1.2400]])</pre> <p>Veja que o tensor foi arredondado, de fato para o valor mais pr\u00f3ximo, seguindo as regras matem\u00e1ticas, pois veja que o elemento a11 = -1.20, ele est\u00e1 bem mais pr\u00f3ximo de 1, do que de 2 e portanto foi arredondado para a11 = -1.</p> In\u00a0[9]: Copied! <pre>y1 = torch.round(x1)\n\ny1\n</pre> y1 = torch.round(x1)  y1 Out[9]: <pre>tensor([[-1.,  5.],\n        [-3.,  1.]])</pre> <p>Uma pergunta que voc\u00ea deve estar se fazendo ap\u00f3s ver o m\u00e9todo <code>torch.round()</code> \u00e9 a seguinte. Ser\u00e1 que existe um m\u00e9todo para forcar que o tensor arredonde para baixo? e a resposta \u00e9 Sim, vamos apenas citar outros 2 m\u00e9todos, sendo o primeiro deles para servir como uma alternativa ao <code>torch.round()</code> e outro para arredondar os valores do elementos do tensor para baixo.</p> <p>Alternativa ao m\u00e9todo <code>torch.round()</code>: A alternativa que podemos utilizar \u00e9 o m\u00e9todo <code>torch.ceil()</code>, ele basicamente calcula o valor \"teto\" dos elementos do tensor que \u00e9 todos os elementos arredondados para cima.</p> <p>Arredondamento para baixo: <code>torch.floor()</code>, este m\u00e9todo arredonda os valores para um valor piso, ou seja para o valor baixo mais pr\u00f3ximo.</p> <p>Agora, vamos abordar m\u00e9todos, que basicamente sao funcoes matem\u00e1tica frequentemente utilizadas em diversos contextos ao se trabalhar com Tensores.</p> <ul> <li><code>torch.cos()</code></li> <li><code>torch.sin()</code></li> <li><code>torch.log()</code></li> <li><code>torch.sqrt()</code></li> </ul> <p>O que estes m\u00e9todos entao fazem, bom eles tem nomes muito intuitivos e fazem realmente o que o nome do m\u00e9todo indica, por\u00e9m vamos comecar comentando sobre o <code>torch.cos()</code> e <code>torch.sin()</code>. Tanto o <code>torch.sin()</code> quanto o <code>torch.cos()</code>, retornam um novo Tensor cujo elemento \u00e9 o seno, ou cosseno dos elementos internos de um dado tensor de entrada em radianos, e lembrando que boa parte dos m\u00e9todos do Pytorch, eles nao alteram o tensor original e sim retornam um novo tensor com a operacao feita. Veja um exemplo abaixo.</p> In\u00a0[16]: Copied! <pre>x = torch.tensor([1, 2, 3])\n\ny = torch.cos(x)\n\ny\n</pre> x = torch.tensor([1, 2, 3])  y = torch.cos(x)  y  Out[16]: <pre>tensor([ 0.5403, -0.4161, -0.9900])</pre> In\u00a0[12]: Copied! <pre>z = torch.sin(x)\n\nz\n</pre> z = torch.sin(x)  z Out[12]: <pre>tensor([0.8415, 0.9093, 0.1411])</pre> <p>Agora, vamos comentar de maneira mais detalhada sobre o <code>torch.log()</code> e <code>torch.sqrt()</code>. De maneira an\u00e1loga ao que vimos para o seno e cosseno, o m\u00e9todo <code>torch.log()</code> tem o papel de retornar um tensor cujo os elementos internos sao a o log ou logar\u00edtimo natural dos elementos internos de um dado tensor de entrada. E j\u00e1 o <code>torch.sqrt()</code>, retorna um tensor cujo os elementos internos \u00e9 a ra\u00edz quadrada de dos elementos de um dado tensor de entrada. Veja como isso funciona na pr\u00e1tica:</p> In\u00a0[19]: Copied! <pre>zt = torch.tensor([2,3,4,5])\n\nzraiz = torch.sqrt(zt)\n\nzraiz\n</pre> zt = torch.tensor([2,3,4,5])  zraiz = torch.sqrt(zt)  zraiz Out[19]: <pre>tensor([1.4142, 1.7321, 2.0000, 2.2361])</pre> In\u00a0[20]: Copied! <pre>zlog = torch.log(zt)\n\nzlog\n</pre> zlog = torch.log(zt)  zlog Out[20]: <pre>tensor([0.6931, 1.0986, 1.3863, 1.6094])</pre>"},{"location":"matrice_methods/","title":"Matrice methods","text":"In\u00a0[1]: Copied! <pre>import torch\n\nx = torch.ones(2,3)\n\nx_tranpose = torch.t(x)\n\nx.shape, x_tranpose.shape\n</pre> import torch  x = torch.ones(2,3)  x_tranpose = torch.t(x)  x.shape, x_tranpose.shape Out[1]: <pre>(torch.Size([2, 3]), torch.Size([3, 2]))</pre> <p>Agora, vamos ver outro m\u00e9todo que pode ser de grande ajuda ao utilizar o Pytorch que \u00e9 o <code>torch.det()</code>, este m\u00e9todo \u00e9 repons\u00e1vel por calcular do determinante de matrizes ou de tensores</p> In\u00a0[9]: Copied! <pre>matrice_x = torch.tensor([[2,4],[4,2]],dtype=float)\n\nmatrice_x.shape\n</pre> matrice_x = torch.tensor([[2,4],[4,2]],dtype=float)  matrice_x.shape Out[9]: <pre>torch.Size([2, 2])</pre> In\u00a0[11]: Copied! <pre>y_det = torch.det(matrice_x)\n\ny_det\n</pre> y_det = torch.det(matrice_x)  y_det Out[11]: <pre>tensor(-12., dtype=torch.float64)</pre> <p>Agora, vamos falar de outro m\u00e9todo muito interessante que \u00e9 o <code>torch.inverse()</code>, este m\u00e9todo como o pr\u00f3prio nome sugere, ele calcula a inversa do tensor, caso seja poss\u00edvel, pois lembrando das disciplinas de algebra linear existem restricoes, e nem toda matriz ou tensor aceita inversa. No caso do m\u00e9todo <code>torch.inverse()</code>, ele s\u00f3 realizar a inversa de tensores onde o tensor de entrada \u00e9 uma \"matriz quadrada\", onde <code>linhas = colunas</code>. Veja:</p> In\u00a0[23]: Copied! <pre>square_matrice = torch.tensor([[4.,5.,6.],[6.,3.,-1.],[2.,4.,8.]])\n\nsquare_matrice\n</pre> square_matrice = torch.tensor([[4.,5.,6.],[6.,3.,-1.],[2.,4.,8.]])  square_matrice    Out[23]: <pre>tensor([[ 4.,  5.,  6.],\n        [ 6.,  3., -1.],\n        [ 2.,  4.,  8.]])</pre> In\u00a0[24]: Copied! <pre>inversed = torch.inverse(square_matrice)\n\ninversed\n</pre> inversed = torch.inverse(square_matrice)  inversed Out[24]: <pre>tensor([[-0.9333,  0.5333,  0.7667],\n        [ 1.6667, -0.6667, -1.3333],\n        [-0.6000,  0.2000,  0.6000]])</pre>"},{"location":"matrice_methods/#metodos-de-matriz-no-pytorch","title":"M\u00e9todos de matriz no Pytorch\u00b6","text":"<p>Podemos usar tamb\u00e9m o m\u00e9todo <code>torch.t()</code>, ele \u00e9 muito parecido, com o <code>tensor.T</code>, por\u00e9m a diferenca \u00e9 que o <code>.T</code> \u00e9 um atributo do objeto tensor. J\u00e1 o <code>torch.t()</code> ele cria um novo Tensor com a transposta do tensor de entrada. Vamos ver na pr\u00e1tica:</p>"},{"location":"pytorch_seed/","title":"Pytorch seed","text":"In\u00a0[6]: Copied! <pre>import torch\n\ntorch.initial_seed()\n\nx = torch.rand(2,3)\n\nx\n</pre> import torch  torch.initial_seed()  x = torch.rand(2,3)  x Out[6]: <pre>tensor([[0.6555, 0.8742, 0.7837],\n        [0.2743, 0.0487, 0.1041]])</pre> <p>Veja que agora, ao utilizar o <code>torch.manual_seed()</code> e informar uma seed como por exemplo 42, que foi o valor usado, veremos que o Pytorch ir\u00e1 retornar um objeto, contendo o \"semeador\" da seed que \u00e9 justamente quem vai me permitir chegar sempre no mesmo tensor. Logo, veja que independente de quantas vezes eu tentar criar um tensor com o m\u00e9todo <code>torch.rand()</code>, ele sempre ir\u00e1 criar o mesmo tensor, isto ocorre justamente pelo simples fato de estarmos, \"conjelando\" ou utilizando um semeador, espec\u00edfico que nos faz saber qual \u00e9 esta pseudo-aleatoriedade.</p> In\u00a0[11]: Copied! <pre>torch.manual_seed(42)\n\nx = torch.rand(2,3)\nx\n</pre> torch.manual_seed(42)  x = torch.rand(2,3) x Out[11]: <pre>tensor([[0.8823, 0.9150, 0.3829],\n        [0.9593, 0.3904, 0.6009]])</pre>"},{"location":"pytorch_seed/#seed-ou-sementes-aleatorias","title":"Seed, ou Sementes aleat\u00f3rias\u00b6","text":"<p>Ao, se trabalhar com intelig\u00eancia artificial, Deep Learning e etc, frequentemente, \u00e9 utilizado o seed. Isto se deve pelo importante papel que o Seed, possui. O Pytorch, e outras bibliotecas como Numpy por exemplo, tem seus geradores de n\u00fameros, o Pytorch, gera n\u00fameros pseudoaleat\u00f3rios, isto significa que os n\u00fameros produzidos pelo Pytorch, podem ser previstos ou melhor re-gerados se soubermos, qual foi o <code>seed</code> que foi usado para gerar aquela sequencia de n\u00fameros pseudoaleat\u00f3rios, ou seja \u00e9 como se disermos o seguinte se soubermos como \"semear estes n\u00fameros podemos descobrir como obter os mesmos n\u00fameros\" no gerador aleat\u00f3rio. E isso \u00e9 de muita utilidade pois pode ser que queremos reproduzir, os mesmos resultados gerados anteriormente em um problema de aprendizado por reforco por exemplo, fazendo isso conseguimos remover a incerteza causada pela geracao dos n\u00fameros que seriam completamente aleat\u00f3rios a cada vez que que esse conjunto de n\u00fameros fosse gerado.</p> <p>Na pr\u00e1tica, o gerador \u00e9 \"semeado\" com um n\u00famero aleat\u00f3rio no inicio da geracao dos n\u00fameros e isso pode ser acessado, utilizando o m\u00e9todo <code>initial_seed()</code> veja:</p>"},{"location":"reduce_methods/","title":"Reduce methods","text":"In\u00a0[1]: Copied! <pre>import torch\n\nv = torch.ones(10,dtype=torch.int16)\nv\n</pre> import torch  v = torch.ones(10,dtype=torch.int16) v Out[1]: <pre>tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int16)</pre> <p>Veja que ao utilizarmos logo abaixo o m\u00e9todo <code>torch.sum()</code> e informando a ele o tensor <code>v</code> ele acessa este tensor de ordem 1, que foi gerado atrav\u00e9s de um m\u00e9todo de inicializacao neste caso o <code>torch.ones()</code> e realiza uma soma neste tensor, ou seja faz uma agregacao o transformando em um escalar de valor 10. Ou seja, realizou a agregacao e reduziu a dimensao</p> In\u00a0[2]: Copied! <pre>torch.sum(v)\n</pre> torch.sum(v) Out[2]: <pre>tensor(10)</pre> <p>Agora, vamos utilizar a mesma ideia por\u00e9m utilizando um tensor de ordem superior neste caso, um tensor de ordem 2 (matriz) e veremos que a mesma ideia feita para o tensor de ordem 1 tamb\u00e9m \u00e9 v\u00e1lida para tensores de ordem superiores</p> In\u00a0[3]: Copied! <pre>matrice_m = torch.ones(2,3,dtype=torch.int16)\nmatrice_m\n</pre> matrice_m = torch.ones(2,3,dtype=torch.int16) matrice_m Out[3]: <pre>tensor([[1, 1, 1],\n        [1, 1, 1]], dtype=torch.int16)</pre> <p>Veja que novamente, o que o m\u00e9todo <code>torch.sum</code> realizou foi literalmente somar cada elemento tanto das linhas quanto das colunas 1 a 1 e reduziu a dimensionalidade do tensor para um tensor de ordem 0, ou seja, retornou um escalar valendo 6</p> In\u00a0[4]: Copied! <pre>tensor_sum = torch.sum(matrice_m)\ntensor_sum\n</pre> tensor_sum = torch.sum(matrice_m) tensor_sum  Out[4]: <pre>tensor(6)</pre> In\u00a0[15]: Copied! <pre>random_matrice = torch.rand(2,3)\nrandom_matrice\n</pre> random_matrice = torch.rand(2,3) random_matrice Out[15]: <pre>tensor([[0.3830, 0.9119, 0.8695],\n        [0.2018, 0.0105, 0.6678]])</pre> <p>Veja que aqui neste caso, o m\u00e9todo <code>torch.sum()</code>, realizou a soma de cada elemento da primeira e da segunda linha do tensor random_matrice, pois especificamos o par\u00e2metro <code>dim</code> tire a prova real calculando:</p> In\u00a0[16]: Copied! <pre>torch.sum(random_matrice,dim = 0)\n</pre> torch.sum(random_matrice,dim = 0) Out[16]: <pre>tensor([0.5848, 0.9224, 1.5372])</pre> <p>Veja que ao somar todos os valores de cada elemento de cada linha, obtivemos o valor de 1.3356, pois questoes de precisao nos algarismos e o tamanho do tipo de dados o \u00faltimo digito da casa decimal foi arredondada. Por\u00e9m o valor de fato esta identico ao valor encontrado quando aplicamos o m\u00e9todo <code>torch.sum()</code></p> In\u00a0[64]: Copied! <pre>0.0307 + 0.5181 + 0.7868\n</pre> 0.0307 + 0.5181 + 0.7868 Out[64]: <pre>1.3356</pre> <p>Vamos, replicar a ideia por\u00e9m agora, para a dimensao 0. Veja que agora, o m\u00e9todo somou apenas os valores que estavam nas colunas dim = 0, de maneira an\u00e1loga ao que fizemos para a dimensao 1, vamos tirar a \"prova real\" para verificar a acur\u00e1cia.</p> <p>Portanto, temos exatamente os valores encontrados ao utilizar o parametro torch.sum por\u00e9m agora apenas em uma diferenca diferente o que podemos concluir que o comando est\u00e1 correto.</p> In\u00a0[17]: Copied! <pre>torch.sum(random_matrice,dim = 1)\n</pre> torch.sum(random_matrice,dim = 1)  Out[17]: <pre>tensor([2.1644, 0.8801])</pre> In\u00a0[66]: Copied! <pre>(0.0307 + 0.0635) , (0.5181 + 0.6163) , (0.7868 + 0.9788)\n</pre> (0.0307 + 0.0635) , (0.5181 + 0.6163) , (0.7868 + 0.9788) Out[66]: <pre>(0.0942, 1.1343999999999999, 1.7656)</pre> <p>Agora, vamos aplicar estes conceitos para Tensores de ordem superior ou ordem 3 e ver se as funcoes de agregacao, seguem os conceitos vistos anteriormente at\u00e9 aqui.</p> <p>Antes de comecarmos, a trabalhar com m\u00e9todos de agregacao em Tensores de ordem superior gostaria de lembrar um conceito visto em estudos anteriores</p> <p>O que sao tensores de ordem superior por exemplo um tensor de ordem 3 ?</p> <p>Vimos, que um tensor de ordem 3 em termos pr\u00e1ticos \u00e9 um vetor que carrega em si matrizes guarde este conceito pois ele ser\u00e1 importante!</p> In\u00a0[67]: Copied! <pre>tensor3d = torch.rand(2,2,3)\n\ntensor3d\n</pre> tensor3d = torch.rand(2,2,3)  tensor3d Out[67]: <pre>tensor([[[0.3254, 0.0531, 0.1916],\n         [0.3600, 0.9394, 0.0500]],\n\n        [[0.5365, 0.5318, 0.1528],\n         [0.3453, 0.2539, 0.2998]]])</pre> In\u00a0[68]: Copied! <pre>torch.sum(tensor3d, dim = 2)\n</pre> torch.sum(tensor3d, dim = 2)  Out[68]: <pre>tensor([[0.5701, 1.3494],\n        [1.2211, 0.8990]])</pre> <p>Vamos, somar manualmente para verificar se de fato a operacao est\u00e1 correta e visualizar que ele est\u00e1 somando as colunas:</p> In\u00a0[69]: Copied! <pre>(0.9441 + 0.8208 + 0.8714) , (0.3552 + 0.5126 + 0.0713) , (0.1126 + 0.4456 + 0.1654) , (0.276 + 0.4406 + 0.5306)\n\n# Logo, vimos que sim est\u00e1 sendo somado corretamente\n</pre> (0.9441 + 0.8208 + 0.8714) , (0.3552 + 0.5126 + 0.0713) , (0.1126 + 0.4456 + 0.1654) , (0.276 + 0.4406 + 0.5306)  # Logo, vimos que sim est\u00e1 sendo somado corretamente  Out[69]: <pre>(2.6363, 0.9390999999999999, 0.7236, 1.2471999999999999)</pre> <p>Agora, vamos explorar a <code>dim = 0</code> que ir\u00e1 realizar a operacao na dimensao \"3\". Pois como tinhamos abordado um tensor de ordem superior, como neste caso de ordem 3, em termos pr\u00e1ticos \u00e9 um vetor que carrega matrizes e portanto, neste caso o que ocorrer\u00e1 \u00e9 uma operacao de soma entre as matrizes vamos ver:</p> In\u00a0[70]: Copied! <pre>tensor3d\n</pre> tensor3d Out[70]: <pre>tensor([[[0.3254, 0.0531, 0.1916],\n         [0.3600, 0.9394, 0.0500]],\n\n        [[0.5365, 0.5318, 0.1528],\n         [0.3453, 0.2539, 0.2998]]])</pre> In\u00a0[71]: Copied! <pre>torch.sum(tensor3d, dim = 0)\n</pre>  torch.sum(tensor3d, dim = 0) Out[71]: <pre>tensor([[0.8619, 0.5849, 0.3444],\n        [0.7054, 1.1933, 0.3497]])</pre> <p>Com isso, vamos realizar os calculos para ver se de fato a operacao foi aplicada na dimensao correta.</p> In\u00a0[72]: Copied! <pre>(0.9441 + 0.1126) , (0.8208 + 0.4456) , (0.8714 + 0.1654)\n\n# Veja que sim, a operacao foi aplicada na dimensao correta, realizando a soma de elemento por elemento\n</pre> (0.9441 + 0.1126) , (0.8208 + 0.4456) , (0.8714 + 0.1654)  # Veja que sim, a operacao foi aplicada na dimensao correta, realizando a soma de elemento por elemento Out[72]: <pre>(1.0567, 1.2664, 1.0368)</pre> In\u00a0[73]: Copied! <pre>another_vector = torch.rand(7)\n\nanother_vector\n</pre> another_vector = torch.rand(7)  another_vector Out[73]: <pre>tensor([0.2145, 0.9650, 0.5482, 0.3012, 0.4591, 0.2867, 0.2246])</pre> In\u00a0[74]: Copied! <pre>torch.mean(another_vector)\n</pre> torch.mean(another_vector) Out[74]: <pre>tensor(0.4285)</pre> <p>Vamos entao testar de maneira \"brute force\" para ver se de fato a operacao esta correta:</p> In\u00a0[75]: Copied! <pre>(0.8636 + 0.0433 + 0.0769 + 0.6007 + 0.6226 + 0.4043 + 0.2674) / 7\n\n#Portanto, de fato est\u00e1 correto a operacao utilizando o m\u00e9todo mean()\n</pre> (0.8636 + 0.0433 + 0.0769 + 0.6007 + 0.6226 + 0.4043 + 0.2674) / 7  #Portanto, de fato est\u00e1 correto a operacao utilizando o m\u00e9todo mean() Out[75]: <pre>0.4112571428571429</pre> <p>Agora, vamos realizar a aplicacao do <code>torch.mean()</code>, por\u00e9m para tensores de ordem 2. E vamos utilizar o parametro <code>dim</code>, que vimos anteriormente para que o m\u00e9todo seja aplicado para a dimensao determinada como tamb\u00e9m j\u00e1 foi visto anteriormente.</p> In\u00a0[76]: Copied! <pre>another_matrice = torch.rand(2,3)\nanother_matrice\n</pre> another_matrice = torch.rand(2,3) another_matrice Out[76]: <pre>tensor([[0.2118, 0.7054, 0.9934],\n        [0.8409, 0.3987, 0.9036]])</pre> In\u00a0[77]: Copied! <pre>torch.mean(another_matrice, dim = 1)\n</pre> torch.mean(another_matrice, dim = 1) Out[77]: <pre>tensor([0.6369, 0.7144])</pre> In\u00a0[\u00a0]: Copied! <pre># Testando:\n\n# o dim = 1, como visto representa as colunas, portanto:\n\n# C\u00e1lculo do primeiro elemento\na11 = (0.7593 + 0.8597 + 0.5941) / 3\n\n# C\u00e1lculo do segundo elemento\na12 = (0.8635 + 0.3544 + 0.4002) / 3\n\nprint(a11,a12)\n</pre> # Testando:  # o dim = 1, como visto representa as colunas, portanto:  # C\u00e1lculo do primeiro elemento a11 = (0.7593 + 0.8597 + 0.5941) / 3  # C\u00e1lculo do segundo elemento a12 = (0.8635 + 0.3544 + 0.4002) / 3  print(a11,a12) <pre>0.7376999999999999 0.5393666666666667\n</pre> <p>Portanto, veja que os mesmos conceitos como por exemplo: uso do <code>dim</code> que utilizamos para o <code>torch.sum()</code>, tamb\u00e9m funcionam para os demais m\u00e9todos como por exemplo neste caso o <code>torch.mean()</code></p> In\u00a0[79]: Copied! <pre>another_matrice\n</pre> another_matrice Out[79]: <pre>tensor([[0.2118, 0.7054, 0.9934],\n        [0.8409, 0.3987, 0.9036]])</pre> In\u00a0[80]: Copied! <pre>torch.mean(another_matrice, dim = 0)\n</pre> torch.mean(another_matrice, dim = 0) Out[80]: <pre>tensor([0.5264, 0.5521, 0.9485])</pre> In\u00a0[\u00a0]: Copied! <pre># Testando: \n# dim = 0 , representa as colunas:\n\na11 = (0.7593 + 0.8635) / 2\na12 = (0.8597 + 0.3544) / 2\na13 = (0.5941 + 0.4002) / 2\n\nprint(a11,a12,a13)\n</pre> # Testando:  # dim = 0 , representa as colunas:  a11 = (0.7593 + 0.8635) / 2 a12 = (0.8597 + 0.3544) / 2 a13 = (0.5941 + 0.4002) / 2  print(a11,a12,a13) <pre>0.8114 0.60705 0.49715\n</pre> In\u00a0[82]: Copied! <pre>another_tensor = torch.rand(2,2,3)\nanother_tensor\n</pre> another_tensor = torch.rand(2,2,3) another_tensor Out[82]: <pre>tensor([[[0.0260, 0.6126, 0.7707],\n         [0.7533, 0.9096, 0.1928]],\n\n        [[0.7389, 0.7655, 0.2077],\n         [0.2560, 0.9505, 0.2074]]])</pre> In\u00a0[83]: Copied! <pre>torch.mean(another_tensor, dim = 0)\n</pre> torch.mean(another_tensor, dim = 0) Out[83]: <pre>tensor([[0.3824, 0.6890, 0.4892],\n        [0.5047, 0.9300, 0.2001]])</pre> In\u00a0[84]: Copied! <pre>torch.mean(another_tensor, dim = 1)\n</pre> torch.mean(another_tensor, dim = 1) Out[84]: <pre>tensor([[0.3896, 0.7611, 0.4817],\n        [0.4975, 0.8580, 0.2075]])</pre> In\u00a0[85]: Copied! <pre>torch.mean(another_tensor, dim = 2)\n</pre> torch.mean(another_tensor, dim = 2) Out[85]: <pre>tensor([[0.4697, 0.6186],\n        [0.5707, 0.4713]])</pre> In\u00a0[10]: Copied! <pre>matrix4 = torch.rand(2,3)\n\nmatrix4\n</pre> matrix4 = torch.rand(2,3)  matrix4 Out[10]: <pre>tensor([[0.7485, 0.3832, 0.5739],\n        [0.8960, 0.9712, 0.3480]])</pre> <p>Veja que aqui nao informamos a dimensao, entao de maneira geral o m\u00e9todo <code>torch.max</code> procurou pelo maior valor entre todos os valores do Tensor e nos retornou.</p> In\u00a0[11]: Copied! <pre>torch.max(matrix4)\n</pre> torch.max(matrix4) Out[11]: <pre>tensor(0.9712)</pre> In\u00a0[12]: Copied! <pre>torch.max(matrix4, dim = 0)\n</pre> torch.max(matrix4, dim = 0) Out[12]: <pre>torch.return_types.max(\nvalues=tensor([0.8960, 0.9712, 0.5739]),\nindices=tensor([1, 1, 0]))</pre> In\u00a0[19]: Copied! <pre>index, values = torch.max(matrix4, dim = 0)\n\nindex, values\n</pre> index, values = torch.max(matrix4, dim = 0)  index, values Out[19]: <pre>(tensor([0.8960, 0.9712, 0.5739]), tensor([1, 1, 0]))</pre> <p>Vamos, ver como isso funciona na <code>dim=1</code></p> In\u00a0[20]: Copied! <pre>index, values = torch.max(matrix4, dim = 1)\n\nindex, values\n</pre> index, values = torch.max(matrix4, dim = 1)  index, values Out[20]: <pre>(tensor([0.7485, 0.9712]), tensor([0, 1]))</pre> In\u00a0[3]: Copied! <pre>t = torch.rand(2,2,3)\n\nt\n</pre> t = torch.rand(2,2,3)  t Out[3]: <pre>tensor([[[0.1781, 0.4586, 0.5749],\n         [0.3675, 0.2402, 0.2650]],\n\n        [[0.6796, 0.1474, 0.3033],\n         [0.5824, 0.4717, 0.6007]]])</pre> <p>Vimos at\u00e9 entao que nos tensores de ordem menor que 3, o parametro <code>dim</code> pode ser <code>dim=0</code> e <code>dim=1</code>. Isso informa para o Pytorch que neste caso \u00e9 para aplicar o m\u00e9todo de reducao em questao na dimensao especificada onde <code>dim = 0</code>, aplica o m\u00e9todo na dimensao das linhas e <code>dim = 1</code>, aplica o m\u00e9todo na dimensao na dimensao das colunas. Por\u00e9m, agora ao trabalharmos com tensores de ordem 3 ou superior, estamos de fato tendo que lidar com um Tensor propriamente dito, ou para simplificar estamos lidando com um \"vetor de matrizes\". E sendo assim, agora nos tensores de ordem 3, estamos lidando com uma dimensao a mais, com isso nosso parametro <code>dim</code> tamb\u00e9m \"ganha\" uma dimensao a mais e temos o seguinte:</p> <ul> <li><code>dim = 0</code>: Aplica o m\u00e9todo na dimensao das matrizes</li> <li><code>dim = 1</code>: Aplica o m\u00e9todo na dimensao das linhas</li> <li><code>dim = 2</code>: Aplica o m\u00e9todo na dimensao das colunas</li> </ul> <p>Para que isso fique um pouco mais claro de entender, \u00e9 que nos Tensores de ordem maior que 3, o valor 0 est\u00e1 associado com a primeira dimensao que neste caso sao as \"matrizes\", o valor 1 est\u00e1 ligado com as linhas das matrizes em questao e o valor 2 fica ligado com as colunas e portanto, temos est\u00e1 pequena diferenca e podemos fazer inclusive uma analogia com o eixo de coordenadas x,y,z , pois \u00e9 como se tivessemos controlando o eixo: $$x = 0; y = 1;  z = 2; $$</p> In\u00a0[5]: Copied! <pre>values , index = torch.max(t, dim = 2)\n</pre> values , index = torch.max(t, dim = 2) In\u00a0[8]: Copied! <pre>t\n</pre> t Out[8]: <pre>tensor([[[0.1781, 0.4586, 0.5749],\n         [0.3675, 0.2402, 0.2650]],\n\n        [[0.6796, 0.1474, 0.3033],\n         [0.5824, 0.4717, 0.6007]]])</pre> In\u00a0[6]: Copied! <pre>values\n</pre> values Out[6]: <pre>tensor([[0.5749, 0.3675],\n        [0.6796, 0.6007]])</pre> In\u00a0[7]: Copied! <pre>index\n</pre> index Out[7]: <pre>tensor([[2, 0],\n        [0, 2]])</pre> <p>Entao veja que aqui ele est\u00e1 comparando elemento por elemento de cada linha e vendo qual deles \u00e9 maior</p> In\u00a0[15]: Copied! <pre>t\n</pre> t Out[15]: <pre>tensor([[[0.1781, 0.4586, 0.5749],\n         [0.3675, 0.2402, 0.2650]],\n\n        [[0.6796, 0.1474, 0.3033],\n         [0.5824, 0.4717, 0.6007]]])</pre> In\u00a0[11]: Copied! <pre>values,index = torch.max(t, dim = 1)\n</pre> values,index = torch.max(t, dim = 1) In\u00a0[13]: Copied! <pre>values\n</pre> values Out[13]: <pre>tensor([[0.3675, 0.4586, 0.5749],\n        [0.6796, 0.4717, 0.6007]])</pre> In\u00a0[14]: Copied! <pre>index\n</pre> index Out[14]: <pre>tensor([[1, 0, 0],\n        [0, 1, 1]])</pre> <p>J\u00e1 aqui veja que ele est\u00e1 comparando elemento por elemento de cada uma das matrizes e vendo qual \u00e9 o maior</p> In\u00a0[16]: Copied! <pre>t\n</pre> t Out[16]: <pre>tensor([[[0.1781, 0.4586, 0.5749],\n         [0.3675, 0.2402, 0.2650]],\n\n        [[0.6796, 0.1474, 0.3033],\n         [0.5824, 0.4717, 0.6007]]])</pre> In\u00a0[17]: Copied! <pre>values,index = torch.max(t, dim = 0)\n</pre> values,index = torch.max(t, dim = 0) In\u00a0[18]: Copied! <pre>values\n</pre> values Out[18]: <pre>tensor([[0.6796, 0.4586, 0.5749],\n        [0.5824, 0.4717, 0.6007]])</pre> In\u00a0[19]: Copied! <pre>index\n</pre> index Out[19]: <pre>tensor([[1, 0, 0],\n        [1, 1, 1]])</pre> In\u00a0[21]: Copied! <pre>t2 = torch.rand(2,2,3)\n\nt2\n</pre> t2 = torch.rand(2,2,3)  t2 Out[21]: <pre>tensor([[[0.0557, 0.1917, 0.2341],\n         [0.6657, 0.9000, 0.5472]],\n\n        [[0.1237, 0.9808, 0.5441],\n         [0.7009, 0.8810, 0.6214]]])</pre> <p>Perceba, que se eu nao informar nenhuma dimensao ou seja nao informar o parametro <code>dim</code> o m\u00e9todo <code>torch.min()</code> ir\u00e1 retornar o menor valor do Tensor entre todos independente das dimensoes do Tensor, isso vale para qualquer um dos m\u00e9todos de reducao como por exemplo: <code>torch.sum()</code>, <code>torch.max()</code> e etc.</p> In\u00a0[22]: Copied! <pre>torch.min(t2)\n</pre> torch.min(t2) Out[22]: <pre>tensor(0.0557)</pre> <p>Veja que no momento em que uma dimensao \u00e9 informada, ele ir\u00e1 realizar e procurar o menor valor daquela dimensao exatamente igual vimos, quando trabalhamos com <code>torch.max()</code></p> In\u00a0[23]: Copied! <pre>values, index = torch.min(t2, dim = 2)\n</pre> values, index = torch.min(t2, dim = 2) In\u00a0[24]: Copied! <pre>values\n</pre> values Out[24]: <pre>tensor([[0.0557, 0.5472],\n        [0.1237, 0.6214]])</pre> In\u00a0[25]: Copied! <pre>index\n</pre> index Out[25]: <pre>tensor([[0, 2],\n        [0, 2]])</pre> In\u00a0[26]: Copied! <pre>values, index = torch.max(t2, dim = 1)\n</pre> values, index = torch.max(t2, dim = 1) In\u00a0[27]: Copied! <pre>values, index\n</pre> values, index Out[27]: <pre>(tensor([[0.6657, 0.9000, 0.5472],\n         [0.7009, 0.9808, 0.6214]]),\n tensor([[1, 1, 1],\n         [1, 0, 1]]))</pre> In\u00a0[28]: Copied! <pre>values, index = torch.min(t2, dim = 0)\n</pre> values, index = torch.min(t2, dim = 0) In\u00a0[29]: Copied! <pre>values, index\n</pre> values, index Out[29]: <pre>(tensor([[0.0557, 0.1917, 0.2341],\n         [0.6657, 0.8810, 0.5472]]),\n tensor([[0, 0, 0],\n         [0, 1, 0]]))</pre> <p>O <code>torch.argmax()</code> tem um funcionamento e l\u00f3gica muito semelhante, ao funcionamento do <code>torch.max()</code>, por\u00e9m a diferenca existente entre eles \u00e9 que o <code>torch.max()</code> retorna os maiores valores jutamente com os indices ou melhor as posicoes onde est\u00e1 localizado estes maiores valores e j\u00e1 no <code>torch.argmax()</code> o retorno que temos como resultado \u00e9 apenas as posicoes de onde estao os maiores valores neste Tensor. Assim como no <code>torch.max()</code>, podemos realizar a busca por este maior valor atrav\u00e9s de uma dimensao especifica, como ja vimos passando o parametro adicional <code>dim = 0</code> , <code>dim = 1</code> e <code>dim = 2</code></p> In\u00a0[3]: Copied! <pre>t3 = torch.rand(2,2,3)\n\nt3\n</pre> t3 = torch.rand(2,2,3)  t3 Out[3]: <pre>tensor([[[0.9311, 0.6232, 0.2368],\n         [0.4186, 0.9345, 0.9238]],\n\n        [[0.7209, 0.4918, 0.0780],\n         [0.7135, 0.0741, 0.5789]]])</pre> In\u00a0[8]: Copied! <pre>torch.argmax(t3, dim = 0)\n</pre> torch.argmax(t3, dim = 0) Out[8]: <pre>tensor([[0, 0, 0],\n        [1, 0, 0]])</pre> In\u00a0[6]: Copied! <pre>torch.argmax(t3, dim = 1)\n</pre> torch.argmax(t3, dim = 1) Out[6]: <pre>tensor([[0, 1, 1],\n        [0, 0, 1]])</pre> In\u00a0[5]: Copied! <pre>torch.argmax(t3, dim = 2)\n</pre> torch.argmax(t3, dim = 2) Out[5]: <pre>tensor([[0, 1],\n        [0, 0]])</pre> <p>O <code>torch.argmin()</code> tamb\u00e9m \u00e9 exatamente o mesmo m\u00e9todo, que o <code>torch.argmax()</code> por\u00e9m com a diferenca de que a l\u00f3gica aqui \u00e9 que teremos como retorno a posicao(index) do menor valor dentro do vetor.</p> In\u00a0[9]: Copied! <pre>t3\n</pre> t3 Out[9]: <pre>tensor([[[0.9311, 0.6232, 0.2368],\n         [0.4186, 0.9345, 0.9238]],\n\n        [[0.7209, 0.4918, 0.0780],\n         [0.7135, 0.0741, 0.5789]]])</pre> In\u00a0[10]: Copied! <pre>torch.argmin(t3, dim = 0)\n</pre> torch.argmin(t3, dim = 0) Out[10]: <pre>tensor([[1, 1, 1],\n        [0, 1, 1]])</pre> In\u00a0[11]: Copied! <pre>torch.argmin(t3, dim = 1)\n</pre> torch.argmin(t3, dim = 1) Out[11]: <pre>tensor([[1, 0, 0],\n        [1, 1, 0]])</pre> In\u00a0[12]: Copied! <pre>torch.argmin(t3, dim = 2)\n</pre> torch.argmin(t3, dim = 2) Out[12]: <pre>tensor([[2, 0],\n        [2, 1]])</pre>"},{"location":"reduce_methods/#funcoes-de-reducao-utilizadas-no-pytorch","title":"\ud83e\udd47 Funcoes de reducao utilizadas no Pytorch\u00b6","text":"<p>No Pytorch as funcoes de reducao, sao utilizadas para reduzir a dimensionalidade dos tensores, agregando valores ao longo de uma ou mais dimensoes.</p> <p>No Pytorch, t\u00eam-se uma s\u00e9rie de m\u00e9todos, entretanto vamos nos concentrar em alguns dos mais utilizados no dia a dia ao se trabalhar com Deep Learning</p>"},{"location":"reduce_methods/#o-parametro-dim-dimension","title":"\ud83d\udd0e O parametro dim (dimension)\u00b6","text":"<p>Antes de ja irmos direto para os m\u00e9todos \u00e9 necess\u00e1rio antes abordar um conceito importantissimo ao se trabalhar com funcoes de reducao no Pytorch que \u00e9 o dimension.</p> <p>Logo, ao trabalhar com m\u00e9todos de reducao no Pytorch, o parametro <code>dim=&lt;value&gt;</code>, serve para auxiliar o usu\u00e1rio pois ao utiliz\u00e1-lo, \u00e9 necess\u00e1rio informar em qual ou para qual dimensao o m\u00e9todo deve ser aplicado.</p> <p>O par\u00e2metro <code>dim</code> representa literalmente a dimensao e portanto, ao utilizarmos precisamos informar passando um n\u00famero inteiro que representa a dimensao.</p> <p>Um conceito muito importante, em relacao ao <code>dim</code> \u00e9 que os valores devem ser n\u00fameros inteiros, que sempre iniciam em 0 e esses valores, podem aumentar sequencialmente, ao passo que o Tensor aumenta a sua ordem.</p> <p>Outro conceito importante \u00e9 que sempre o maior valor de <code>dim</code>, sempre ser\u00e1 relativo as colunas, e os demais valores serao responsav\u00e9is pelas linhas, independente da ordem do tensor que se estiver trabalhando.</p> <p>Sendo assim, as dimensoes em vetores de ordem 2, sao n\u00fameradas por:</p> <ul> <li>0 = Representa as Linhas</li> <li>1 = Representa as Colunas</li> </ul> <p>Para facilitar um pouco o entendimento, pense que quando voce delimita <code>dim=0</code> voce est\u00e1 aplicando o m\u00e9todo de reducao no eixo Y. J\u00e1 quando voce delimita <code>dim=1</code> voce est\u00e1 aplicando o m\u00e9todo no eixo X.</p> <p>Vamos entao ver como trabalhar com um tensores de ordem 2 (matriz) para verificar como que esses m\u00e9todos de agregacao funcionam nestes tensores:</p>"},{"location":"reduce_methods/#1-torchsum","title":"1. torch.sum():\u00b6","text":"<p>O m\u00e9todo <code>torch.sum()</code>, basicamente faz o que o nome exprime, calcula a soma de todos os elementos de um Tensor ao longo de uma dimensao que \u00e9 passada por argumento</p>"},{"location":"reduce_methods/#2-torchmean","title":"2. torch.mean():\u00b6","text":"<p>O m\u00e9todo <code>torch.mean()</code>, basicamente faz o que o nome exprime, calcula a m\u00e9dia de todos os elementos de um Tensor ao longo de uma dimensao especificada</p>"},{"location":"reduce_methods/#3-torchmax","title":"3. torch.max():\u00b6","text":"<p>O m\u00e9todo <code>torch.max()</code>, nos mostra qual \u00e9 o maior valor presente dentro de um tensor. Retornando isso, atrav\u00e9s do seu valor ou da sua posicao(index). Al\u00e9m disso, como ja visto nos m\u00e9todos anteriores como <code>torch.sum()</code> e <code>torch.mean()</code>, aqui com o <code>torch.max()</code> tamb\u00e9m podemos especificar em qual dimensao, queremos que o m\u00e9todo seja aplicado.</p> <p>Um ponto importante a ser salientado \u00e9 que quando estamos trabalhando com os m\u00e9todos de reducao, eles verificam elemento por elemento aplicando suas operacoes, entao por exemplo no m\u00e9todo <code>torch.max()</code> ele ir\u00e1 sair comparando elemento a elemento para verificar qual deles \u00e9 maior. E isso vale tamb\u00e9m para quando passamos parametros adicionais como por exemplo <code>torch.max(dim='0 ou 1')</code> Ou seja, quando delimitamos a dimensao o que ir\u00e1 acontecer \u00e9 que o m\u00e9todo ir\u00e1 aplicar sua l\u00f3gica neste caso \"filtrar\" pelo elemento de maior valor e para isso ele ir\u00e1 comparar elemento por elemento na dimensao informada, se <code>dim=0</code> ele ir\u00e1 comparar elemento por elemento presente nas linhas e retornar\u00e1 o valor e a respectiva posicao dos maiores elementos.</p>"},{"location":"reduce_methods/#aplicando-torchmax-em-tensores-de-ordem-2-ou-matrizes","title":"Aplicando torch.max() em tensores de ordem 2 ou matrizes\u00b6","text":""},{"location":"reduce_methods/#aplicando-o-torchmax-em-tensores-de-ordem-3-ou-superior","title":"Aplicando o torch.max() em tensores de ordem 3 ou superior\u00b6","text":""},{"location":"reduce_methods/#testando-a-dimensao-dim-2-que-controla-as-colunas-em-tensores-de-ordem-superior","title":"Testando a dimensao <code>dim = 2</code> que controla as colunas em tensores de ordem superior\u00b6","text":""},{"location":"reduce_methods/#testando-a-dimensao-dim-1-que-controla-as-linhas-em-tensores-de-ordem-superior","title":"Testando a dimensao <code>dim = 1</code> que controla as linhas em tensores de ordem superior\u00b6","text":""},{"location":"reduce_methods/#testando-a-dimensao-dim-0-que-controla-as-matrizes-em-tensores-de-ordem-superior","title":"Testando a dimensao <code>dim = 0</code> que controla as matrizes em tensores de ordem superior\u00b6","text":""},{"location":"reduce_methods/#4-torchmin","title":"4. torch.min():\u00b6","text":"<p>O <code>torch.min()</code> tem o pr\u00edncipio de funcinamento exatamente igual ao <code>torch.max()</code> visto anteriormente, a \u00fanica diferenca \u00e9 que o <code>torch.min()</code> ir\u00e1 retornar o menor valor do Tensor ou ir\u00e1 aplicar o m\u00e9todo e retornar\u00e1 os menores valores de um tensor em uma dada dimensao.</p>"},{"location":"reduce_methods/#5-torchargmax","title":"5. torch.argmax():\u00b6","text":""},{"location":"reduce_methods/#6-torchargmin","title":"6. torch.argmin():\u00b6","text":""},{"location":"save_load_tensors/","title":"Save load tensors","text":"In\u00a0[9]: Copied! <pre>import torch\n\nm = torch.randn(2,2)\n\nm\n</pre> import torch  m = torch.randn(2,2)  m   Out[9]: <pre>tensor([[-0.3296,  0.1876],\n        [-0.7432, -1.1214]])</pre> In\u00a0[10]: Copied! <pre>torch.save(m,'./tensor.pt')\n</pre> torch.save(m,'./tensor.pt') In\u00a0[11]: Copied! <pre>torch.load('./tensor.pt')\n</pre> torch.load('./tensor.pt') Out[11]: <pre>tensor([[-0.3296,  0.1876],\n        [-0.7432, -1.1214]])</pre>"},{"location":"slice_tensors/","title":"Slice tensors","text":"In\u00a0[1]: Copied! <pre>import torch\nvector = torch.tensor([1,2,3,4,5,6])\n</pre> import torch vector = torch.tensor([1,2,3,4,5,6]) <pre>C:\\Users\\Joao\\AppData\\Local\\Temp\\ipykernel_15712\\109296347.py:2: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n  vector = torch.tensor([1,2,3,4,5,6])\n</pre> In\u00a0[2]: Copied! <pre>vector.shape\n</pre> vector.shape Out[2]: <pre>torch.Size([6])</pre> In\u00a0[3]: Copied! <pre>vector[5]\n</pre> vector[5] Out[3]: <pre>tensor(6)</pre> In\u00a0[4]: Copied! <pre>matrice = torch.tensor([[1,2,3],[4,5,6]])\n\nmatrice\n</pre> matrice = torch.tensor([[1,2,3],[4,5,6]])  matrice Out[4]: <pre>tensor([[1, 2, 3],\n        [4, 5, 6]])</pre> In\u00a0[5]: Copied! <pre>matrice.shape\n</pre> matrice.shape Out[5]: <pre>torch.Size([2, 3])</pre> In\u00a0[6]: Copied! <pre>matrice[0]\n</pre> matrice[0] Out[6]: <pre>tensor([1, 2, 3])</pre> In\u00a0[7]: Copied! <pre>matrice[0][2]\n</pre> matrice[0][2] Out[7]: <pre>tensor(3)</pre> In\u00a0[8]: Copied! <pre>matrice\n</pre> matrice Out[8]: <pre>tensor([[1, 2, 3],\n        [4, 5, 6]])</pre> <p>O slicing \u00e9 uma maneira de fatiar filtrar ou at\u00e9 mesmo pode ser interpretado como uma maneira de buscar elementos em container ou melhor em colecoes como \u00e9 chamado no Python.</p> <p>Ao se trabalhar com Tensores esta t\u00e9cnica e extremamente utilizada e portanto \u00e9 de extrema import\u00e2ncia ter um bom d\u00f4minio desta ferramenta</p> <p>Quando trabalhamos com Tensores de ordem 2 ou superior utilizamos a seguinte notacao: $$[:,:]$$</p> <ul> <li>os primeiros dois pontos percorrem ou melhor buscam tudo referente a linhas</li> <li>os \u00faltimos dois pontos sao responsaveis por percorrer tudo referente as colunas</li> </ul> In\u00a0[9]: Copied! <pre>matrice[1:,2:]\n</pre> matrice[1:,2:] Out[9]: <pre>tensor([[6]])</pre> In\u00a0[10]: Copied! <pre>tensor = torch.tensor([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]])\n</pre> tensor = torch.tensor([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]]) In\u00a0[11]: Copied! <pre>tensor\n</pre> tensor Out[11]: <pre>tensor([[[ 1,  2,  3],\n         [ 4,  5,  6]],\n\n        [[ 7,  8,  9],\n         [10, 11, 12]]])</pre> In\u00a0[12]: Copied! <pre>tensor.shape\n</pre> tensor.shape Out[12]: <pre>torch.Size([2, 2, 3])</pre> In\u00a0[13]: Copied! <pre>tensor[0][1][0]\n</pre> tensor[0][1][0] Out[13]: <pre>tensor(4)</pre> In\u00a0[14]: Copied! <pre>tensor[:,:,:]\n</pre> tensor[:,:,:] Out[14]: <pre>tensor([[[ 1,  2,  3],\n         [ 4,  5,  6]],\n\n        [[ 7,  8,  9],\n         [10, 11, 12]]])</pre>"},{"location":"slice_tensors/#acessando-elementos-pelo-indice-em-tensores","title":"Acessando elementos pelo indice em Tensores\u00b6","text":""},{"location":"slice_tensors/#realizando-slicing-em-tensores-de-ordem-2-para-acessar-os-indices","title":"Realizando slicing em Tensores de ordem 2 para acessar os indices\u00b6","text":""},{"location":"slice_tensors/#tensores-de-ordem-3","title":"Tensores de ordem 3\u00b6","text":"<p>Ao se trabalhar com tensores de ordem 3, temos tudo com uma dimensao a mais como vimos nos primeiros tutorias os tensores podem ser de ordem 0 at\u00e9 ordem n.</p> <p>Logo, como comentamos na pr\u00e1tica os tensores de ordem 3 por exemplo em termos pr\u00e1ticos ele \u00e9 como se fosse um vetor ou melhor um tensor de ordem 1 que carrega matrizes ou seja tensores de ordem 2 e assim sucessivamente.</p> <p>Logo, em termos pr\u00e1ticos os tensores serao uma estrutura n-2 que carrega em si um conjunto de estruturas n-1, onde n \u00e9 a ordem do Tensor</p> <p>Portanto, como os tensores seguem essa \"hierarquia\" ou melhor estrutura, eles tamb\u00e9m recebem as propriedades e seguem a mesma maneira de manipulacao de suas estruturas, e sendo assim o tensores de ordem 3 tamb\u00e9m respeitam as mesmas propriedades de \"varredura\" e slicing dos tensores de ordem 1 e 2 por\u00e9m com uma dimensao a mais:</p>"},{"location":"tensor_transpose/","title":"Tensor transpose","text":"In\u00a0[\u00a0]: Copied! <pre>import torch\n\nt = torch.rand(2,3)\n\nt\n</pre> import torch  t = torch.rand(2,3)  t Out[\u00a0]: <pre>tensor([[0.6398, 0.6448, 0.9003],\n        [0.6457, 0.4787, 0.6847]])</pre> <p>Entao, podemos realizar, a transposicao de um tensor simplesmente, utilizando o m\u00e9todo `tensor.T. Utilizamos este m\u00e9todo normalmente quando estamos trabalhando com tensores de ordem 2. Ou, quando queremos rapidamente realizar a transposta deste tensor de maneira r\u00e1pida ou quando nao \u00e9 exigido transpor uma dimensao espec\u00edfica. Lembrando que a operacao transposta e trocar o que \u00e9 linha por coluna ou vice-versa de um tensor ou matriz</p> In\u00a0[5]: Copied! <pre>t.shape\n</pre> t.shape Out[5]: <pre>torch.Size([2, 3])</pre> In\u00a0[8]: Copied! <pre>r = t.T\nprint(r)\nr.shape\n</pre> r = t.T print(r) r.shape <pre>tensor([[0.6398, 0.6457],\n        [0.6448, 0.4787],\n        [0.9003, 0.6847]])\n</pre> Out[8]: <pre>torch.Size([3, 2])</pre> <p>Agora, pode ser que em algum momento ao se trabalhar com tensores que se queira realizar a transposicao de alguma dimensao em espec\u00edfico, e neste caso o m\u00e9todo mais adequado \u00e9 o <code>torch.tranpose()</code> que ser justamente para realizar a operacao de transposicao em uma determinada dimensao informada. Mas como esse cara funciona?</p> <p>Bom o <code>torch.transpose()</code> funciona, justamente voce informando 3 par\u00e2metros:</p> <ul> <li>tensor;</li> <li>dimensao que vai virar linha;</li> <li>dimensao que vai virar coluna;</li> </ul> <p>Vamos observar um exemplo na pr\u00e1tica que ficar\u00e1 mais simples de entender:</p> In\u00a0[10]: Copied! <pre>tensor2 = torch.rand(2,2,3)\n\ntensor2\n</pre> tensor2 = torch.rand(2,2,3)  tensor2 Out[10]: <pre>tensor([[[0.1792, 0.4405, 0.5630],\n         [0.4977, 0.3212, 0.1692]],\n\n        [[0.5843, 0.0139, 0.7012],\n         [0.5323, 0.6129, 0.1417]]])</pre> In\u00a0[11]: Copied! <pre>tensor2.shape\n</pre> tensor2.shape Out[11]: <pre>torch.Size([2, 2, 3])</pre> <p>Veja enta que eu informei o tensor, a qual iriamos aplicar esta operacao de transposicao e as dimensoes deste tensor, informando a dimensao que vai virar linha e a dimensao que vai virar coluna ou em outras palavras passamos as dimensoes que irao sofrer transposicao.</p> In\u00a0[16]: Copied! <pre>r2 = torch.transpose(tensor2, 2, 0)\n\nr2\n</pre> r2 = torch.transpose(tensor2, 2, 0)  r2 Out[16]: <pre>tensor([[[0.1792, 0.5843],\n         [0.4977, 0.5323]],\n\n        [[0.4405, 0.0139],\n         [0.3212, 0.6129]],\n\n        [[0.5630, 0.7012],\n         [0.1692, 0.1417]]])</pre> In\u00a0[14]: Copied! <pre>r2.shape\n</pre> r2.shape Out[14]: <pre>torch.Size([3, 2, 2])</pre> <p>Uma outra maneira que podemos realizar a transposicao de tensores utilizando o <code>PyTorch</code> \u00e9 usar o m\u00e9todo <code>torch.permute</code>. O <code>torch.permute</code> tem um pr\u00edncipio de funcionamento e aplicacao muito parecido com o <code>torch.transpose()</code>. Entretano, o <code>torch.permute</code> \u00e9 utilizado quando quero realizar a transposicao informando mais de uma dimensao por vez ou seja, al\u00e9m disso ele \u00e9 muito idicado para quando estamos trabalhando com tensores de ordem superior a ordem3, entretanto cada profissional usa estes 3 m\u00e9todos da maneira como acha melhor e isso \u00e9 pessoal, pois ambos realizam a operacao de transposicao dos tensores perfeitamente.</p> <p>Logo, vamos ver na pr\u00e1tica e entender melhor como funciona o m\u00e9todo <code>torch.permute()</code></p> In\u00a0[19]: Copied! <pre>tensor3 = torch.rand(2,3,4)\n\ntensor3\n</pre> tensor3 = torch.rand(2,3,4)  tensor3 Out[19]: <pre>tensor([[[0.4772, 0.9213, 0.7913, 0.7180],\n         [0.2099, 0.9476, 0.9990, 0.6070],\n         [0.0100, 0.6326, 0.4509, 0.1024]],\n\n        [[0.3299, 0.7543, 0.4103, 0.2181],\n         [0.6550, 0.4508, 0.4600, 0.5979],\n         [0.9470, 0.8033, 0.7679, 0.0559]]])</pre>"},{"location":"tensor_transpose/#transposicao","title":"\ud83d\udd04 Transposi\u00e7\u00e3o\u00b6","text":"<p>A matriz transposta \u00e9 uma matriz que quando aplicamos o processo de transposicao ela troca a posicao de suas linhas por colunas (ou vice-versa), basicamente girando a matriz ao redor eixo da diagonal principal.</p> <p>Isso significa que o elemento na i-\u00e9sima linha e j-\u00e9sima coluna da matriz original se torna o elemento na j-\u00e9sima linha e i-\u00e9sima coluna na matriz transposta. Se tivermos uma matriz A, sua transposta \u00e9 frequentemente denotada por $(A^T)$.</p> <p>Por que a transposi\u00e7\u00e3o \u00e9 importante?</p> <ol> <li><p>C\u00e1lculo do Produto Escalar: A ordem dos fatores no c\u00e1lculo do produto escalar \u00e9 importante. A transposi\u00e7\u00e3o permite ajustar as dimens\u00f5es das matrizes para que elas satisfa\u00e7am as regras da multiplica\u00e7\u00e3o de matrizes (o n\u00famero de colunas da primeira matriz deve ser igual ao n\u00famero de linhas da segunda matriz).</p> </li> <li><p>Processamento de Dados e Machine Learning: Na manipula\u00e7\u00e3o de dados e no machine learning, a transposi\u00e7\u00e3o \u00e9 frequentemente usada para rearranjar dados, especialmente quando se trabalha com matrizes de caracter\u00edsticas, onde cada linha representa uma observa\u00e7\u00e3o e cada coluna representa uma vari\u00e1vel. Transpor essas matrizes pode ser necess\u00e1rio para alinhar os dados de acordo com as exig\u00eancias de certos algoritmos ou opera\u00e7\u00f5es.</p> </li> </ol> <p>\ud83d\udee0\ufe0f Operacoes de Transposi\u00e7\u00e3o nativa do PyTorch:</p> <ul> <li><code>tensor.T</code></li> <li><code>torch.transpose(x, 0, 1)</code></li> <li><code>torch.permute(x, (2, 0, 1))</code></li> </ul> <p>Estas s\u00e3o algumas das maneiras de realizar a transposi\u00e7\u00e3o em bibliotecas como PyTorch, oferecendo flexibilidade para manipular as dimens\u00f5es dos dados conforme necess\u00e1rio.</p> <p>Vamos entao ver na pr\u00e1tica, como usamos cada um dos m\u00e9todos apresentados, e como usar cada caso</p>"},{"location":"tensors_broadcasting/","title":"Tensors broadcasting","text":"In\u00a0[1]: Copied! <pre>import torch\n\nn = torch.rand(2,2)\n\nn\n</pre> import torch  n = torch.rand(2,2)  n Out[1]: <pre>tensor([[0.8484, 0.3655],\n        [0.6376, 0.4929]])</pre> In\u00a0[2]: Copied! <pre>j = torch.rand(2)\n\nj\n</pre> j = torch.rand(2)  j Out[2]: <pre>tensor([0.6048, 0.5671])</pre> <p>Perceba que aqui as regras, para a existencia do Broadcasting estao sendo satisfeitas, pois, temos que a dimensao mais a direita \u00e9 <code>1</code> ou uma dimensao maior, e neste caso se percebermos ambos os tensores possuem o mesmo n\u00famero de colunas (dimensao mais a direita) ambos possuem 2 colunas (dimensoes iguais), portanto o Tensor j sofrer\u00e1 Broadcasting e ter\u00e1 seus valores duplicados de tal forma que tenhamos uma matriz (2 x 2)</p> In\u00a0[3]: Copied! <pre>broad = n + j\n\nbroad\n</pre> broad = n + j  broad Out[3]: <pre>tensor([[1.4532, 0.9326],\n        [1.2424, 1.0600]])</pre> <p>Um outro comentario interessante a se fazer em relacao ao Broadcasting \u00e9 que ele \u00e9 aplicado a todas as operacoes aritim\u00e9ticas que vimos at\u00e9 aqui, incluindo por exemplo produtos escalares entre Tensores, sem nos esquecermos das operacoes arit\u00e9mticas entre Tensores fundamentais como por exemplo (+, - , * , /) e etc.</p> <p>O que devemos salientar \u00e9 para os produtos escalares que no final das contas, a grosso modo sao modelos simplificados de um neuronio artificial e portanto, ao lidar com Redes MLP, ou Redes Convolucionais, estamos a todo momento trabalhando com o Broadcasting. Vamos ver um exemplo para consildar este conhecimento tao importante:</p> In\u00a0[13]: Copied! <pre>x = torch.rand(2,2,3)\nw = torch.rand(3,2)\nb = torch.rand(2)\n</pre> x = torch.rand(2,2,3) w = torch.rand(3,2) b = torch.rand(2) In\u00a0[15]: Copied! <pre>neuron = torch.matmul(x, w) + b\n\nneuron\n</pre> neuron = torch.matmul(x, w) + b  neuron Out[15]: <pre>tensor([[[1.6143, 1.1010],\n         [1.2022, 1.0477]],\n\n        [[1.0583, 1.0030],\n         [2.0519, 1.3876]]])</pre> In\u00a0[16]: Copied! <pre>neuron.shape\n</pre> neuron.shape Out[16]: <pre>torch.Size([2, 2, 2])</pre>"},{"location":"tensors_broadcasting/#broadcasting","title":"\ud83d\udce1 Broadcasting\u00b6","text":"<p>Broadcasting \u00e9 um mecanismo que permite realizar opera\u00e7\u00f5es aritm\u00e9ticas entre tensores de diferentes formas (ou tamanhos):</p> <ol> <li>Como foi poss\u00edvel observar at\u00e9 agora, cada tensor possui pelo menos uma dimens\u00e3o.</li> </ol> <ul> <li>Isso significa que os tensores envolvidos na opera\u00e7\u00e3o devem ter pelo menos uma dimens\u00e3o. Um tensor n\u00e3o pode ser um escalar (um \u00fanico n\u00famero sem dimens\u00f5es) se quisermos aplicar as regras de broadcasting.</li> </ul> <ol> <li>O Broadcasting \u00e9 algo que acontece tanto no Pytorch quanto no Numpy por\u00e9m \u00e9 algo ocorrendo debaixo dos panos nao \u00e9 nenhum m\u00e9todo que ser\u00e1 aplicado ao tensor por exemplo. Entretanto, o broadcasting possui alguns pr\u00edncipios de funcionamento, e compreender isto, aux\u00edlia ao desenvolvedor ter um maior dom\u00ednio do que est\u00e1 acontecendo com seus tensores.</li> </ol> <p>Para que o broadcasting entao aconteca esta operacao ir\u00e1 sempre comecar verificando se a dimens\u00e3o final possui os tamanhos das compativ\u00e9is que permitem o broadcasting sendo rem resumo:</p> <ul> <li>As dimensoes mais a direita devem ser iguais;</li> <li>Algum dos valores ou dimensao \u00e9 1, ou um deles n\u00e3o existe;</li> </ul> <p>Como mencionado, o brodcasting inicia:</p> <ul> <li>Comparando as dimens\u00f5es dos dois tensores, come\u00e7ando pela \u00faltima dimens\u00e3o (a mais \u00e0 direita) e movendo-se para a primeira dimens\u00e3o (a mais \u00e0 esquerda).</li> </ul> <p>Agora de maneira detalhada como j\u00e1 foi citado anteriormente para cada par de dimens\u00f5es comparadas, tr\u00eas cen\u00e1rios permitem o broadcasting: 1. Os tamanhos das dimens\u00f5es s\u00e3o iguais: Se as dimens\u00f5es correspondentes nos dois tensores t\u00eam o mesmo tamanho, ent\u00e3o elas s\u00e3o compat\u00edveis. (ocorre broadcasting) 2. Uma das dimens\u00f5es \u00e9 1: Se um tensor tem uma dimens\u00e3o de tamanho 1 na posi\u00e7\u00e3o que est\u00e1 sendo comparada, ele pode ser \"esticado\" ou \"expandido\" para corresponder ao tamanho da outra dimens\u00e3o. Isso \u00e9 feito sem realmente copiar dados em mem\u00f3ria, mas sim apenas adaptando a opera\u00e7\u00e3o aritm\u00e9tica para se comportar como se o tensor menor tivesse sido expandido. 3. Uma das dimens\u00f5es n\u00e3o existe: Se um dos tensores tem menos dimens\u00f5es que o outro, podemos considerar que ele tem dimens\u00f5es extras de tamanho 1 na frente (mais \u00e0 esquerda). Essas dimens\u00f5es \"faltantes\" s\u00e3o implicitamente consideradas como 1, permitindo o broadcasting.</p> <p></p> <p>(Fonte: https://deeplearninguniversity.com/pytorch/pytorch-broadcasting/)</p> <p>O broadcasting permite que voc\u00ea realize opera\u00e7\u00f5es aritm\u00e9ticas entre tensores de formas diferentes de uma maneira eficiente e intuitiva. Por exemplo, voc\u00ea pode adicionar um vetor a cada linha de uma matriz ou multiplicar uma matriz 3D por um vetor, aplicando a opera\u00e7\u00e3o em cada \"fatia\" da matriz 3D, sem a necessidade de loops expl\u00edcitos ou duplica\u00e7\u00e3o de dados.</p> <p>Vamos, observar alguns exemplos para que fique ainda mais claro, e tamb\u00e9m para podermos entender como o broadcasting ocorre na pr\u00e1tica</p>"},{"location":"tensors_dotproduct/","title":"Tensors dotproduct","text":"In\u00a0[1]: Copied! <pre>import torch\n</pre> import torch In\u00a0[\u00a0]: Copied! <pre>a = torch.rand(5)\nb = torch.rand(5)\n</pre> a = torch.rand(5) b = torch.rand(5)   In\u00a0[13]: Copied! <pre>a\n</pre> a Out[13]: <pre>tensor([0.5952, 0.3507, 0.0260, 0.1331, 0.2415, 0.4984])</pre> In\u00a0[5]: Copied! <pre>b\n</pre> b Out[5]: <pre>tensor([0.9816, 0.7538, 0.5317, 0.3351, 0.1633])</pre> In\u00a0[7]: Copied! <pre>c = a * b\n\nc\n</pre> c = a * b  c Out[7]: <pre>tensor([0.2109, 0.5796, 0.4587, 0.3141, 0.0383])</pre> <p>Perceba, que se observarmos matematicamente a equacao matematica do produto escalar vemos que ele realiza a multiplicacao (produto) elemento por elemento realizando um somat\u00f3rio. Portanto, se tentarmos transferir esta ideia, para o <code>PyTorch</code> ou at\u00e9 mesmo implementar sem utilizar o Pytorch, temos que ap\u00f3s realizar o produto, devemos realizar uma soma. Ou seja, estaremos utilizando um m\u00e9todo de reducao, que ir\u00e1 transformar esses produtos elemento a elemento do vetor em um \u00fanico escalar. Sendo assim, vamos tentar reproduzir esta ideia.</p> In\u00a0[9]: Copied! <pre>def dot_product(a,b):\n    return torch.sum(a * b)\n\nvetor1 = torch.rand(10)\nvetor2 = torch.rand(10)\n\nprint(vetor1)\nprint(vetor2)\ndot_product(vetor1, vetor2)\n</pre> def dot_product(a,b):     return torch.sum(a * b)  vetor1 = torch.rand(10) vetor2 = torch.rand(10)  print(vetor1) print(vetor2) dot_product(vetor1, vetor2) <pre>tensor([0.2669, 0.9414, 0.8007, 0.3315, 0.8320, 0.5161, 0.5016, 0.4714, 0.3182,\n        0.8395])\ntensor([0.3715, 0.6113, 0.8205, 0.5979, 0.8898, 0.4659, 0.5087, 0.2948, 0.3538,\n        0.2411])\n</pre> Out[9]: <pre>tensor(3.2197)</pre> <p>Mas, veja isso gera um certo trabalho adicional, pois \u00e9 necess\u00e1rio criar uma funcao, e utilizar um m\u00e9todo de reducao do Pytorch para s\u00f3 assim entao termos o resultado.</p> <p>Por\u00e9m seria bem mas conveniente se o Pytorch possuir alguns m\u00e9todos que j\u00e1 facilitem essa etapa de calcular o produto escalar e \u00e9 justamente isso que iremos explorar a seguir:</p> <p>O primeiro m\u00e9todo, e pode ser considerado o mais comum \u00e9 o <code>torch.dot()</code> o que ele faz ? R: Basicamente o <code>torch.dot()</code> reproduz o que implementamos anteriormente, ou seja recebe 2 tensores de ordem 1 e retorna um escalar, por\u00e9m o torch.dot, nos fornece isso bastando cham\u00e1-lo.</p> <p>Como podemos usar entao o <code>torch.dot()</code> e seus detalhes:</p> <p><code>torch.dot()</code>:</p> <ul> <li>Descri\u00e7\u00e3o: Calcula o produto escalar (\ud83c\udfaf) de dois tensores 1-D (vetores).</li> <li>Entrada: Dois vetores (tensores 1-D) \ud83d\udccf\ud83d\udccf.</li> <li>Sa\u00edda: Um \u00fanico n\u00famero escalar (\ud83d\udd22), que \u00e9 o produto escalar dos dois vetores.</li> <li>Uso: Apropriado quando voc\u00ea quer a intera\u00e7\u00e3o direta ponto a ponto de dois vetores. Por exemplo: <code>torch.dot(tensor1, tensor2)</code>.</li> </ul> In\u00a0[12]: Copied! <pre>a = torch.rand(6)\nb = torch.rand(6)\n\nescalar = torch.dot(a,b)\n\nescalar\n</pre> a = torch.rand(6) b = torch.rand(6)  escalar = torch.dot(a,b)  escalar Out[12]: <pre>tensor(0.6679)</pre> <p>Vamos, agora ver uma alternativa existente ao <code>torch.dot()</code>. Basicamente o <code>torch.mm()</code> ele \u00e9 muito utilizado, quando temos tensores de ordem 2.</p> <p><code>torch.mm()</code>:</p> <ul> <li>Descri\u00e7\u00e3o: Executa a multiplica\u00e7\u00e3o de matrizes (\u2716\ufe0f) entre dois tensores 2-D.</li> <li>Entrada: Duas matrizes (tensores 2-D) \ud83d\udcc4\ud83d\udcc4.</li> <li>Sa\u00edda: Um tensor 2-D que \u00e9 o produto das duas matrizes (\ud83d\udcd8).</li> <li>Uso: Perfeito para quando voc\u00ea precisa multiplicar duas matrizes como em \u00e1lgebra linear cl\u00e1ssica. Por exemplo: <code>torch.mm(matrix1, matrix2)</code>.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>a_m = torch.rand(2,3)\nb_m = torch.rand(2,3)\n\na_m\n</pre> a_m = torch.rand(2,3) b_m = torch.rand(2,3)  a_m Out[\u00a0]: <pre>tensor([[0.2435, 0.7150, 0.6578],\n        [0.4218, 0.4848, 0.6333]])</pre> In\u00a0[20]: Copied! <pre>b_m\n</pre> b_m Out[20]: <pre>tensor([[0.2435, 0.7150, 0.6578],\n        [0.4218, 0.4848, 0.6333]])</pre> <p>Veja que se executarmos a linha abaixo teremos um erro. Este erro ocorre porque devemos lembrar que na matematica, ou melhor dizendo, matematicamente existem algumas condicoes para que exista o produto escalar entre dois vetores sendo $m = p$ ou seja. Se tiver um tensor <code>a_m</code> que \u00e9 <code>2 x 3</code> do tipo <code>(n x m)</code> e um outro tensor <code>b_m</code> que \u00e9 <code>2 x 3</code> <code>(p x q)</code> Veja que para que existe o produto escalar devemos garantir que $m = p$. Em outras palavras para que possamos, entao realizar o produto escalar, devemos ter que o n\u00famero de colunas do primeiro tensor deve ser igual ao n\u00famero de linhas do outro E podemos, resolver isso sem modificar a estrutura interna do tensor realizando a operacao de transposicao. Caso as condicoes sejam satisfeitas o produto escalar ir\u00e1 existir e sua sa\u00edda ser\u00e1 sempre no formato: <code>n x q</code> onde podemos fazer um paralelo com redes neurais.</p> <p>Alguns pontos importantes a salientar sao que tudo isso existe porque o produto escalar \u00e9 um caso especial de multiplicacao de matrizes e expandindo ainda mais devemos lembrar que um produto escalar ou tamb\u00e9m conhecido como produto interno s\u00f3 pode ser definido se ambos possuem o mesmo n\u00famero de componentes.</p> <p>Portanto, para que a operacao do produto escalar seja feita precisamos alterar ou melhor dizendo realizar a transposta do segundo tensor transformando-o em uma matriz coluna, isso ir\u00e1 satisfazer a condicao da multiplicacao de matrizes onde $m = p$, pois se m for diferente de p, entao nao teremos o produto escalar.</p> In\u00a0[21]: Copied! <pre>result = torch.mm(a_m, b_m)\n</pre> result = torch.mm(a_m, b_m) <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[21], line 1\n----&gt; 1 result = torch.mm(a_m, b_m)\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (2x3 and 2x3)</pre> In\u00a0[23]: Copied! <pre>result = torch.mm(a_m, b_m.T)\n\nresult\n</pre> result = torch.mm(a_m, b_m.T)  result Out[23]: <pre>tensor([[1.2983, 1.2372],\n        [0.9794, 0.9677]])</pre> <p>Por fim, vamos entender um outro m\u00e9todo existente do Pytorch que \u00e9 o <code>torch.matmul()</code> que \u00e9 extremamente utilizado quando queremos calcular o produto escalar, pois ela se trata de uma ferramenta um pouco mais geral, para calcular produto escalar nao ficando restrita a ordem dos tensores de entrada.</p> <p><code>torch.matmul()</code>:</p> <ul> <li>Descri\u00e7\u00e3o: Uma ferramenta de multiplica\u00e7\u00e3o de matrizes mais geral (\ud83d\udd17) que lida com matrizes de alta dimens\u00e3o e broadcasting.</li> <li>Entrada: Pode ser tensores de qualquer dimens\u00e3o (\u269b\ufe0f).</li> <li>Sa\u00edda: O tensor resultante pode variar em dimens\u00e3o, de acordo com as regras de broadcasting (\ud83d\udcc8).</li> <li>Uso: Vers\u00e1til e potente, essa fun\u00e7\u00e3o \u00e9 adequada para uma gama mais ampla de opera\u00e7\u00f5es de \u00e1lgebra tensorial. Por exemplo: <code>torch.matmul(tensor1, tensor2)</code>.</li> </ul> In\u00a0[25]: Copied! <pre>a_t = torch.rand(2,2,3)\nb_t = torch.rand(2,2,3)\n\nt_result = torch.matmul(a_t,b_t.T)\n</pre> a_t = torch.rand(2,2,3) b_t = torch.rand(2,2,3)  t_result = torch.matmul(a_t,b_t.T) <pre>/var/folders/h0/jqw3wsl561j81vjn6nzz1j740000gn/T/ipykernel_18135/3582778196.py:4: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3641.)\n  t_result = torch.matmul(a_t,b_t.T)\n</pre> <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[25], line 4\n      1 a_t = torch.rand(2,2,3)\n      2 b_t = torch.rand(2,2,3)\n----&gt; 4 t_result = torch.matmul(a_t,b_t.T)\n\nRuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0</pre> In\u00a0[29]: Copied! <pre>t_result = torch.matmul(a_t,b_t.transpose(1,2))\n\nt_result\n</pre> t_result = torch.matmul(a_t,b_t.transpose(1,2))  t_result Out[29]: <pre>tensor([[[0.5704, 0.8776],\n         [1.1949, 1.5624]],\n\n        [[0.9798, 0.3756],\n         [0.1970, 0.0779]]])</pre>"},{"location":"tensors_dotproduct/#produto-escalar","title":"\ud83d\udcd0\u2716\ufe0f Produto Escalar\u00b6","text":"<p>O produto escalar, tamb\u00e9m conhecido como produto interno, \u00e9 uma opera\u00e7\u00e3o matem\u00e1tica que permite combinar dois vetores para resultar em um \u00fanico n\u00famero escalar. \ud83c\udfaf</p> <p>Por que o produto escalar \u00e9 t\u00e3o utilizado no mundo da AI? \ud83e\udd14\ud83d\udca1</p> <ol> <li><p>Combina\u00e7\u00e3o Linear de Entradas: \ud83e\udde0 Em uma rede neural, cada neur\u00f4nio calcula uma combina\u00e7\u00e3o linear de seus inputs, que s\u00e3o os valores recebidos de outros neur\u00f4nios ou da entrada inicial. O produto escalar faz justamente isso, multiplicando inputs por pesos e somando-os para criar a sa\u00edda do neur\u00f4nio.</p> </li> <li><p>Produto Escalar \u00e9 Diferenci\u00e1vel: \ud83d\udd04 A diferenciabilidade do produto escalar \u00e9 essencial para aplicar algoritmos de otimiza\u00e7\u00e3o baseados em gradiente, como o Gradiente Descendente, cruciais para o treinamento de redes neurais.</p> </li> </ol> <p>A equa\u00e7\u00e3o do produto escalar entre dois vetores $ \\mathbf{a} $ e $ \\mathbf{b} $ pode ser escrita como:</p> <p>$$ \\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^{n} a_i b_i $$</p> <p>E como \u00e9 o produto escalar em matriz?</p> <p>https://www.mathsisfun.com/algebra/matrix-multiplying.html</p>"},{"location":"tensors_fundamentals/","title":"Tensors fundamentals","text":"In\u00a0[2]: Copied! <pre>import torch\n</pre> import torch In\u00a0[3]: Copied! <pre>escalar = torch.tensor(6)\nprint(type(escalar))\n</pre> escalar = torch.tensor(6) print(type(escalar)) <pre>&lt;class 'torch.Tensor'&gt;\n</pre> <pre>C:\\Users\\Joao\\AppData\\Local\\Temp\\ipykernel_19928\\3055716407.py:1: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n  escalar = torch.tensor(6)\n</pre> In\u00a0[4]: Copied! <pre>escalar.shape \n</pre> escalar.shape  Out[4]: <pre>torch.Size([])</pre> <p>\u00c9 poss\u00edvel realizar operacoes matem\u00e1ticas normalmente por exemplo: Adicao, subtracao , Multiplicacao, Divisao e etc.</p> In\u00a0[5]: Copied! <pre>escalar / 2\n</pre> escalar / 2 Out[5]: <pre>tensor(3.)</pre> <p>\u00c9 poss\u00edvel tamb\u00e9m realizar operacoes de entre escalares, como por exemplo multiplicacao de 2 escalares</p> In\u00a0[6]: Copied! <pre>escalar2 = torch.tensor(4)\n\nresult = escalar * escalar2\n\nresult\n</pre> escalar2 = torch.tensor(4)  result = escalar * escalar2  result Out[6]: <pre>tensor(24)</pre> In\u00a0[8]: Copied! <pre>vector = torch.tensor([4 , 5 , 6, 7])\n\nvector\n</pre> vector = torch.tensor([4 , 5 , 6, 7])  vector Out[8]: <pre>tensor([4, 5, 6, 7])</pre> <p>Veja que aqui de maneira an\u00e1loga aos escalares verificamos que o vetor \u00e9 um tensor de ordem 1, ou seja, possui uma dimensao.</p> In\u00a0[9]: Copied! <pre>vector.shape\n</pre> vector.shape Out[9]: <pre>torch.Size([4])</pre> <p>Aqui temos a divisao de um tensor de ordem 1, por um n\u00famero por exemplo</p> In\u00a0[10]: Copied! <pre>vector / 2 \n</pre> vector / 2  Out[10]: <pre>tensor([2.0000, 2.5000, 3.0000, 3.5000])</pre> <p>Como visto para os escalares as mesmas propriedades valem para os vetores, por exemplo podemos realizar operacoes matem\u00e1ticas entre tensores, por exemplo:</p> In\u00a0[11]: Copied! <pre>vector2 = torch.tensor([4,5,7,8])\n\nresult = vector2 / vector\n\nresult\n</pre> vector2 = torch.tensor([4,5,7,8])  result = vector2 / vector  result Out[11]: <pre>tensor([1.0000, 1.0000, 1.1667, 1.1429])</pre> In\u00a0[13]: Copied! <pre>matrix = torch.tensor([[1,2,3],[4,5,6]])\n</pre> matrix = torch.tensor([[1,2,3],[4,5,6]]) In\u00a0[15]: Copied! <pre>matrix.shape\n</pre> matrix.shape Out[15]: <pre>torch.Size([2, 3])</pre> <p>Veja que novamente, todas as propriedades e m\u00e9todos utilizados tanto por escalares quanto pelos vetores, tamb\u00e9m podem ser utilizados e aplicados para se trabalhar com matrizes.</p> <p>Em termos pr\u00e1ticos, podemos dizer que as matrizes, sao vetores que carregam vetores. Vamos, mostrar que as mesmas propriedades matem\u00e1ticas se aplicam.</p> <p>Um ponto importante a salientar que as operacoes entre tensores ir\u00e1 retornar sempre a dimensao do tensor mais a esquerda independente de sua ordem</p> In\u00a0[16]: Copied! <pre>matrix / 4\n</pre> matrix / 4 Out[16]: <pre>tensor([[0.2500, 0.5000, 0.7500],\n        [1.0000, 1.2500, 1.5000]])</pre> In\u00a0[17]: Copied! <pre>matrix2 = torch.tensor([[4,2,4],[5,6,8]])\n</pre> matrix2 = torch.tensor([[4,2,4],[5,6,8]]) In\u00a0[19]: Copied! <pre>result_m = matrix * matrix2\nresult_m\n</pre> result_m = matrix * matrix2 result_m Out[19]: <pre>tensor([[ 4,  4, 12],\n        [20, 30, 48]])</pre> In\u00a0[21]: Copied! <pre>tensor = torch.tensor([[[3,5,6],[7,8,8]]])\n\ntensor\n</pre> tensor = torch.tensor([[[3,5,6],[7,8,8]]])  tensor Out[21]: <pre>tensor([[[3, 5, 6],\n         [7, 8, 8]]])</pre> In\u00a0[22]: Copied! <pre>tensor.shape\n</pre> tensor.shape Out[22]: <pre>torch.Size([1, 2, 3])</pre> <p>Veja que o resultado acima retornou um shape do tipo ([1,2,3]) ou seja est\u00e1 nos falando que ele \u00e9 um vetor que carrega uma matriz 2x3 (2 linhas e 3 colunas). Logo, podemos afirmar que ele de fato \u00e9 um Tensor de ordem 3.</p> <p>Al\u00e9m disso, um outro detalhe que \u00e9 poss\u00edvel afirmar \u00e9 que, perfeito o Tensor \u00e9 de ordem 3. Sendo assim, todas as propriedades matematicas feitas para os Escalares,Vetore e Matrizes, tamb\u00e9m valem para o tensor</p> In\u00a0[24]: Copied! <pre>tensor + 1\n</pre> tensor + 1 Out[24]: <pre>tensor([[[4, 6, 7],\n         [8, 9, 9]]])</pre> <p>Um ponto muito importante a ser observado e que a primeiro momento, por algu\u00e9m mais despercebido, nao \u00e9 necess\u00e1rio ao se trabalhar com o m\u00e9todo <code>tensor()</code> fazer loops for para varrer cada elemento da lista para aplicar as operacoes matem\u00e1ticas o m\u00e9todo tensor ja realiza este c\u00e1lculo automaticamente o que aumenta muito a produtividade e facilita muito o trabalho do profissional que trabalha com Visao Computacional por exemplo.</p> In\u00a0[25]: Copied! <pre>tensor2 = torch.tensor([[[2,3,5],[5,9,0]]])\n</pre> tensor2 = torch.tensor([[[2,3,5],[5,9,0]]]) In\u00a0[26]: Copied! <pre>t_result = tensor2 - tensor\n\nt_result\n</pre> t_result = tensor2 - tensor  t_result Out[26]: <pre>tensor([[[-1, -2, -1],\n         [-2,  1, -8]]])</pre>"},{"location":"tensors_fundamentals/#veja-que-o-metodo-shape-retorna-um-array-vazio-ou-seja-a-dimensao-e-zero","title":"Veja que o m\u00e9todo shape retorna um array vazio, ou seja a dimensao \u00e9 zero.\u00b6","text":"<p>Isso significa que o escalar \u00e9 um tensor de ordem 0. (0 dimensoes)</p>"},{"location":"tensors_fundamentals/#vetores","title":"Vetores\u00b6","text":""},{"location":"tensors_fundamentals/#matrizes","title":"Matrizes\u00b6","text":""},{"location":"tensors_fundamentals/#tensores","title":"Tensores\u00b6","text":"<p>Tensores de ordem 3 carregam 3 dimensoes, e sao ideias para se trabalhar com imagens, pois eles possuem 3 dimensoes assim como as imagens ou seja, eles carregam as posicoes x e y e informacoes do pixeis dos canais de cores totalizando 3 dimensoes</p> <p>Em termos pr\u00e1ticos podemos dizer que assim como as matrizes sao elementos que carregam um conjunto de vetores, podemos afirmar que os Tensores sao elementos que carregam uma matriz de matrizes ou melhor carrega um conjunto de matrizes</p>"},{"location":"tensors_methods/","title":"Tensors methods","text":"<ol> <li>torch.zeros(): Respons\u00e1vel por criar tensores 'n' dimensional preencchido com zeros, parecido com outros geradores como Numpy no Matlab e ou outras linguagens</li> </ol> <ul> <li>Caso de uso: Este m\u00e9todo \u00e9 muito \u00fatil quando \u00e9 necess\u00e1rio um tensor base para acumular valores, como em somas em um loop ou por exemplo \u00e9 extremamente utilizado ao se trabalhar com Backpropagation em redes MLP, para acumular valores ou para inicializar os pesos de uma rede neural, embora seja uma partica menos utilizada.</li> </ul> <p>Existem diversas maneiras de se utilizar o <code>torch.zeros</code>, para que o m\u00e9todo seja funcional devemos informar para ele pelo menos 1 valor ou tamb\u00e9m podemos dizer que \u00e9 necess\u00e1rio passar ao menos um tensor de ordem 1, pois neste caso se este valor \u00fanico for informado ele ir\u00e1 retornar um vetor ou tensor de ordem 1 com a quantidade de zeros preenchida correspondente a quantidade que foi informada no m\u00e9todo.</p> In\u00a0[12]: Copied! <pre>import torch\n\nvector = torch.zeros(4)\n\nprint(vector)\nprint(vector.shape)\n</pre> import torch  vector = torch.zeros(4)  print(vector) print(vector.shape) <pre>tensor([0., 0., 0., 0.])\ntorch.Size([4])\n</pre> <p>\u00c9 poss\u00edvel tamb\u00e9m criar tensores de ordem maior por exemplos tensores de ordem 2 e ordem 3 veja:</p> <ul> <li>Vamos inicialmente criar um tensor de ordem 2, para isso veja que iremos informar que queremos 2 linhas e 3 colunas. Logo, o m\u00e9todo torch.zeros(2,3) ir\u00e1 retornar uma matriz (tensor ordem 2), contendo 2 linhas e 3 colunas. Portanto, \u00e9 s\u00f3 informar a quantidade de elementos que se quer pela quantidade de dimensoes que se quer</li> </ul> In\u00a0[14]: Copied! <pre>matrice = torch.zeros(2,3, dtype=torch.uint32)\nprint(matrice)\nprint(matrice.shape)\n</pre> matrice = torch.zeros(2,3, dtype=torch.uint32) print(matrice) print(matrice.shape) <pre>tensor([[0, 0, 0],\n        [0, 0, 0]], dtype=torch.uint32)\ntorch.Size([2, 3])\n</pre> <p>Al\u00e9m disso, podemos tamb\u00e9m ter de maneira an\u00e1loga ao feito para o tensor de ordem 2, \u00e9 poss\u00edvel fazer para o tensor de ordem 3 e a \u00fanica modificacao que \u00e9 necess\u00e1ria de ser feita \u00e9 adicionar uma dimensao.</p> In\u00a0[4]: Copied! <pre>tensor = torch.zeros(2,2,3)\nprint(tensor)\nprint(tensor.shape)\n</pre> tensor = torch.zeros(2,2,3) print(tensor) print(tensor.shape) <pre>tensor([[[0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.]]])\ntorch.Size([2, 2, 3])\n</pre> <ol> <li>torch.ones(): Respons\u00e1vel por criar um tensor 'n' dimensional preenchido com n\u00fameros 1</li> </ol> <ul> <li>Caso de uso:  Muito utilizado para criar \"mascaras\", normalmente utilizado no contexto de redes neurais ao se trabalhar com dropout com objetivo de evitar ou diminuir o overfit do modelo. Mas como assim? o procedimento \u00e9 simples como o m\u00e9todo <code>torch.ones()</code> nos retorna vetores preenchidos com valores 1, entre estes valores 1 se adicionam valores zero e posteriormente se multiplica pelos valores da entrada da rede neural e ai entao tem-se uma esp\u00e9cie de m\u00e1scara pois qualquer valor multiplicado por zero, sera 0.</li> </ul> In\u00a0[5]: Copied! <pre>ord1_tensor = torch.ones(4)\nprint(ord1_tensor)\n</pre> ord1_tensor = torch.ones(4) print(ord1_tensor) <pre>tensor([1., 1., 1., 1.])\n</pre> In\u00a0[6]: Copied! <pre>ord2_tensor = torch.ones(3,5)\nprint(ord2_tensor)\n</pre> ord2_tensor = torch.ones(3,5) print(ord2_tensor) <pre>tensor([[1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1.]])\n</pre> In\u00a0[7]: Copied! <pre>ord3_tensor = torch.ones(3,3,3)\nprint(ord3_tensor)\n</pre> ord3_tensor = torch.ones(3,3,3) print(ord3_tensor) <pre>tensor([[[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]]])\n</pre> <p>Logo, podemos notar que por mais que estejamos trabalhando com tensores de ordem 0,1,2,3 desde o \u00ednicio do estudo de tensores isso nao significa que o m\u00f3dulo tensor seja limitado a isso. Como ja mencionado durante algumas observacoes e explanacoes, a inicializacao dos tensores pode ser de ordem o at\u00e9 ordem n, onde basta adaptar o n\u00famero de dimensoes conforme for necess\u00e1rio.</p> <ol> <li>torch.rand(): Cria um tensor com valores alet\u00f3rios com valores uniformemente distribu\u00eddos ou em outras palavras cria ou inicializa um tensor com valores aleat\u00f3rios seguindo uma distribuicao uniforme</li> </ol> <ul> <li>Caso de uso: Este m\u00e9todo \u00e9 muito utilizado para inicializar os pesos de uma rede neural por exemplo, j\u00e1 que normalmente os pesos de uma rede neural sao inicializados com valores aleat\u00f3rios que posteriormente serao optimizados atrav\u00e9s de alguma t\u00e9cnica de otimizacao como Stocastic Gradient Descent por exemplo. gera valores entre 0 e 1</li> </ul> In\u00a0[22]: Copied! <pre>random_tensor = torch.rand(4)\nprint(random_tensor)\n</pre> random_tensor = torch.rand(4) print(random_tensor) <pre>tensor([0.4887, 0.7744, 0.1333, 0.4912])\n</pre> In\u00a0[9]: Copied! <pre>rand_tensor1 = torch.rand(2,3)\nprint(rand_tensor1)\n</pre> rand_tensor1 = torch.rand(2,3) print(rand_tensor1) <pre>tensor([[0.2579, 0.8253, 0.2264],\n        [0.3322, 0.0997, 0.7361]])\n</pre> In\u00a0[10]: Copied! <pre>rand_tensor2 = torch.rand(2,3,6)\nprint(rand_tensor2)\n</pre> rand_tensor2 = torch.rand(2,3,6) print(rand_tensor2) <pre>tensor([[[0.8243, 0.5449, 0.3278, 0.4985, 0.2112, 0.6157],\n         [0.0431, 0.5848, 0.0809, 0.1324, 0.6064, 0.2210],\n         [0.2432, 0.0865, 0.9929, 0.1290, 0.7732, 0.8747]],\n\n        [[0.5709, 0.8903, 0.0398, 0.6204, 0.4639, 0.6038],\n         [0.2114, 0.3910, 0.7623, 0.7430, 0.4407, 0.9496],\n         [0.4874, 0.6306, 0.2957, 0.8788, 0.6004, 0.7478]]])\n</pre> <ol> <li>torch.randn(): Este m\u00e9todo \u00e9 muito parecido com o torch.rand, por\u00e9m diferentemente ele cria um tensor com valores aleat\u00f3rios por\u00e9m seguindo uma distribuicao normal, ou tamb\u00e9m muito conhecida como distribuicao gaussiana</li> </ol> <ul> <li>Caso de uso: E de maneira an\u00e1loga um dos principais caso de uso deste m\u00e9todo e para realizar a inicializacao de pesos de uma rede neural, por\u00e9m este m\u00e9todo tem uma particularidade ele gera valores randomicos com valores entre -1 e 1. (Normal padronizada)  $(\\alpha = 0 , \\theta = 1)$</li> </ul> In\u00a0[11]: Copied! <pre>gaussian_tensor = torch.randn(10)\ngaussian_tensor\n</pre> gaussian_tensor = torch.randn(10) gaussian_tensor Out[11]: <pre>tensor([ 2.4450,  0.1846,  1.5569, -0.8070,  0.8241, -0.8518, -2.6100, -1.0074,\n        -1.4556, -0.9220])</pre> In\u00a0[12]: Copied! <pre>gaussian_tensor2d = torch.randn(3,6)\ngaussian_tensor2d\n</pre> gaussian_tensor2d = torch.randn(3,6) gaussian_tensor2d Out[12]: <pre>tensor([[ 0.6851,  1.7652,  0.1203,  1.9294,  0.9716,  0.4198],\n        [ 0.4864, -1.0118, -0.1279,  0.1566, -1.4981,  0.5170],\n        [-0.4545,  1.3726,  0.7099,  0.1283,  1.2079,  0.1762]])</pre> In\u00a0[23]: Copied! <pre>gaussian_tensor3d = torch.randn(2,3,6)\ngaussian_tensor3d\n</pre> gaussian_tensor3d = torch.randn(2,3,6) gaussian_tensor3d Out[23]: <pre>tensor([[[ 0.0791,  1.9714,  1.5102,  1.2269,  1.0618, -1.6533],\n         [-1.7467, -0.4388, -0.9460, -2.9648, -0.0447,  0.7426],\n         [-0.0795, -0.5235,  0.2265,  1.3435, -0.8059, -0.8603]],\n\n        [[-0.2372, -0.1399, -2.5591,  0.4283,  2.1688, -0.1868],\n         [ 0.1828,  0.3666, -2.7581, -0.6882,  1.4558,  1.3762],\n         [-1.1815, -0.0949,  0.4675, -0.2590, -0.5231, -0.8510]]])</pre> <ol> <li>torch.arange(): Este m\u00e9todo cria um tensor com uma sequ\u00eancia de valores em um intrevalo espec\u00edfico informado como argumento. parecido com as generators functions do Python <code>range()</code></li> </ol> <ul> <li>Caso de uso: \u00c9 bastante utilizado quando se precisa de uma sequ\u00eancia n\u00famerica cont\u00ednua. Um ponto importante a ser mencionado \u00e9 que o m\u00e9todo <code>arange()</code> s\u00f3 cria tensores de ordem 1 (vetores), nao sendo poss\u00edvel criar tensores de ordem superior</li> </ul> In\u00a0[24]: Copied! <pre>numbers = torch.arange(10)\nnumbers\n</pre> numbers = torch.arange(10) numbers Out[24]: <pre>tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</pre> In\u00a0[32]: Copied! <pre>numb_matrice = torch.arange(2,3,2)\nnumb_matrice\n</pre> numb_matrice = torch.arange(2,3,2) numb_matrice Out[32]: <pre>tensor([2])</pre> <ol> <li>torch.full(): Cria um tensor preenchido com valor especificado como argumento.</li> </ol> <ul> <li>Caso de uso: Este m\u00e9todo \u00e9 muito utilizado quando \u00e9 necess\u00e1rio um tensor que cont\u00e9m um valor constante em todas as suas entradas. Isso pode ser \u00fatil em v\u00e1rias situacoes, como inicializacao de um tensor com um valor de bias por exemplo no contexto de redes neurais. Um coisa importante a ser mencionado \u00e9 que o m\u00e9todo <code>torch.full()</code> funciona um pouco diferente da seguinte maneira: \u00e9 necess\u00e1rio informarmos uma <code>tupla()</code> contendo as dimensoes do tensor e tamb\u00e9m o valor que ser\u00e1 preenchido veja abaixo:</li> </ul> In\u00a0[26]: Copied! <pre>one_d = torch.full((10,),0.5)\none_d\n</pre> one_d = torch.full((10,),0.5) one_d Out[26]: <pre>tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n        0.5000])</pre> In\u00a0[27]: Copied! <pre>two_d = torch.full((3,6),2.5)\ntwo_d\n</pre> two_d = torch.full((3,6),2.5) two_d Out[27]: <pre>tensor([[2.5000, 2.5000, 2.5000, 2.5000, 2.5000, 2.5000],\n        [2.5000, 2.5000, 2.5000, 2.5000, 2.5000, 2.5000],\n        [2.5000, 2.5000, 2.5000, 2.5000, 2.5000, 2.5000]])</pre> In\u00a0[28]: Copied! <pre>tree_d = torch.full((2,3,4),0.98)\ntree_d\n</pre> tree_d = torch.full((2,3,4),0.98) tree_d Out[28]: <pre>tensor([[[0.9800, 0.9800, 0.9800, 0.9800],\n         [0.9800, 0.9800, 0.9800, 0.9800],\n         [0.9800, 0.9800, 0.9800, 0.9800]],\n\n        [[0.9800, 0.9800, 0.9800, 0.9800],\n         [0.9800, 0.9800, 0.9800, 0.9800],\n         [0.9800, 0.9800, 0.9800, 0.9800]]])</pre> <ol> <li>torch.from_numpy(): Este m\u00e9todo \u00e9 muito intuitivo pois ele faz literalmente o que seu nome exprime ou seja, este m\u00e9todo cria ou permite inicializar um tensor a partir de um array Numpy e como exibido abaixo \u00e9 poss\u00edvel criar o tensor, atrav\u00e9s do numpy diretamente sem precisar de utilizar o m\u00e9todo <code>from_numpy()</code> devido a compatibilidade do torch com o numpy e portanto, pode-se criar diretamente o tensor utilizando o m\u00f3dulo <code>torch.tensor()</code></li> </ol> In\u00a0[29]: Copied! <pre>import numpy as np\n\nx = np.array([1,2,3])\n\nnp_tensor = torch.from_numpy(x)\nnp_tensor\n</pre> import numpy as np  x = np.array([1,2,3])  np_tensor = torch.from_numpy(x) np_tensor Out[29]: <pre>tensor([1, 2, 3], dtype=torch.int32)</pre> In\u00a0[30]: Copied! <pre>y = torch.tensor(x)\ny\n</pre> y = torch.tensor(x) y Out[30]: <pre>tensor([1, 2, 3], dtype=torch.int32)</pre> <p>Durante a explanacao mostrando os conceitos de inicializacao de tensores, foi poss\u00edvel aprender os seguintes m\u00e9todos:</p> <ol> <li>torch.zeros()</li> <li>torch.ones()</li> <li>torch.rand()</li> <li>torch.randn()</li> <li>torch.arange()</li> <li>torch.full()</li> <li>torch.from_numpy()</li> </ol> <p>E sendo assim, uma observacao interessante a se fazer e que nao foi explanada durante este notebook \u00e9 que todos m\u00e9todos citados acima, cada um deles suportam o argumento: <code>dtype=torch.&lt;type&gt;</code>. Entretanto, mesmo que eles suportem o argumento dtype, isso nao quer dizer que todos aceitam os mesmos dytpes, pois existem algumas consideracoes a se fazer.</p> <ul> <li>Tipos de dados suportados pelo dtype<ul> <li>torch.int8 , torch.int16 , torch.int32 , torch.int64</li> <li>torch.uint8 , torch.uint16 , torch.uint32 , torch.uint64</li> <li>torch.float16 , torch.float32 , torch.float64</li> </ul> </li> </ul> <p>OBS: Entretanto, como observacao o m\u00e9todo torch.from_numpy() nao aceita nenhum <code>dtype</code> e m\u00e9todos como torch.rand() nao aceita nenhuma estrutura de dados float, justamente por sua caracteristica de gerar n\u00fameros aleat\u00f3rios entre 0 e 1, a mesma coisa acontece para o torch.randn() pois gera n\u00fameros entre -1 e 1.</p>"},{"location":"tensors_methods/#criacao-e-inicializacao-de-tensores","title":"\ud83d\udca1 Criacao e inicializacao de Tensores\u00b6","text":"<p>As operacoes de criacao de tensores no Pytorch sao ferramentas fundamentais, pois sao extremamente utilizadas em Machine Learning, Deep Learning Data Science e em muitas outras \u00e1reas. Nestas \u00e1reas, existe uma grande necessidade de se trabalhar e manipular tensores e abaixo, vamos ilustrar os m\u00e9todos mais utilizados:</p>"},{"location":"torch_activate_functions/","title":"Torch activate functions","text":"In\u00a0[2]: Copied! <pre>import torch\nfrom torch import nn\ninput = torch.tensor([-2, -1 , 0 , 1 , 2])\n</pre> import torch from torch import nn input = torch.tensor([-2, -1 , 0 , 1 , 2])  In\u00a0[5]: Copied! <pre># Criando o objeto da funcao de ativacao ReLU\nrelu_layer1 = nn.ReLU()\n\n# Aplicando a funcao de ativacao na entrada, para armazenar o resultado da sa\u00edda deste processo.\noutput = relu_layer1(input)\n\noutput\n</pre> # Criando o objeto da funcao de ativacao ReLU relu_layer1 = nn.ReLU()  # Aplicando a funcao de ativacao na entrada, para armazenar o resultado da sa\u00edda deste processo. output = relu_layer1(input)  output  Out[5]: <pre>tensor([0, 0, 0, 1, 2])</pre> In\u00a0[6]: Copied! <pre>from torch.nn import functional as F\n</pre> from torch.nn import functional as F <p>Para ser poss\u00edvel utilizar a funcao de ativacao atrav\u00e9s da definicao, devemos fazer algo de maneira muito parecida com o que tinhamos feito com a classe ou seja, criar um objeto e passar para ele uma entrada. Vamos ver na pr\u00e1tica como realizar isso:</p> In\u00a0[7]: Copied! <pre># Criando um tensor de entrada\ninput2 = torch.tensor([-2, -1 , 0 , 1 , 2])\n\n# Aplicando a funcao de ativacao ReLU ao tensor de entrada\noutput2 = F.relu(input2)\n\noutput2\n</pre> # Criando um tensor de entrada input2 = torch.tensor([-2, -1 , 0 , 1 , 2])  # Aplicando a funcao de ativacao ReLU ao tensor de entrada output2 = F.relu(input2)  output2 Out[7]: <pre>tensor([0, 0, 0, 1, 2])</pre>"},{"location":"torch_activate_functions/#as-funcoes-de-ativacao-no-pytorch","title":"As funcoes de ativacao no Pytorch\u00b6","text":"<p>As funcoes de ativacao sao muito utilizadas no contexto de Deep Learning, pois elas permitem que a rede neural aprenda padroes mais complexos e etc.</p> <p>Normalmente, as funcoes de ativacao sao aplicadas a sa\u00edda da soma ponderada das entradas. De maneira resumida o papel de uma funcao de ativacao \u00e9 introduzir uma nao linearidade no limiar de decisao da rede neural.</p> <p>As funcoes de ativacao no Pytorch podem ser aplicadas a uma camada da rede neural da seguinte maneira:</p> <ul> <li>Usando a camada de ativacao: Que sao basicamente classes que podem ser utilizadas como funcoes de ativacao.</li> <li>Usando a definicao de funcao de ativacao: Que sao basicamente m\u00e9todos ou funcoes que podem ser utilizadas como funcao de ativacao.</li> </ul>"},{"location":"torch_activate_functions/#classe-funcao-de-ativacao","title":"Classe funcao de ativacao\u00b6","text":"<p>Na maior parte das vezes, ao trabalhar com funcoes de ativacao utilizamos a classe da camada de ativacao e \u00e1 adicionamos a camada da arquitetura do nosso modelo, como se tivessemos adicionando normalmente uma camada de uma rede neural. Portanto, a camada de funcao de ativacao \u00e9 adicionada dentro do construtor da classe do modelo, atrav\u00e9s da API \"torch.nn.SequentialAPI.\"</p> <p>Como ela \u00e9 uma classe devemos importar a camada da funcao de ativacao da seguinte maneira:</p> <pre>from torch import nn\n</pre>"},{"location":"torch_activate_functions/#usando-funcao-de-ativacao-com-o-uso-da-classe-camada-de-funcao-de-ativacao","title":"Usando funcao de ativacao com o uso da classe camada de funcao de ativacao.\u00b6","text":"<p>Para utilizar a classe de funcao de ativacao, devemos inicialmente criar um objeto ou uma instancia da classe camada de funcao de ativacao. Em seguida, precisamos fornecer uma entrada para a camada, sendo assim o que ir\u00e1 ocorre \u00e9 que a camada ir\u00e1 aplicar a funcao na entrada informada e retornar\u00e1 a sa\u00edda. Vamos entao ver um exemplo na pr\u00e1tica onde iremos criar uma camada utilizando um objeto da classe ReLUClass, forneceremos como entrada um tensor qualquer para ver o resultado da sa\u00edda.</p>"},{"location":"torch_activate_functions/#definicoes-de-funcao-de-ativacao","title":"Definicoes de funcao de ativacao\u00b6","text":"<p>O Pytorch implementou muitas das funcoes de ativacao conhecidas na bibliografia, funcoes essas que podem ser aplicadas e utilizadas diretamente no nosso modelo apenas chamando estas funcoes ou m\u00e9todos. Essas funcoes, sao utilizadas dentro do <code>forward()</code> o qual \u00e9 um m\u00e9todo que serve ou define o fluxo de dados ao longo do modelo. Ou define, se a rede neural \u00e9 feedforward ou se possui backpropagation.</p> <p>Como vimos anteriormente, podemos utilizar as funcoes de ativacao de duas maneiras no Pytorch utilizando a classe, ou utilizando a definicao de funcao de ativacao, como j\u00e1 praticamos a maneira de utilizar funcoes de ativacao atrav\u00e9s das classe, vamos agora praticar utilizando a definicao. E para isso vamos ver como pode importar a definicao de funcao de ativacao.</p>"},{"location":"torch_activate_functions/#importando-a-definicao-de-funcao-de-ativacao","title":"Importando a definicao de funcao de ativacao\u00b6","text":""},{"location":"torch_activate_functions/#funcoes-de-ativacao-disponiveis-no-pytorch","title":"Funcoes de ativacao disponiveis no Pytorch\u00b6","text":"<p>O Pytorch possui uma variedade de funcoes de ativacao, que podem ser aplicadas a uma camada da rede neural. Vamos ver agora algumas das funcoes de ativacao disponiveis no Pytorch. Essas funcoes de ativacao estao disponiveis como Layers(classes) ou as \"definitions\" (m\u00e9todos) que foi como vimos durante o texto, Veja a tabela abaixo que cont\u00e9m algumas das funcoes de ativacao mais comumente usadas:</p> Classe de camada de fun\u00e7\u00e3o de ativa\u00e7\u00e3o (em <code>torch.nn</code>) Defini\u00e7\u00e3o de fun\u00e7\u00e3o de ativa\u00e7\u00e3o (em <code>torch.nn.functional</code>) Breve Descri\u00e7\u00e3o <code>Sigmoid</code> <code>sigmoid()</code> Calcula o sigmoide da entrada. A fun\u00e7\u00e3o sigmoide \u00e9 definida como:\u03c3(x) = 1 / (1 + exp(-x)) <code>Tanh</code> <code>tanh()</code> Calcula a tangente hiperb\u00f3lica da entrada. A fun\u00e7\u00e3o \u00e9 definida como:tanh(x) = 2\u03c3(2x) - 1 <code>ReLU</code> <code>relu()</code> Unidade Linear Retificada. Definida como:relu(x) = max(0, x) <code>LeakyReLU</code> <code>leaky_relu()</code> Unidade Linear Retificada Vazada. Definida como:leakyrelu(x) = max(\u03b1x, x)\u03b1 \u00e9 um hiperpar\u00e2metro (geralmente 0.01). <code>ELU</code> <code>elu()</code> Unidade Linear Exponencial. Definida como:elu(x) = \u03b1(exp(x) \u2013 1) se x &lt; 0elu(x) = x se x \u2265 0 <code>Softmax</code> <code>softmax()</code> Usada como camada final para problemas de classifica\u00e7\u00e3o multiclasse. Retorna a probabilidade de cada classe. <code>Softplus</code> <code>softplus()</code> Aproxima\u00e7\u00e3o suave da fun\u00e7\u00e3o ReLU."},{"location":"torch_layers/","title":"Torch layers","text":"In\u00a0[1]: Copied! <pre>import torch\nfrom torch import nn\n</pre> import torch from torch import nn <p>Agora, com a importacao do m\u00f3dulo <code>torch.nn</code> podemos come\u00e7ar a usar qualquer uma das camadas que o Pytorch, disponibiliza, criando uma inst\u00e2ncia dela e passando os argumentos necess\u00e1rios. Agora, vamos dar uma olhada nas camadas mais b\u00e1sicas do Pytorch e como us\u00e1-las.</p> In\u00a0[2]: Copied! <pre>dense_layer = nn.Linear(300, 100)\n</pre> dense_layer = nn.Linear(300, 100) In\u00a0[4]: Copied! <pre># A layer possui 32 \"mapas\" na entrada e 64 na sa\u00edda com stride 1 e padding 0.\nconv_layer1 = nn.Conv2d(32, 64,(5,5), stride = 1, padding = 0)\n\nconv_layer1\n</pre> # A layer possui 32 \"mapas\" na entrada e 64 na sa\u00edda com stride 1 e padding 0. conv_layer1 = nn.Conv2d(32, 64,(5,5), stride = 1, padding = 0)  conv_layer1 Out[4]: <pre>Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))</pre> In\u00a0[5]: Copied! <pre># Vamos criar uma camada de Max Pooling, com kernel de tamanho (3x3) com stride de 3 e padding 0.\n\nmax_pool_1 = nn.MaxPool2d((3,3), stride = 3, padding = 0)\n</pre> # Vamos criar uma camada de Max Pooling, com kernel de tamanho (3x3) com stride de 3 e padding 0.  max_pool_1 = nn.MaxPool2d((3,3), stride = 3, padding = 0)"},{"location":"torch_layers/#camadas-no-pytorch","title":"Camadas no Pytorch\u00b6","text":"<p>Uma camada no contexto de redes neurais, Deep Learning ou at\u00e9 mesmo Machine Learning \u00e9 um dos componente mais b\u00e1sicos e fundamentais para qualquer modelo ou rede neural. Isso pois a grosso modo uma rede neural pode ser mais ou menos considerada como sendo uma pilha de camadas que vai compondo esta rede. O Pytorch tem classes ja implementadas para as camadas mais comumente utilizadas e a diante veremos como podemos trabalhar com essas classes pr\u00e9 prontas que implementam estas camadas.</p>"},{"location":"torch_layers/#importando-o-torchnn","title":"Importando o torch.nn\u00b6","text":"<p>Todas as camadas do Pytorch estao localizadas no m\u00f3dulo torch.nn do Pytorch. Portanto, \u00e9 muito importante lembrar de importar este m\u00f3dulo antes de usar essas camadas para construir a rede neural ou modelo.</p>"},{"location":"torch_layers/#dense-layer-camada-densa","title":"Dense Layer (Camada densa)\u00b6","text":"<p>A dense layer, ou camada densa, \u00e9 uma camada fully connected mais comumente utilizadas em redes neurais. Podemos criar uma dense layer instanciando a <code>Linearclass</code>, e podemos fazer isso fornecendo o n\u00famero de entradas para a camada e o n\u00famero de sa\u00eddas dela, vamos ver um exemplo.</p>"},{"location":"torch_layers/#exemplo","title":"Exemplo:\u00b6","text":"<p>Neste exemplo, vamos criar uma dense layer que recebe como entrada 300 valores e que na sa\u00edda tenha 100 valores ou \"100 classes\". Ou melhor dizendo esta camada ter\u00e1 100 neur\u00f4nios e recebe como entrada uma camada anterior que possui 300 neur\u00f4nios.</p>"},{"location":"torch_layers/#camada-convolucional","title":"Camada Convolucional\u00b6","text":"<p>As camadas convolucionais sao as camadas mais utilizadas nas CNNs (Convolutional Neural Network) juntamente com as camadas de Pooling. Elas servem para o prop\u00f3sito de extrair recursos de uma regiao/local de uma determinada entrada. Como se trata de uma das camadas mais importantes dentro do contexto das CNNs, o Pytorch fornece v\u00e1rias classes para que seja poss\u00edvel executar concolucoes em dimensoes variadas. Vamos ver como criar uma camada convolucional na pr\u00e1tica</p> <p>Exemplo:</p> <p>Neste exemplo, vamo criar uma camada convolucional 2D com um kernel de (5x5) que se aplica a 32 mapas de recrusos da Camada Convolucional anterior e produz 64 mapas de recursos. Ou falando de uma outra maneira, a camada cont\u00e9m 64 kernels de tamanho (5x5) com parametros que se aplicam a todos os 32 mapas de recursos da entrada. Esta camada convlucional possui um stride(passo) de 1 e padding igual a 0.</p>"},{"location":"torch_layers/#camadas-de-pooling","title":"Camadas de Pooling\u00b6","text":"<p>As camadas de pooling sao tamb\u00e9m outro tipo de camada muito utilizado em CNNs. Ela tem o papel de reduzir a dimensionalidade dos mapas de recursos equanto preservam simultaneamente a maior quantidade de informacoes da imagem de entrada. O Pytorch oferece assim como nas camadas convolucionais uma s\u00e9rie de classes para tratar e executar diversos tipos de operacoes de pooling nas mais v\u00e1riadas dimensoes. Vamos ver um exemplo de como podemos usar uma camada de pooling 2D.</p>"},{"location":"torch_layers/#camadas-mais-utilizadas-em-deep-learning-e-pytorch","title":"Camadas mais utilizadas em Deep Learning e Pytorch\u00b6","text":"<p>Vamos, apresentar abaixo algumas das camadas mais utilizadas no Pytorch e algumas que ja discutimos anteriormente:</p> Classe de camada Breve Descri\u00e7\u00e3o <code>nn.Dropout</code> Elimina aleatoriamente alguns neur\u00f4nios da camada com uma determinada probabilidade durante o treinamento. <code>nn.AvgPool2d</code> Aplica o Average Pooling sobre a entrada. <code>nn.BatchNorm2d</code> Aplica Normaliza\u00e7\u00e3o em Lote sobre um lote de imagens com m\u00faltiplos canais. Por exemplo: Um lote de imagens RGB. <code>nn.RNN</code> Cria uma camada de rede neural recorrente."},{"location":"torch_loss/","title":"Torch loss","text":"In\u00a0[3]: Copied! <pre>import torch\nimport torch.nn as nn\n\nloss_fn = nn.MSELoss()\n\nloss_fn\n</pre> import torch import torch.nn as nn  loss_fn = nn.MSELoss()  loss_fn Out[3]: <pre>MSELoss()</pre> <p>No exemplo acima podemos ver que a utilizacao da funcao de perda MSE funcao de perda extremamente conhecida na teoria de aprendizado de m\u00e1quina e estatistica chamada de erro quadratico medio ou tamb\u00e9m conhecida como norma L2.</p> <p>Entretanto, al\u00e9m da loss function MSE, o Pytorch possui uma variedade de funcoes de perda, veja a seguir a uma simples tabela que mostra algumas funcoes de perda dispon\u00edveis no Pytorch.</p> Classe de Perda Breve Descri\u00e7\u00e3o Perda de Entropia Cruzada \u00c9 usada para calcular a perda em problemas de classifica\u00e7\u00e3o. Tamb\u00e9m \u00e9 conhecida como perda de log. Perda MSE (L2) \u00c9 usada para calcular a perda em problemas de regress\u00e3o com poucos outliers. Corresponde ao Erro Quadr\u00e1tico M\u00e9dio (norma L2 quadrada). Perda L1 \u00c9 usada para calcular a perda em problemas de regress\u00e3o com muitos outliers. Corresponde ao Erro Absoluto M\u00e9dio (norma L1)."},{"location":"torch_loss/#funcoes-de-perda-loss-function-no-pytorch","title":"Funcoes de perda (Loss function) no Pytorch\u00b6","text":"<p>Em sua essencia uma funcao de perda tem como objetivo ser intuitiva como o pr\u00f3prio nome j\u00e1 induz. Ou seja, ela \u00e9 utilizada para calcular a perda, que no contexto de redes neurais ela \u00e9 uma medida de quao bom o meu modelo est\u00e1 performando, ou a grosso modo posso dizer que \u00e9 uma medida que expressa quao distante ou quanto perto meu modelo est\u00e1 de aprender ou cumprir seu objetivo.</p> <p>As funcoes de perda em conjunto com otimizadores sao utilizadas para ajustar os par\u00e2metros de uma Rede Neural. Feita esta breve introducao, vamos ver quais sao as funcoes de perda dispon\u00edveis no Pytorch e como podemo utiliz\u00e1-las.</p>"},{"location":"torch_loss/#importanto-as-loss-functions","title":"Importanto as Loss functions\u00b6","text":"<p>As funcoes de perda assim como as camadas funcoes de ativacao e etc, tamb\u00e9m estao implementadas ou incorporadas pelo m\u00f3dulo <code>torch.nn</code>, portanto se importamos o <code>torch.nn</code> para outro pr\u00f3posito como por exemplo construir as camadas da rede neural, ja estamos importando tamb\u00e9m as funcoes de perda.</p> <pre>    import torch.nn as nn\n</pre>"},{"location":"torch_loss/#utilizando-as-loss-functions","title":"Utilizando as Loss functions\u00b6","text":"<p>Assim como muitas coisas no Pytorch, para utilizarmos as funcoes de perda isso nao \u00e9 muito diferente, pois para utiliz\u00e1-las precisamos criar um objeto ou uma instancia da funcao de perda. Veja na pr\u00e1tica como isso \u00e9 simples.</p>"},{"location":"torch_loss/#funcao-de-perda-no-metodo-forward-feedforward","title":"Funcao de perda no m\u00e9todo forward (feedforward)\u00b6","text":"<p>Nas redes neurais feedforward, a funcao de perda \u00e9 aplicada no m\u00e9todo forward() da classe que representa a Rede Neural e ela \u00e9 utilizada neste m\u00e9todo para calcular a perda. E podemos fazer isso chamando o objeto <code>loss_fn</code> que criamos anteriormente, e aplicando a ele a sa\u00edda da rede neural e os r\u00f3tulos (labels) reais dos dados de treinamento, tudo isso dentro de um loop for, pois com isso conseguiremos observar como o modelo esta aprendendo. Entao, basicamente na pr\u00e1tica temos:</p> <pre>    loss = loss_fn(output, label)\n</pre>"},{"location":"torch_loss/#funcao-de-perda-no-metodo-backward-backpropagation","title":"Funcao de perda no m\u00e9todo backward (backpropagation)\u00b6","text":"<p>J\u00e1 quando nao estamos utilizando redes feedforward, ou seja, quando estamos trabalhando com redes neurais recorrentes ou redes neurais convolucionais, a funcao de perda \u00e9 aplicada no m\u00e9todo backward() da classe que representa a Rede Neural. No backward pass, calculamos os gradientes para cada par\u00e2metro do modelo. Logo, podemos calcular os gradientes do backpropagation chamando o m\u00e9todo <code>backward()</code> na perda funcao de perda <code>loss</code>.</p> <pre>    loss.backward()\n</pre>"},{"location":"torch_loss/#conclusoes-sobre-as-loss-functions","title":"Conclusoes sobre as Loss functions\u00b6","text":"<p>A Perda ou loss \u00e9 basicamente um valor num\u00e9rico. Este valor \u00e9 o resultado que \u00e9 uma fun\u00e7\u00e3o da sa\u00edda prevista do modelo e da verdade fundamental para um conjunto particular de par\u00e2metros do modelo.</p> <p>Para simplificar, \u00e9 uma maneira de medir o quanto a sa\u00edda real \u00e9 diferente da sa\u00edda alvo. Portanto, uma fun\u00e7\u00e3o de perda pode ser matematicamente definida como:</p> <p>$$ loss = f(y, \\hat{y}) $$</p> <p>Onde:</p> <ul> <li>$y$ \u00e9 a sa\u00edda real (label)</li> <li>$\\hat{y}$ \u00e9 a sa\u00edda prevista (output)</li> <li>$f$ \u00e9 a fun\u00e7\u00e3o de perda</li> </ul> <p>Sendo tudo isso utilizado com o objetivo de treinar o modelo que \u00e9 a grosso modoatualizar iterativamente os par\u00e2metros do modelo de modo que esta \"loss\" seja minimizada ou o mais pr\u00f3xima de zero.</p> <p>E isso \u00e9 obtido por meio do algoritmo de retropropaga\u00e7\u00e3o(backpropagation). A perda \u00e9 retropropagada da camada de sa\u00edda para a camada de entrada e os gradientes s\u00e3o calculados para cada um desses par\u00e2metros. Esses gradientes s\u00e3o ent\u00e3o utilizados \u200b\u200bpara atualizar os par\u00e2metros do modelo.</p>"},{"location":"torch_neuralnetwork/","title":"Torch neuralnetwork","text":"In\u00a0[6]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MyNeuralNetwork(nn.Module):\n    def __init__(self):\n    # Arquitetura de como funciona a ANN\n        pass\n    def forward(self, x):\n        # implementacao dos dados\n        pass\n</pre> import torch import torch.nn as nn import torch.nn.functional as F  class MyNeuralNetwork(nn.Module):     def __init__(self):     # Arquitetura de como funciona a ANN         pass     def forward(self, x):         # implementacao dos dados         pass  <p>Entao, o que temos acima \u00e9 um exemplo, bem simplificado apenas mostrando o construtor e o m\u00e9todo <code>forward()</code>. Vamos agora implementar a arquitetura da rede e termos de fato um c\u00f3digo mais completo e coerente com o que seria a construcao de uma ANN, no Pytorch.</p> In\u00a0[2]: Copied! <pre>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass MyNeuralNetwork(nn.Module):\n    def __init__(self):\n        super(MyNeuralNetwork, self).__init__()\n        dense_layer1 = nn.Linear(784, 300)\n        dense_layer2 = nn.Linear(300, 100)\n        dense_layer3 = nn.Linear(100, 10)\n</pre> import torch.nn as nn import torch.nn.functional as F  class MyNeuralNetwork(nn.Module):     def __init__(self):         super(MyNeuralNetwork, self).__init__()         dense_layer1 = nn.Linear(784, 300)         dense_layer2 = nn.Linear(300, 100)         dense_layer3 = nn.Linear(100, 10) In\u00a0[3]: Copied! <pre>def forward(self, x):\n   x = F.relu(self.dense_layer1)\n\n   x = F.relu(self.dense_layer2)\n\n   x = F.softmax(self.dense_layer3)\n\n   return x\n</pre> def forward(self, x):    x = F.relu(self.dense_layer1)     x = F.relu(self.dense_layer2)     x = F.softmax(self.dense_layer3)     return x <p>Agora, pronto ja temos nossa ANN implementada. devemos apenas instanciar ou inicializar a nossa classe e teremos nosso modelo de ANN.</p> In\u00a0[4]: Copied! <pre>ann_model = MyNeuralNetwork()\n</pre> ann_model = MyNeuralNetwork() In\u00a0[9]: Copied! <pre>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass ANN_net(nn.Module):\n    def __init__(self):\n        super(ANN_net, self).__init__()\n        self.dense_layer1 = nn.Linear(784, 300)\n        self.dense_layer2 = nn.Linear(300, 100)\n        self.dense_layer3 = nn.Linear(100, 10)\n        \n    def forward(self, x):\n        x = F.relu(self.dense_layer1)\n        x = F.relu(self.dense_layer2)\n        x = F.softmax(self.dense_layer3)\n        return x\n\nann_model = ANN_net()\nann_model\n</pre> import torch.nn as nn import torch.nn.functional as F  class ANN_net(nn.Module):     def __init__(self):         super(ANN_net, self).__init__()         self.dense_layer1 = nn.Linear(784, 300)         self.dense_layer2 = nn.Linear(300, 100)         self.dense_layer3 = nn.Linear(100, 10)              def forward(self, x):         x = F.relu(self.dense_layer1)         x = F.relu(self.dense_layer2)         x = F.softmax(self.dense_layer3)         return x  ann_model = ANN_net() ann_model Out[9]: <pre>ANN_net(\n  (dense_layer1): Linear(in_features=784, out_features=300, bias=True)\n  (dense_layer2): Linear(in_features=300, out_features=100, bias=True)\n  (dense_layer3): Linear(in_features=100, out_features=10, bias=True)\n)</pre>"},{"location":"torch_neuralnetwork/#criando-uma-rede-neural-no-pytorch","title":"Criando uma rede neural no Pytorch\u00b6","text":"<p>Uma das funcionalidades mais b\u00e1sicas de qualquer biblioteca de Deep Learning \u00e9 proporcionar para os usu\u00e1rios uma maneira mais simplificada de se criar redes neurais. E este \u00e9 justamente o que o Pytorch faz.</p> <p>Vamos entao, neste cap\u00edtulo, ver como podemos criar uma rede neural no Pytorch. Neste caso, vamos implementar um modelo mais simples de rede neural ou seja, vamos ver na pr\u00e1tica como criar um modelo feedforward de rede neural (ANN) utilizando o Pytorch.</p> <p>Bom, o primeiro passo para criar uma rede neural no Pytorch \u00e9 criar uma classe para a rede neural, e depois inst\u00e2nciar</p> <p>Por\u00e9m antes disso \u00e9 importante importar alguns m\u00f3dulos como por exemplo:</p> <pre>import torch.nn as nn\nimport torch.nn.functional as F\n</pre>"},{"location":"torch_neuralnetwork/#criando-sua-propria-rede-neural","title":"Criando sua pr\u00f3pria rede neural\u00b6","text":"<p>Vimos, em alguns m\u00f3dulos anteriores como criar seu pr\u00f3prio DataLoader e at\u00e9 mesmo a como criar e trabalhar com uma implementacao simples de uma rede neural. Agora vamos detalhar um pouco mais como criar sua pr\u00f3pria ANN (rede neural artificial) do tipo feedfoward.</p> <p>Para criar um modelo pr\u00f3prio, normalmente devemos herdar da classe <code>nn.Module</code>, e como estamos trabalhando com um modelo de ANN. \u00c9 necess\u00e1rio que facamos a implementacao do construtor da classe da minha ANN e tamb\u00e9m do m\u00e9todo <code>forward()</code>. Algo que podemos tirar como conclusao disso \u00e9 que a arquitetura do modelo ser\u00e1 definida pela classe que criarmos, e o fluxo de como os dados sao propagados pela ANN \u00e9 definido pelo m\u00e9todo <code>forward()</code> Vamos, implementar na pr\u00e1tica para ter um melhor entendimento do que foi exposto neste paragr\u00e1fo.</p>"},{"location":"torch_neuralnetwork/#arquitetura-do-modelo","title":"Arquitetura do Modelo\u00b6","text":"<p>Como mencionado anteriormente, a arquitetura do modelo \u00e9 definida no construtor da classe. E sendo assim, as camadas da rede sao adicionada como atributos ou (\"variaveis\") dentro do construtor.</p> <p>Para o nosso modelo em questao, vamos utilizar apenas 3 camadas densas que consistem:</p> <ul> <li>dense_layer1: A primeira camada densa contendo 784 entradas e 300 sa\u00eddas.</li> <li>dense_layer2: A segunda camada densa contendo 300 entradas e 100 sa\u00eddas.</li> <li>dense_layer3: A terceira camada densa contendo 100 entradas e 10 sa\u00eddas.</li> </ul> <p>Para fazermos isso, devemos herdar da classe <code>nn.Module</code> e herdar tamb\u00e9m seu construtor atrav\u00e9s de <code>super()</code>. E isso dever\u00e1 acontecer sempre que formos criar uma nova classe para um modelo de rede neural. Esta chamada \u00e9 fundamental para o que Pytorch possa inicializar corretamente a classe pai <code>nn.Module</code>, configurando o sistema de registro autom\u00e1tico de parametros, para que tenhamos se necess\u00e1rio o mecanismo de autograd para o backpropagation e tamb\u00e9m o gerenciamento correto dos parametros treinaveis. Resumindo, precisamos sempre ter a estrutura:</p> <pre>class MyNeuralNetwork(nn.Module):\n    def __init__(self):\n        super(MyNeuralNetwork, self).__init__()\n        # Modelo da rede aqui.\n</pre> <p>Vamos agora entao implementar de fato a arquitetura da ANN:</p>"},{"location":"torch_neuralnetwork/#fluxo-de-dados-atraves-do-modelo","title":"Fluxo de dados atrav\u00e9s do modelo\u00b6","text":"<p>Vamos, agora implementar o m\u00e9todo <code>forward()</code> que como vimos \u00e9 repons\u00e1vel por propagar os dados ao longo da ANN. Quando mencionamos propagar queremos dizer a maneira como os dados da entrada serao modificados por exemplo, quais camadas de ativacao serao utilizadas, tudo isso \u00e9 definido no m\u00e9todo <code>forward()</code>. Para este exemplo, iremos aplicar 2 funcoes de ativacao nas sa\u00eddas das dense_layer1 e dense_layer2 aplicaremos <code>ReLU</code> e por fim na dense_layer3 aplicaremos <code>Softmax</code>. Opcionalmente podemos executar operacoes para modificar os dados do tensor de entrada para o formato desejado. Portanto, vamos implementar</p>"},{"location":"torch_neuralnetwork/#codigo-completo","title":"C\u00f3digo completo:\u00b6","text":""},{"location":"torch_optimizer/","title":"Torch optimizer","text":"<p>Bom, entao para utilizarmos um otimizador no Pytorch, ap\u00f3s termos importado o m\u00f3dulo, devemos criar uma instancia do otimizador. Vamos ver um exemplo na pr\u00e1tica.</p> <p>Para este exemplo, vamos utilizar uma inst\u00e2ncia da classe <code>SGD</code> que \u00e9 uma implementacao do algoritmo de otimizacao Stochastic Gradient Descent.</p> <p>Logo, ao criar uma instancia do otimizador, devemos fornecer os par\u00e2metros do modelo e tamb\u00e9m a learning rate como argumento. Al\u00e9m disso, \u00e9 importante fornecer outros hiperparametros como argumento para o otimizador, pois dependendo do otimizador, eles podem ser necess\u00e1rios. Para o caso do SGD, devemos apenas fornecer os par\u00e2metros do modelo e a learning rate.</p> In\u00a0[2]: Copied! <pre>import torch\nfrom torch import optim\n\nmodel = torch.nn.Linear(10, 1)\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n</pre> import torch from torch import optim  model = torch.nn.Linear(10, 1) optimizer = optim.SGD(model.parameters(), lr=0.01) <p>Al\u00e9m do SGD, no Pytorch temos uma variedade de otimizadores dispon\u00edveis. Vamos ver alguns exemplos:</p> Classe Otimizadora Nome do Otimizador SGD Descida de Gradiente Estoc\u00e1stico. O Otimizador de Momentum pode ser usado passando o argumento <code>momentum</code> para SGD. O Otimizador de Gradiente Acelerado de Nesterov (NAG) pode ser usado passando o argumento <code>nesterov=True</code> para SGD. Adagrad Otimizador de Gradiente Adapt\u00e1vel. RMSprop Otimizador de propaga\u00e7\u00e3o da raiz quadrada m\u00e9dia, mas \u00e9 comumente conhecido como RMSProp. Adam Otimizador de Estimativa de Momento Adaptativo, mas \u00e9 comumente conhecido como Adam. In\u00a0[3]: Copied! <pre>optimizer.zero_grad()\n</pre> optimizer.zero_grad() In\u00a0[4]: Copied! <pre>optimizer.step()\n</pre> optimizer.step()"},{"location":"torch_optimizer/#otimizadores-no-pytorch","title":"Otimizadores no Pytorch\u00b6","text":"<p>Os otimizadores no Pytorch s\u00e3o respons\u00e1veis por atualizar os par\u00e2metros do modelo durante o treinamento ou seja iterativamente. Normalmente, eles s\u00e3o utilizados juntos com as loss functions e tem o objetivo de ajustar os pesos e vieses do modelo para minimizar essas funcoes de perda.</p>"},{"location":"torch_optimizer/#importando-otimizadores","title":"Importando otimizadores\u00b6","text":"<p>Para utilizarmos um otimizador no Pytorch, precisamos importa-lo. E o m\u00f3dulo que cont\u00e9m as implementcoes dos otimizadores \u00e9 o m\u00f3dulo <code>torch.optim</code> e \u00e9 justamente este m\u00f3dulo que vamos utilizar para importar os otimizadores. Portanto, para importar os otimizadores devemos fazer o seguinte:</p> <pre>    import torch.optim as optim\n</pre> <p>Ou podemos importar otimizadores de forma direta:</p> <pre>    from torch import optim\n</pre>"},{"location":"torch_optimizer/#zero-grad","title":"Zero Grad\u00b6","text":"<p>No Pytorch, por padrao os gradientes calculados durante o backpropagation no loop de treino continuam sendo acumulados. Como isso \u00e9 algo indesejav\u00e9l, \u00e9 necess\u00e1rio zerar os gradientes ap\u00f3s cada loop de treino. E para este contexto o m\u00e9todo <code>zero_grad()</code> \u00e9 muito \u00fatil. Pois, ele define todos os gradientes como 0 e este m\u00e9todo \u00e9 implementado por todos os otimizadores dispon\u00edveis no Pytorch.</p>"},{"location":"torch_optimizer/#ajuste-dos-pesos","title":"Ajuste dos pesos\u00b6","text":"<p>Ap\u00f3s os gradientes serem calculados, devemos ajustar os pesos do modelo e isso pode ser feito com o m\u00e9todo <code>step()</code>. Este m\u00e9todo \u00e9 implementado por todos os otimizadores dispon\u00edveis no Pytorch.</p>"},{"location":"torch_sequential/","title":"Torch sequential","text":"<p>Vamos construir uma rede neural utiliza nn.Sequential() para criar uma pilha de 3 camadas densas e 2 camadas com funcao de ativacao ReLU em uma ordem especificada.</p> In\u00a0[8]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndensenet = nn.Sequential(\n    nn.Linear(784,300),\n    nn.ReLU(),\n    nn.Linear(300,100),\n    nn.ReLU(),\n    nn.Linear(100,10)\n)\n\ndensenet\n</pre> import torch import torch.nn as nn import torch.nn.functional as F  densenet = nn.Sequential(     nn.Linear(784,300),     nn.ReLU(),     nn.Linear(300,100),     nn.ReLU(),     nn.Linear(100,10) )  densenet  Out[8]: <pre>Sequential(\n  (0): Linear(in_features=784, out_features=300, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=300, out_features=100, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=100, out_features=10, bias=True)\n)</pre> In\u00a0[9]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass NeuralNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.densenet = nn.Sequential (\n            nn.Linear(784,300),\n            nn.ReLU(),\n            nn.Linear(300,100),\n            nn.ReLU(),\n            nn.Linear(100,10)\n        )\n\n    def forward(self, x):\n        x = self.densenet(x)\n\n        return x\n    \nann_model = NeuralNet()\n\nann_model\n</pre> import torch import torch.nn as nn import torch.nn.functional as F  class NeuralNet(nn.Module):     def __init__(self):         super().__init__()         self.densenet = nn.Sequential (             nn.Linear(784,300),             nn.ReLU(),             nn.Linear(300,100),             nn.ReLU(),             nn.Linear(100,10)         )      def forward(self, x):         x = self.densenet(x)          return x      ann_model = NeuralNet()  ann_model Out[9]: <pre>NeuralNet(\n  (densenet): Sequential(\n    (0): Linear(in_features=784, out_features=300, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=300, out_features=100, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=100, out_features=10, bias=True)\n  )\n)</pre> <p>Veja, que criar redes neurais utilizando o Sequential, torna o c\u00f3digo mais compacto e simples de escrever, tornando visualmente um pouco mais agradav\u00e9l. Entretanto, nao existe uma obrigatoriedade de se utilizar o Sequential(), como vimos ele nao \u00e9 a \u00fanica maneira que existe para se criar com redes neurais no Pytorch, existindo outras maneiras a ess\u00eancia \u00e9 encontrar qual maneira \u00e9 a mais adequada.</p>"},{"location":"torch_sequential/#a-api-sequential-do-pytorch","title":"A API Sequential do Pytorch\u00b6","text":"<p>J\u00e1 vimos, at\u00e9 aqui como podemos criar uma Rede neural. Vamos agora aprender um pouco sobre a API Sequential e como criar uma rede neural utilizando ela.</p> <p>Utilizar a API Sequential para criar redes neurais, proporciona uma abordagem mais simples, f\u00e1cil e compacto, reduzindo a redund\u00e2ncia e algumas poss\u00edveis complicacoes que possam surgir.</p>"},{"location":"torch_sequential/#mas-entao-o-que-e-a-api-sequential","title":"Mas entao o que \u00e9 a API Sequential?\u00b6","text":"<p>A API Sequential permite que o usu\u00e1rio no momento da criacao da Rede Neural possa empilhar camadas da Rede neural umas sobre as outras. Essa pilha de camadas \u00e9 tratada como sendo uma \u00fanica camada ao criar a arquitetura da Rede Neural. Ou seja, a API entao sequential permite \"encapsularmos\" todas as layers da arquitetura da rede neural tratando elas como uma c\u00e1psula s\u00f3. Um ponto interessante \u00e9 que a Sequential API tamb\u00e9m \u00e9 tratada como uma \u00fanica camada ao definir o fluxo de dados pelo modelo. O fluxo de dados de uma camada para a outra est\u00e1 na mesma ordem em que sao passadas as camada para nn.Sequential().</p>"},{"location":"torch_sequential/#criando-uma-rede-neural-completa-com-nnsequential","title":"Criando uma rede neural completa com nn.Sequential()\u00b6","text":"<p>Criar uma Rede Neural usando a API Sequencial n\u00e3o \u00e9 muito diferente de criar uma Rede Neural de outra forma. As etapas para criar uma Rede Neural usando a API Sequencial sao as mesmas que vimos anteriormente:</p> <ol> <li>Criar uma classe para sua Rede Neural e criando um construtor da classe e herdar a classe pai nn.Module que implementa tudo que precisamos para criar uma Rede Neural.</li> <li>Definir as camadas que compoe a arquitetura da Rede Neural ap\u00f3s ter herdade o construtor da classe pai nn.Module.</li> <li>Por fim, implementar o m\u00e9todo forward() que ir\u00e1 definir o fluxo de dados(camadas de ativacao, poolings, etc) atrav\u00e9s do modelo.</li> </ol> <p>Com isso podemos criar uma Rede Neural completa utilizando a API Sequential. Sendo assim, vamos criar uma Rede Neural completa utilizando a API Sequential, utilizando a denselayer que criamos anteriormente.</p>"},{"location":"torch_training/","title":"Torch training","text":"In\u00a0[1]: Copied! <pre>import torch\nfrom torch import optim\nimport torch.nn as nn\n\nepochs = 10\nfor epoch in range(epochs):\n    for batch in train_loader:\n        optim.zero_grad()\n        input, label = batch\n        output = mynet(input)\n        loss = loss_function(output, label)\n        loss.backward()\n        optim.step()\n</pre> import torch from torch import optim import torch.nn as nn  epochs = 10 for epoch in range(epochs):     for batch in train_loader:         optim.zero_grad()         input, label = batch         output = mynet(input)         loss = loss_function(output, label)         loss.backward()         optim.step() In\u00a0[\u00a0]: Copied! <pre>model = mynet()\nfor epoch in range(epochs):\n    # Performance do treino para cada epoch(epoca)\n    train_loss = 0\n    model.train()\n\n    # Loop de treino\n    for batch in train_dataloader:\n        optim.zero_grad()\n        input, label = batch\n        output = mynet(input)\n        loss = loss_function(output, label)\n        loss.backward()\n        optim.step()\n        train_loss += loss.item()\n    # Performance do validacao para cada epoch(epoca)\n    validation_loss = 0\n    model.eval()\n    \n    # Loop de validacao\n    for batch in validation_dataloader:\n        input, label = batch\n        output = mynet(input)\n        loss = loss_function(output, label)\n        validation_loss += loss.item()\n\n    # Calculo da media de perda para cada epoch(epoca)\n    train_loss_avg = train_loss / len(train_dataloader)\n    validation_loss = validation_loss / len(validation_dataloader)\n    \n</pre> model = mynet() for epoch in range(epochs):     # Performance do treino para cada epoch(epoca)     train_loss = 0     model.train()      # Loop de treino     for batch in train_dataloader:         optim.zero_grad()         input, label = batch         output = mynet(input)         loss = loss_function(output, label)         loss.backward()         optim.step()         train_loss += loss.item()     # Performance do validacao para cada epoch(epoca)     validation_loss = 0     model.eval()          # Loop de validacao     for batch in validation_dataloader:         input, label = batch         output = mynet(input)         loss = loss_function(output, label)         validation_loss += loss.item()      # Calculo da media de perda para cada epoch(epoca)     train_loss_avg = train_loss / len(train_dataloader)     validation_loss = validation_loss / len(validation_dataloader)"},{"location":"torch_training/#treinando-um-modelo-no-pytorch","title":"Treinando um modelo no Pytorch\u00b6","text":"<p>At\u00e9 o dado momento vimos, como construir a arquitetura da rede neural ou modelo, vimos tamb\u00e9m cada m\u00f3dulo, classe e m\u00e9todo desacoplado e entendemos um pouquinho das suas particularidades. Vimos tamb\u00e9m sobre os otimizadores as funcoes de perda e todo o necess\u00e1rio para ajustarmos os pesos durante o processo de treinamento de uma rede neural.</p> <p>Agora para fechar o \"Loop\" devemos, ver entao como de fato podemos treinar uma rede neural para que ela aprenda. Vamos entao ver como construir um loop de treino para treinar um modelo no Pytorch:</p>"},{"location":"torch_training/#escrevendo-um-loop-de-treinamento-usando-pytorch","title":"Escrevendo um loop de treinamento usando Pytorch\u00b6","text":"<p>Vamos, ver como \u00e9 poss\u00edvel construir um loop de treinamento b\u00e1sico utilizando Pytorch:</p>"},{"location":"torch_training/#detalhando-o-script-de-treino","title":"Detalhando o script de treino\u00b6","text":"<p>Vamos detalhar o que o nosso script de treino faz de fato:</p> <pre>for epoch in range(epochs)  # Executa o loop de treino por um determinado n\u00famero de vezes ao longo do dataset.\n\nfor batch in train_loader # Faz com que o loop de treino seja executado para cada lote/batelada(batch) do conjunto de dados.\n\noptim.zero_grad() # Define o valor de todos os gradientes para o otimizador como sendo zero.\n\ninput, label = batch # Desempacota a \"tupla\" e extrai os valores das entradas (input) e r\u00f3tulos(label)\n\noutput = mynet(input) # Alimenta as entradas do objeto da rede neural(modelo) para prever o output.\n\nloss = loss_function(output, label) # Calcula a funcao de perda para tentar estimar 'qu\u00e3o longe' est\u00e1 o output da label(verdadeiro).\n\nloss.backward() # Calcula o gradiente para o backpropagation para cada parametro do modelo (mynet).\n\noptim.step() # Ajusta os pesos do modelo com base no gradiente calculado.\n</pre>"},{"location":"torch_training/#validando-o-modelo-no-pytorch","title":"Validando o modelo no Pytorch\u00b6","text":"<p>Perfeito, j\u00e1 entendemos como construir um script ou  loop de treino para ser poss\u00edvel treinar um modelo utilizando Pytorch. Ap\u00f3s, treinar o modelo, \u00e9 muito importante ter um dataset de valida\u00e7\u00e3o (\"val\") para que possamos ter uma ideia de como o modelo est\u00e1 performando. Sendo assim, vamos ver como podemos escrever um script para validar o modelo que foi treinado anteriormente pelo nosso loop de treino.</p>"},{"location":"transform_tensors/","title":"Transform tensors","text":"In\u00a0[1]: Copied! <pre>import torch\n</pre> import torch In\u00a0[8]: Copied! <pre>v = torch.ones(50)\n\nv\n</pre> v = torch.ones(50)  v Out[8]: <pre>tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])</pre> In\u00a0[4]: Copied! <pre>v.shape\n</pre> v.shape Out[4]: <pre>torch.Size([50])</pre> <p>Vamos, entao agora ver na pr\u00e1tica como utilizar o <code>reshape()</code> e <code>view()</code> e tamb\u00e9m entender melhor o que ocorre no back-end de ambos estes m\u00e9todos que tem algumas diferencas entre si.</p> In\u00a0[10]: Copied! <pre>r_v = torch.reshape(v, (25,2))\n</pre> r_v = torch.reshape(v, (25,2)) In\u00a0[11]: Copied! <pre>r_v.shape\n</pre> r_v.shape Out[11]: <pre>torch.Size([25, 2])</pre> In\u00a0[12]: Copied! <pre>r_v\n</pre> r_v Out[12]: <pre>tensor([[1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.]])</pre> <p>Como citado anteriormente podemos modificar o tensor de quaisquer forma desde que seja respeitado a quantidade de elementos. Neste caso temos que o tensor original possui 50 elementos, logo os tensores modificados pelo <code>reshape</code> podem possuir quaisquer dimensao desde que a multiplicacao das dimensoes seja 50. Vamos ver na pr\u00e1tica:</p> In\u00a0[13]: Copied! <pre>t_2 = torch.reshape(v,(2,5,5))\n</pre> t_2 = torch.reshape(v,(2,5,5)) In\u00a0[14]: Copied! <pre>t_2.shape\n</pre> t_2.shape Out[14]: <pre>torch.Size([2, 5, 5])</pre> In\u00a0[15]: Copied! <pre>t_2\n</pre> t_2 Out[15]: <pre>tensor([[[1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.]],\n\n        [[1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.]]])</pre> <p>Entao, veja que o <code>reshape</code> \u00e9 muito flex\u00edvel e nao foi necess\u00e1rio, se preocupar se a estrutura de dados neste caso o <code>Tensor</code>, possuia uma estrutura de dados cont\u00edgua em mem\u00f3ria.</p> <p>Vamos, agora visualizar na pr\u00e1tica como funciona o <code>view</code> e vamos comentar um pouco das diferencas sutis na pr\u00e1tica entre o uso do <code>reshape()</code> e <code>view()</code></p> In\u00a0[16]: Copied! <pre>r = v.view(2,25)\n</pre> r = v.view(2,25) In\u00a0[17]: Copied! <pre>r.shape\n</pre> r.shape Out[17]: <pre>torch.Size([2, 25])</pre> In\u00a0[18]: Copied! <pre>t = v.view(5,5,2)\n</pre> t = v.view(5,5,2) In\u00a0[19]: Copied! <pre>t.shape\n</pre> t.shape Out[19]: <pre>torch.Size([5, 5, 2])</pre> <p>Se observarmos at\u00e9 aqui, se voce perceber nao existe diferenca significativa entre o <code>reshape</code> e <code>view</code> at\u00e9 aqui uma das diferencas notaveis \u00e9 que o <code>view</code> \u00e9 na implementacao muito provavelmente um m\u00e9todo de inst\u00e2ncia, pois o view \u00e9 um atributo do objeto tensor, j\u00e1 o <code>reashape</code> \u00e9 um m\u00e9todo do <code>Pytorch</code>.</p> <p>Al\u00e9m disso, para utilizarmos o <code>reshape</code> \u00e9 necess\u00e1rio passar o tensor o qual queremos modificar e uma tupla contendo o formato ou as dimensoes para qual queremos que este tensor se torne.</p> <p>Por\u00e9m como ja mencionado, esta nao \u00e9 uma das diferencas de fato. A diferenca que existe nao \u00e9 transparente para o usu\u00e1rio, e ocorre no back-end mais especificamente falando a n\u00edvel de mem\u00f3ria.</p> <p>Sendo assim, vamos tentar tornar isso transparente ou mais claro, mostrando mais claramente estas diferencas.</p> <p>Para fazer isso de maneira pr\u00e1tica vamos comecar tentando mostrar como funciona a estrutura de dados do tensor.</p> In\u00a0[21]: Copied! <pre>m = torch.rand(2,3)\n\nm\n</pre> m = torch.rand(2,3)  m Out[21]: <pre>tensor([[0.1155, 0.6781, 0.7776],\n        [0.9788, 0.0502, 0.2136]])</pre> <p>Entao veja que aqui, vamos utilizar o m\u00e9todo <code>is_contiguous()</code> que ir\u00e1 nos retornar <code>True</code> caso a estrutura de dados tenha tensor um tensor onde seus elementos estao organizados de maneira cont\u00edgua na mem\u00f3ria.</p> In\u00a0[22]: Copied! <pre>m.is_contiguous()\n</pre> m.is_contiguous() Out[22]: <pre>True</pre> <p>Agora veja que se aplicarmos alguma operacao aritim\u00e9tica ou alguma transformacao neste mesmo tensor, que seus elementos estao organizados de maneira cont\u00edgua na mem\u00f3ria, iremos ver que ele nao ser\u00e1 mais cont\u00edguo pois a n\u00edvel de mem\u00f3ria estes dados sao deslocados para uma outra regiao mem\u00f3ria que \"suporte\" armazenar e manipular melhor estes dados.</p> In\u00a0[23]: Copied! <pre>t = m.T\n</pre> t = m.T <p>Entao, perceba que aplicamos uma operacao de transposicao, no tensor <code>m</code> e vimos que o m\u00e9todo <code>is_contiguous()</code> retornou False, mostrando que a n\u00edvel de mem\u00f3ria nao temos mais dados contiguos.</p> In\u00a0[24]: Copied! <pre>t.is_contiguous()\n</pre> t.is_contiguous() Out[24]: <pre>False</pre> <p>Agora, se tentarmos modificar o tensor, utilizando o <code>view</code> iremos receber um erro justamente pelo fato de que o tensor nao \u00e9 cont\u00edguo em mem\u00f3ria. Veja</p> In\u00a0[26]: Copied! <pre>t.view(1,2,3)\n</pre> t.view(1,2,3) <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[26], line 1\n----&gt; 1 t.view(1,2,3)\n\nRuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.</pre> <p>Entretanto, se tentarmos realizar o mesmo procedimento de transformacao de tensor utilizando o <code>reshape()</code> ir\u00e1 funcionar normalmente, pois como mencionado anteriormente, no <code>reshape()</code> nao \u00e9 necess\u00e1rio, se preocupar se a estrutura de dados \u00e9 cont\u00edgua em mem\u00f3ria.</p> In\u00a0[29]: Copied! <pre>f = torch.reshape(t,(1,2,3))\n\nf\n</pre> f = torch.reshape(t,(1,2,3))  f Out[29]: <pre>tensor([[[0.1155, 0.9788, 0.6781],\n         [0.0502, 0.7776, 0.2136]]])</pre> In\u00a0[31]: Copied! <pre>type(f)\n</pre> type(f) Out[31]: <pre>torch.Tensor</pre> <p>Perceba, agora que ap\u00f3s utilizarmos o <code>reshape()</code> e armazenarmos esta transformacao, no objeto <code>f.Tensor</code> temos que a n\u00edvel de mem\u00f3ria este tensor, agora \u00e9 cont\u00edguo em mem\u00f3ria e portanto, se tentarmos realizar uma transformacao utilizando o <code>view</code>, agora nao receberemos mais nenhum erro. Por fim, nao podemos afirmar, por\u00e9m o reshape nos retorna um tensor com dados ou elementos cont\u00edguos em mem\u00f3ria.</p> In\u00a0[30]: Copied! <pre>f.is_contiguous()\n</pre> f.is_contiguous() Out[30]: <pre>True</pre> In\u00a0[32]: Copied! <pre>f.view(2,3)\n</pre> f.view(2,3) Out[32]: <pre>tensor([[0.1155, 0.9788, 0.6781],\n        [0.0502, 0.7776, 0.2136]])</pre> <p>Entretanto, mesmo que agora temos um tensor que possui elementos cont\u00edguos em mem\u00f3ria, se aplicarmos alguma transformacao, novamente teremos um tensor que nao \u00e9 cont\u00edguo em mem\u00f3ria.</p> In\u00a0[35]: Copied! <pre>g = f.T\n</pre> g = f.T <pre>/var/folders/h0/jqw3wsl561j81vjn6nzz1j740000gn/T/ipykernel_94977/3401940231.py:1: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3641.)\n  g = f.T\n</pre> In\u00a0[36]: Copied! <pre>g.is_contiguous()\n</pre> g.is_contiguous() Out[36]: <pre>False</pre> <p>Por\u00e9m caso, por algum motivo o usu\u00e1rio queira modificar a estrutura de dados do tensor de tal forma que ela se transforme em uma estrutura cont\u00edgua podemos utilizar o m\u00e9todo <code>contiguous()</code> E assim, podemos ap\u00f3s converter para um tensor cont\u00edguos podemos usar livremente o <code>view</code></p> In\u00a0[39]: Copied! <pre>g_c = g.contiguous()\n\ng_c\n</pre> g_c = g.contiguous()  g_c Out[39]: <pre>tensor([[[0.1155],\n         [0.0502]],\n\n        [[0.9788],\n         [0.7776]],\n\n        [[0.6781],\n         [0.2136]]])</pre> In\u00a0[40]: Copied! <pre>g_c.view(2,3,1)\n</pre> g_c.view(2,3,1) Out[40]: <pre>tensor([[[0.1155],\n         [0.0502],\n         [0.9788]],\n\n        [[0.7776],\n         [0.6781],\n         [0.2136]]])</pre> <p>Por fim, vimos que podemos transformar tensores, de diversas maneiras, nao s\u00f3 na estrutura do tensor mas tamb\u00e9m modificar a estrutura de dados a n\u00edvel de mem\u00f3ria. Entretanto, a recomendacao \u00e9 usar sempre o <code>reshape()</code> como m\u00e9todo, padrao para se transformar tensores e utilizar o <code>view</code> em situacoes espec\u00edficas.</p> In\u00a0[2]: Copied! <pre>t1 = torch.ones(10,10)\n\nt1\n</pre> t1 = torch.ones(10,10)  t1 Out[2]: <pre>tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])</pre> <p>Veja que ele realizou uma \"mascara\", o local onde antes eram valores 1 ele trocou por 0 realizando um triangulo na parte superior do tensor. J\u00e1 na parte inferior do tensor est\u00e1 da mesma maneira</p> In\u00a0[3]: Copied! <pre>torch.tril(t1)\n</pre> torch.tril(t1) Out[3]: <pre>tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])</pre> <p>Mas, um ponto interessante a se salientar em relacao ao <code>torch.tril()</code> \u00e9 que o mesmo possui alguns parametros especiais, como por exemplo o par\u00e2metro <code>diagonal</code>. Esse parametro por padrao \u00e9 0. Quando o valor de diagonal \u00e9 zero teremos o comportamento padrao que vimos anteriormente. Caso esse valor seja aumentado, ele ir\u00e1 diminuir o tamanho do triangulo na parte superior do tensor, ou seja, diminuiiremos a quantidade de diagonais que tem o n\u00famero 0 presente</p> In\u00a0[4]: Copied! <pre>torch.tril(t1, diagonal = 0)\n</pre> torch.tril(t1, diagonal = 0) Out[4]: <pre>tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])</pre> <p>Veja que agora como utilizamos diagonal=2, o tensor diminui a diagonal de \"0\" significativamente.</p> In\u00a0[5]: Copied! <pre>torch.tril(t1,diagonal = 2)\n</pre> torch.tril(t1,diagonal = 2) Out[5]: <pre>tensor([[1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])</pre> <p>E veja que se utilizarmos valores negativos, o m\u00e9todo <code>torch.tril()</code>inverte a ordem de crescimento do triangulo, entao a direcao do crescimento da \"mascara\" agora \u00e9 do canto superior em direcao ao canto inferior.</p> In\u00a0[6]: Copied! <pre>torch.tril(t1, diagonal = -7)\n</pre> torch.tril(t1, diagonal = -7) Out[6]: <pre>tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.]])</pre> In\u00a0[7]: Copied! <pre>t_1 = torch.ones(2,3)\nt_2 = torch.zeros(2,3)\n</pre> t_1 = torch.ones(2,3) t_2 = torch.zeros(2,3) In\u00a0[8]: Copied! <pre>t_1\n</pre> t_1 Out[8]: <pre>tensor([[1., 1., 1.],\n        [1., 1., 1.]])</pre> In\u00a0[9]: Copied! <pre>t_2\n</pre> t_2 Out[9]: <pre>tensor([[0., 0., 0.],\n        [0., 0., 0.]])</pre> <p>Entao criamos, dois tensores e agora vamos concatenar ambos os tensores utilizando primeiramente o <code>torch.cat()</code>. Para usar, o m\u00e9todo devemos passar como argumento uma tupla contendo ambos os tensores que queremos concatenar.</p> In\u00a0[10]: Copied! <pre>r = torch.cat((t_1, t_2))\n\nr\n</pre> r = torch.cat((t_1, t_2))  r Out[10]: <pre>tensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [0., 0., 0.],\n        [0., 0., 0.]])</pre> In\u00a0[11]: Copied! <pre>r.shape\n</pre> r.shape Out[11]: <pre>torch.Size([4, 3])</pre> <p>Como foi poss\u00edvel ver, na c\u00e9lula de c\u00f3digo acima nao informamos nada, al\u00e9m da tupla contendo ambos os tensores que queremos concatenar. Por\u00e9m, podemos informar tamb\u00e9m a dimensao para qual queremos que essa concatenacao ocorra, caso nao seja informado nenhuma <code>dim</code> a dimensao ser\u00e1 igual a <code>dim = 0</code> que neste caso ir\u00e1 realizar concatenacao na dimensao das linhas. Vamos ver um exemplo informando o par\u00e2metro <code>dim</code> de maneira explicita.</p> <p>Veja que abaixo o exemplo que temos utilizaremos o <code>dim = 1</code> e ao utilizarmos este valor de <code>dim = 1</code> a concatencao ser\u00e1 feita em relacao as colunas ou seja o que est\u00e1 de fato ocorrendo aqui \u00e9 que vamos ter o tensor <code>t_1</code> e iremos \"grudar\" \"unir\" este <code>t_1</code> a partir da sua \u00faltima coluna ao tensor <code>t_2</code>.</p> In\u00a0[12]: Copied! <pre>r1 = torch.cat((t_1, t_2), dim = 1)\n\nr1\n</pre> r1 = torch.cat((t_1, t_2), dim = 1)  r1 Out[12]: <pre>tensor([[1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.]])</pre> <p>Veja que aqui de fato, ocorre o que mencionamos anteriormente e uma das mais de \"provar\" isto \u00e9 utilizar o m\u00e9todo de instancia <code>shape</code> que ir\u00e1 mostrar o formato do Tensor. E aqui o que vemos \u00e9 isso pois temos um Tensor com formato <code>[2,6]</code> ou seja concatenei 2 Tensores <code>[2,3]</code> e como passamos a <code>dim = 1</code>. Logo, este Tensor resultante, ter\u00e1 a dimensao como mostramos pois ser\u00e1 concatenado a partir da dimensao das colunas, logo de fato \u00e9 algo obvio de perceber pois teremos \"3 colunas a serem concatenadas\".</p> In\u00a0[13]: Copied! <pre>r1.shape\n</pre> r1.shape Out[13]: <pre>torch.Size([2, 6])</pre> <p>Veja que caso seja informado um valor de <code>dim</code> que seja maior do que o n\u00famero de dimensoes existentes entre ambos os tensores teremos um erro.</p> In\u00a0[14]: Copied! <pre>r2_test = torch.cat((t_1, t_2), dim = 2)\n</pre> r2_test = torch.cat((t_1, t_2), dim = 2) <pre>\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[14], line 1\n----&gt; 1 r2_test = torch.cat((t_1, t_2), dim = 2)\n\nIndexError: Dimension out of range (expected to be in range of [-2, 1], but got 2)</pre> <p>Vamos, agora abordar o <code>torch.stack()</code> e ver de fato as diferencas existentes entre eles.</p> <p>Bom, o torch.stack() empilha os tensores ao longo de uma nova dimens\u00e3o, diferentemente do <code>torch.cat()</code> que concatena os Tensores. Entao um tem a ideia de concatenar e o <code>torch.stack</code> tem a ideia de empilhar, logo isso provoca um aumento no otal de dimens\u00f5es do tensor resultante.</p> In\u00a0[15]: Copied! <pre>m_1 = torch.ones(2,3)\nm_2 = torch.zeros(2,3)\n</pre> m_1 = torch.ones(2,3) m_2 = torch.zeros(2,3) In\u00a0[16]: Copied! <pre>m_1\n</pre> m_1 Out[16]: <pre>tensor([[1., 1., 1.],\n        [1., 1., 1.]])</pre> In\u00a0[17]: Copied! <pre>m_2\n</pre> m_2 Out[17]: <pre>tensor([[0., 0., 0.],\n        [0., 0., 0.]])</pre> <p>Agora, com os tensores criados vamos, aplicar o <code>torch.stack()</code> e ver como tudo isso funciona. O stack funciona de maneira extremamente parecida com o <code>torch.cat()</code> ou seja vamos informar um tupla contendo os Tensores que iremos empilhar. Vamos ver entao na pr\u00e1tica como ele funciona.</p> In\u00a0[19]: Copied! <pre>r_tensor = torch.stack((m_1, m_2))\n\nr_tensor\n</pre> r_tensor = torch.stack((m_1, m_2))  r_tensor Out[19]: <pre>tensor([[[1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.]]])</pre> In\u00a0[20]: Copied! <pre>r_tensor.shape\n</pre> r_tensor.shape Out[20]: <pre>torch.Size([2, 2, 3])</pre> In\u00a0[21]: Copied! <pre>r2_tensor = torch.stack((m_1, m_2), dim = 1)\n\nr2_tensor\n</pre> r2_tensor = torch.stack((m_1, m_2), dim = 1)  r2_tensor Out[21]: <pre>tensor([[[1., 1., 1.],\n         [0., 0., 0.]],\n\n        [[1., 1., 1.],\n         [0., 0., 0.]]])</pre> In\u00a0[22]: Copied! <pre>r2_tensor.shape\n</pre> r2_tensor.shape Out[22]: <pre>torch.Size([2, 2, 3])</pre> <p>Agora, vamos falar de outro metodo de transformacao de <code>Tensores</code> que neste caso se trata do m\u00e9todo <code>torch.chunk</code>.</p> <p>O <code>torch.chunk</code> tem a funcao, de dividir um tensor em um n\u00famero definido de partes, ou seja, consegue quebrar o tensor em varios \"pedacos\" de tensores menores da maneira como se queira. Entretanto, pode ocorrer que ele seja quebrado em partes de tamanho irregular caso o tensor nao puder ser dividido igualmente.</p> <p>Vamos ver um pouco melhor como isso funciona.</p> In\u00a0[2]: Copied! <pre>import torch\nv = torch.zeros(9)\n\nv\n</pre> import torch v = torch.zeros(9)  v Out[2]: <pre>tensor([0., 0., 0., 0., 0., 0., 0., 0., 0.])</pre> In\u00a0[5]: Copied! <pre>ch = torch.chunk(v, chunks=3)\n\nch\n</pre> ch = torch.chunk(v, chunks=3)  ch Out[5]: <pre>(tensor([0., 0., 0.]), tensor([0., 0., 0.]), tensor([0., 0., 0.]))</pre> <p>Perceba, que criamos nas c\u00e9lulas anteriores um tensor de ordem 1, contendo 9 elementos, e quando, utilizamos <code>chunks = 3</code>, foi possivel quebrar o tensor em exatas 3 partes iguais.</p> <p>Entretanto, se observarmos a c\u00e9lula abaixo, ser\u00e1 poss\u00edvel ver que quando utilizamos <code>chunks = 5</code>, foi poss\u00edvel obter 4 tensores contendo 2 elementos cada um e 1 tensor contendo apenas 1 elemento, e de fato isso ocorreu, pois criamos um tensor de ordem 1 de 9 elementos. E esta \u00e9 a caracteristica do <code>torch.chunk()</code> ele quebra o tensor em uma quantidade informada como parametro da funcao. Uma observacao importante \u00e9 que podemos, realizar o chunk em torno de uma dimensao informando o parametro <code>dim</code>. Neste caso seria redundante pois estamos trabalhando com tensores de ordem 1 e portanto s\u00f3 possuimos esta dimensao para realizar o chunck, entretanto a seguir iremos trabalhar com tensores de ordem 3 ou superior e ser\u00e1 poss\u00edvel ver como realizar o chunk atrav\u00e9s de uma dimensao.</p> In\u00a0[7]: Copied! <pre>ch = torch.chunk(v, chunks = 5)\n\nch\n</pre> ch = torch.chunk(v, chunks = 5)  ch Out[7]: <pre>(tensor([0., 0.]),\n tensor([0., 0.]),\n tensor([0., 0.]),\n tensor([0., 0.]),\n tensor([0.]))</pre> In\u00a0[11]: Copied! <pre>tt = torch.ones(2,3,10)\ntt.shape\n</pre> tt = torch.ones(2,3,10) tt.shape Out[11]: <pre>torch.Size([2, 3, 10])</pre> In\u00a0[12]: Copied! <pre>a,b = torch.chunk(tt, chunks = 2, dim = 2)\n</pre> a,b = torch.chunk(tt, chunks = 2, dim = 2) In\u00a0[13]: Copied! <pre>a.shape\n</pre> a.shape Out[13]: <pre>torch.Size([2, 3, 5])</pre> In\u00a0[14]: Copied! <pre>b.shape\n</pre> b.shape Out[14]: <pre>torch.Size([2, 3, 5])</pre> In\u00a0[15]: Copied! <pre>a,b,c = torch.chunk(tt, chunks = 3, dim = 2)\n\na.shape, b.shape, c.shape\n</pre> a,b,c = torch.chunk(tt, chunks = 3, dim = 2)  a.shape, b.shape, c.shape Out[15]: <pre>(torch.Size([2, 3, 4]), torch.Size([2, 3, 4]), torch.Size([2, 3, 2]))</pre> <p>Entao, veja que ao usar, o parametro <code>dim</code>, e poss\u00edvel quebrar o tensor a partir daquela dada dimensao que foi passada como parametro, e assim como j\u00e1 vimos o m\u00e9todo <code>torch.chunk()</code> ele ir\u00e1 quebrar em partes iguais, at\u00e9 que seja poss\u00edvel. E veja que isso ocorre pois se observarmos o formato dos tensores <code>a</code> e <code>b</code>, ambos possuem dimensao do tipo [2,3,4] enquanto que o tensor <code>c</code>possui uma dimensao [2,3,2] e isso ocorre pois fizemos, um chunk = 3, na <code>dim = 2</code> em um tensor <code>tt</code> de 10 elementos na dimensao 2. E portanto, isso nos retorna um \"comportamento\" que ja era esperado como retorno do m\u00e9todo.</p>"},{"location":"transform_tensors/#manipulacao-e-transformacao-de-tensores","title":"Manipulacao e Transformacao de Tensores\u00b6","text":"<p>No PyTorch, <code>torch.reshape()</code> e <code>torch.view()</code> s\u00e3o duas fun\u00e7\u00f5es usadas para modificar a forma de tensores, mas elas t\u00eam diferen\u00e7as sutis no seu comportamento e uso:</p> <ol> <li><code>torch.reshape(tensor, shape(tuple))</code></li> </ol> <ul> <li>Flexibilidade: <code>torch.reshape()</code> pode ser usado para alterar a forma de um tensor para qualquer outra forma, desde que o n\u00famero total de elementos seja o mesmo. \u00c9 mais flex\u00edvel porque, internamente, pode copiar dados para um novo tensor se necess\u00e1rio.</li> <li>C\u00f3pia vs. Vista: Dependendo da disposi\u00e7\u00e3o original dos dados na mem\u00f3ria, <code>torch.reshape()</code> pode retornar uma vista (um novo tensor que compartilha dados com o tensor original) ou uma c\u00f3pia do tensor original. Isso significa que, se voc\u00ea modificar os dados em um tensor retornado por <code>reshape()</code>, essas modifica\u00e7\u00f5es podem ou n\u00e3o ser refletidas no tensor original, dependendo de se uma vista foi retornada ou n\u00e3o.</li> </ul> <ol> <li><code>tensor.view(tensor, shape)</code></li> </ol> <ul> <li>Exig\u00eancia de Contiguidade: <code>tensor.view()</code> exige que o tensor original seja cont\u00edguo na mem\u00f3ria (ou seja, os elementos s\u00e3o armazenados em sequ\u00eancia sem lacunas) para poder retornar uma vista com a nova forma desejada. Se o tensor original n\u00e3o for cont\u00edguo, voc\u00ea receber\u00e1 um erro ao tentar usar <code>view()</code>. Nesse caso, voc\u00ea pode usar <code>tensor.contiguous()</code> antes de chamar <code>view()</code>.</li> <li>Sempre Retorna uma Vista: Diferente de <code>reshape()</code>, <code>view()</code> sempre retorna uma view do tensor original, n\u00e3o uma c\u00f3pia. Isso significa que qualquer modifica\u00e7\u00e3o nos dados do tensor retornado por <code>view()</code> ser\u00e1 refletida no tensor original, e vice-versa.</li> </ul> <p>Quando utilizar <code>torch.reshape()</code> ou <code>torch.view()</code></p> <p>A escolha entre <code>torch.reshape()</code> e <code>torch.view()</code> geralmente depende de dois fatores: se voc\u00ea precisa garantir que o tensor retornado seja uma view do tensor original e n\u00e3o uma c\u00f3pia (caso em que voc\u00ea usaria <code>view()</code>), e se o tensor original \u00e9 cont\u00edguo na mem\u00f3ria (caso em que voc\u00ea pode usar <code>view()</code> sem problemas). Se n\u00e3o tiver certeza sobre a contiguidade do tensor ou se precisar de flexibilidade para lidar com tensores n\u00e3o cont\u00edguos, <code>torch.reshape()</code> \u00e9 a escolha mais segura. No fim, de maneira geral o mais recomendado a se utilizar \u00e9 o <code>torch.reshape()</code> por ser o mais utilizado e generalista.</p>"},{"location":"transform_tensors/#torchtril","title":"torch.tril()\u00b6","text":"<p>Vamos, ver agora o m\u00e9todo <code>torch.tril()</code>. O que este m\u00e9todo faz ? bom este m\u00e9todo e respons\u00e1vel por criar um matriz triangular com o mesmo comportamento das matrizes que estudamos em algebra linear.</p> <p>Legal, mas onde iremos utilizar este tipo de matriz triangular?</p> <p>Bom, as matrizes triangular, ou melhor dizendo os tensores triangulares sao extremamente \u00fateis quando queremos criar \"m\u00e1scaras\" em nossos tensores. Um exemplo de aplicacao extremamente famosa \u00e9 na implementacao do mecanismo de atencao da arquitetura Transformer do paper \"Attetion is all you need\"</p> <p>Vamos entao observar como utilizar o <code>torch.tril()</code> na pr\u00e1tica</p>"},{"location":"transform_tensors/#torchcat-e-torchstack","title":"torch.cat() e torch.stack()\u00b6","text":"<p>Vamos, agora entender melhor sobre dois m\u00e9todos que sao muito parecidos entre si mas com pequenas sutis diferencas.</p> <p>Os m\u00e9todo que estamos falando sao o <code>torch.cat()</code> e o <code>torch.stack()</code>. O <code>torch.cat()</code> tem a funcao de concatenar 2 tensores o transformando ou redimensionando em um tensor bem maior, o usu\u00e1rio pode informar a dimensao a qual queremos que os tensores sejam concatenados, por\u00e9m caso nao seja informada nenhuma dimensao, entao ser\u00e1 concatenado atrav\u00e9s da dimensao 0 que \u00e9 a padrao. Vamos ver na pr\u00e1tica como tudo isso funciona:</p>"}]}